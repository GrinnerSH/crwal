# import os
# import pandas as pd
# import networkx as nx
# import recordlinkage
# from jarowinkler import jarowinkler_similarity
# from py2neo import Graph as Py2neoGraph, Node, Relationship

# from config import config

# class GraphManager:
#     def __init__(self, graph_path=None):
#         if graph_path and os.path.exists(graph_path):
#             self.load_graph(graph_path)
#         else:
#             self.G = nx.MultiDiGraph() # æ”¯æŒå¹³è¡Œè¾¹å’Œæœ‰å‘è¾¹

#     def add_triples(self, triples):
#         """æ·»åŠ ä¸‰å…ƒç»„åˆ—è¡¨åˆ°å›¾ä¸­ï¼Œå¹¶è¿”å›æ–°å¢çš„å®ä½“èŠ‚ç‚¹"""
#         new_nodes = set()
#         for triple in triples:
#             subj, rel, obj = triple['subject'], triple['relation'], triple['object']
            
#             # ä¸ºèŠ‚ç‚¹æ·»åŠ ç±»å‹å±æ€§ï¼Œå¦‚æœLLMèƒ½æä¾›çš„è¯
#             subj_type = triple.get('subject_type', 'Unknown')
#             obj_type = triple.get('object_type', 'Unknown')

#             if subj not in self.G:
#                 self.G.add_node(subj, type=subj_type, names={subj})
#                 new_nodes.add(subj)
#             if obj not in self.G:
#                 self.G.add_node(obj, type=obj_type, names={obj})
#                 new_nodes.add(obj)
            
#             # è¾¹ä¸Šå­˜å‚¨å…³ç³»ç±»å‹å’ŒæºURL
#             self.G.add_edge(subj, obj, key=rel, relation=rel, source_url=triple.get('source_url'))
#         return list(new_nodes)

#     def resolve_entities_incremental(self, new_nodes):
#         """å¯¹æ–°å¢èŠ‚ç‚¹è¿›è¡Œå¢é‡å¼å®ä½“å¯¹é½"""
#         if not new_nodes:
#             return

#         existing_nodes = list(self.G.nodes)
#         all_nodes = list(set(existing_nodes + new_nodes))
        
#         # åˆ›å»ºä¸€ä¸ªPandas DataFrameç”¨äºrecordlinkage
#         import pandas as pd
#         df = pd.DataFrame({'name': all_nodes, 'type': [self.G.nodes[n].get('type', 'Unknown') for n in all_nodes]})
#         df.index.name = 'id'

#         # 1. é˜»å¡ï¼šåªæ¯”è¾ƒåŒç±»å‹çš„å®ä½“
#         indexer = recordlinkage.Index()
#         indexer.block('type')
#         candidate_links = indexer.index(df)

#         print(f"Generated {len(candidate_links)} candidate pairs for entity resolution.")

#         # 2. æ¯”è¾ƒï¼šä½¿ç”¨Jaro-Winklerç›¸ä¼¼åº¦
#         compare = recordlinkage.Compare()
#         compare.string('name', 'name', method='jarowinkler', threshold=config.SIMILARITY_THRESHOLD, label='name_sim')
#         features = compare.compute(candidate_links, df)
        
#         matches = features[features.sum(axis=1) > 0].index
#         print(f"Found {len(matches)} matching pairs above threshold.")

#         # 3. èšç±»ä¸åˆå¹¶ - å¢åŠ é¢å¤–çš„è¿‡æ»¤é€»è¾‘
#         if not matches.empty:
#             match_graph = nx.Graph(list(matches))
#             for component in nx.connected_components(match_graph):
#                 nodes_to_merge_indices = list(component)
#                 nodes_to_merge = [df.iloc[i]['name'] for i in nodes_to_merge_indices]
                
#                 # å¢åŠ ä¸¥æ ¼çš„è¿‡æ»¤æ¡ä»¶
#                 if self._should_merge_entities(nodes_to_merge):
#                     # é€‰æ‹©æœ€é•¿çš„åå­—ä½œä¸ºè§„èŒƒå
#                     canonical_name = max(nodes_to_merge, key=len)
                    
#                     print(f"Merging {nodes_to_merge} into {canonical_name}")
                    
#                     # åœ¨ä¸»å›¾ä¸­æ‰§è¡Œåˆå¹¶
#                     # nx.contracted_nodesä¸ç›´æ¥æ”¯æŒMultiDiGraphï¼Œéœ€è¦æ‰‹åŠ¨åˆå¹¶
#                     self._merge_nodes(nodes_to_merge, canonical_name)
#                 else:
#                     print(f"Skipping merge for {nodes_to_merge} - failed validation checks")

#     def _should_merge_entities(self, entities):
#         """åˆ¤æ–­å®ä½“æ˜¯å¦åº”è¯¥è¢«åˆå¹¶çš„ä¸¥æ ¼æ£€æŸ¥"""
#         if len(entities) < 2:
#             return False
        
#         # æ£€æŸ¥å…³é”®è¯å†²çª - é˜²æ­¢æ€§åˆ«ã€ç±»å‹ç­‰å…³é”®å·®å¼‚è¢«å¿½ç•¥
#         conflict_keywords = [
#             ('ç”·', 'å¥³'), ('ç”·æ€§', 'å¥³æ€§'), ('ç”·ä¸»è§’', 'å¥³ä¸»è§’'),
#             ('æœ€ä½³', 'æœ€å·®'), ('è·å¥–', 'æå'), 
#             ('ç”µå½±', 'ç”µè§†å‰§'), ('å¯¼æ¼”', 'æ¼”å‘˜'), ('åˆ¶ç‰‡äºº', 'ç¼–å‰§')
#         ]
        
#         # è¯­ä¹‰å†²çªæ£€æµ‹ - é˜²æ­¢æ¦‚å¿µä¸å…·ä½“å®ä¾‹/äººç‰©çš„é”™è¯¯åˆå¹¶
#         semantic_conflicts = [
#             # æŠ€æœ¯æ¦‚å¿µ vs äººç‰©ç§°è°“
#             ('æ·±åº¦å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ä¹‹çˆ¶'),
#             ('äººå·¥æ™ºèƒ½', 'äººå·¥æ™ºèƒ½ä¹‹çˆ¶'),
#             ('æœºå™¨å­¦ä¹ ', 'æœºå™¨å­¦ä¹ ä¸“å®¶'),
#             # æ¦‚å¿µ vs å‘å±•/å†å²
#             ('æ·±åº¦å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ å‘å±•'),
#             ('äººå·¥æ™ºèƒ½', 'äººå·¥æ™ºèƒ½å‘å±•'),
#             ('äººå·¥æ™ºèƒ½', 'äººå·¥æ™ºèƒ½æ¦‚å¿µ'),  # æ·»åŠ è¿™ä¸ª
#             ('æœºå™¨å­¦ä¹ ', 'æœºå™¨å­¦ä¹ å†å²'),
#             ('ç¥ç»ç½‘ç»œ', 'ç¥ç»ç½‘ç»œå†å²'),  # æ·»åŠ è¿™ä¸ª
#             # æŠ€æœ¯ vs æ¦‚å¿µ
#             ('æŠ€æœ¯', 'æ¦‚å¿µ'),
#             ('ç†è®º', 'å®è·µ'),
#             ('æ–¹æ³•', 'ç»“æœ'),
#             # æœºæ„ vs äººç‰©
#             ('å¤§å­¦', 'æ•™æˆ'),
#             ('å…¬å¸', 'åšå£«'),
#         ]
        
#         # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸åŒç±»å‹çš„å®ä½“ï¼ˆæ¦‚å¿µ/äººç‰©/äº‹ä»¶ç­‰ï¼‰
#         entity_type_indicators = {
#             'person': ['ä¹‹çˆ¶', 'ä¹‹æ¯', 'ä¸“å®¶', 'å­¦è€…', 'æ•™æˆ', 'åšå£«', 'å…ˆç”Ÿ', 'å¥³å£«', 'é™¢å£«'],
#             'concept': ['æ¦‚å¿µ', 'ç†è®º', 'æ€æƒ³', 'åŸç†', 'å®šä¹‰'],
#             'process': ['å‘å±•', 'å†å²', 'æ¼”è¿›', 'è¿‡ç¨‹', 'é˜¶æ®µ', 'è¶‹åŠ¿'],
#             'technology': ['æŠ€æœ¯', 'æ–¹æ³•', 'ç®—æ³•', 'æ¨¡å‹', 'ç³»ç»Ÿ'],
#             'organization': ['å…¬å¸', 'ä¼ä¸š', 'æœºæ„', 'ç»„ç»‡', 'å›¢é˜Ÿ', 'å®éªŒå®¤'],
#             'event': ['ä¼šè®®', 'å¤§ä¼š', 'è®ºå›', 'æ¯”èµ›', 'ç«èµ›', 'æ´»åŠ¨']
#         }
        
#         # æ£€æŸ¥æ—¥æœŸç²¾åº¦å†²çª - é˜²æ­¢ä¸åŒç²¾åº¦çš„æ—¥æœŸè¢«åˆå¹¶
#         date_patterns = ['å¹´', 'æœˆ', 'æ—¥', 'å·']
        
#         for entity1 in entities:
#             for entity2 in entities:
#                 if entity1 != entity2:
#                     # æ£€æŸ¥æ˜¯å¦åŒ…å«å†²çªå…³é”®è¯
#                     for kw1, kw2 in conflict_keywords:
#                         if (kw1 in entity1 and kw2 in entity2) or (kw2 in entity1 and kw1 in entity2):
#                             print(f"Conflict detected between '{entity1}' and '{entity2}': {kw1}/{kw2}")
#                             return False
                    
#                     # æ£€æŸ¥è¯­ä¹‰å†²çª
#                     for concept, related in semantic_conflicts:
#                         if (concept in entity1 and related in entity2) or (related in entity1 and concept in entity2):
#                             print(f"Semantic conflict detected between '{entity1}' and '{entity2}': concept/related entity")
#                             return False
                    
#                     # æ£€æŸ¥å®ä½“ç±»å‹å†²çª
#                     entity1_types = set()
#                     entity2_types = set()
                    
#                     for entity_type, indicators in entity_type_indicators.items():
#                         for indicator in indicators:
#                             if indicator in entity1:
#                                 entity1_types.add(entity_type)
#                             if indicator in entity2:
#                                 entity2_types.add(entity_type)
                    
#                     # å¦‚æœä¸¤ä¸ªå®ä½“è¢«åˆ†ç±»ä¸ºä¸åŒç±»å‹ï¼Œåˆ™ä¸åˆå¹¶
#                     if entity1_types and entity2_types and entity1_types.isdisjoint(entity2_types):
#                         print(f"Entity type conflict between '{entity1}' ({entity1_types}) and '{entity2}' ({entity2_types})")
#                         return False
                    
#                     # æ£€æŸ¥æ—¥æœŸç²¾åº¦å†²çªï¼ˆå¦‚"1929å¹´" vs "1929å¹´5æœˆ16æ—¥"ï¼‰
#                     entity1_has_date = any(dp in entity1 for dp in date_patterns)
#                     entity2_has_date = any(dp in entity2 for dp in date_patterns)
                    
#                     if entity1_has_date and entity2_has_date:
#                         # éƒ½åŒ…å«æ—¥æœŸå…ƒç´ ï¼Œæ£€æŸ¥ç²¾åº¦æ˜¯å¦ä¸åŒ
#                         entity1_precision = 0
#                         entity2_precision = 0
                        
#                         if 'å¹´' in entity1: entity1_precision += 1
#                         if 'æœˆ' in entity1: entity1_precision += 1
#                         if 'æ—¥' in entity1 or 'å·' in entity1: entity1_precision += 1
                        
#                         if 'å¹´' in entity2: entity2_precision += 1
#                         if 'æœˆ' in entity2: entity2_precision += 1
#                         if 'æ—¥' in entity2 or 'å·' in entity2: entity2_precision += 1
                        
#                         if entity1_precision != entity2_precision:
#                             print(f"Date precision conflict between '{entity1}' and '{entity2}': {entity1_precision} vs {entity2_precision}")
#                             return False
                    
#                     # é•¿åº¦å·®å¼‚è¿‡å¤§çš„ä¸åˆå¹¶ï¼ˆå¯èƒ½æ˜¯ç¼©å†™vså…¨ç§°ä¹‹å¤–çš„æƒ…å†µï¼‰
#                     len_ratio = min(len(entity1), len(entity2)) / max(len(entity1), len(entity2))
#                     if len_ratio < 0.5:  # é•¿åº¦æ¯”ä¾‹å°äº0.5
#                         print(f"Length ratio too small between '{entity1}' and '{entity2}': {len_ratio}")
#                         return False
        
#         return True

#     def _merge_nodes(self, nodes_to_merge, canonical_name):
#         """æ‰‹åŠ¨åˆå¹¶MultiDiGraphä¸­çš„èŠ‚ç‚¹"""
#         if canonical_name not in nodes_to_merge:
#             # å¦‚æœè§„èŒƒåæœ¬èº«ä¸åœ¨å¾…åˆå¹¶åˆ—è¡¨ä¸­ï¼Œå…ˆæ·»åŠ å®ƒçš„å±æ€§
#             self.G.add_node(canonical_name, **self.G.nodes[nodes_to_merge[0]])
        
#         all_names = set()
#         for node_name in nodes_to_merge:
#             if node_name in self.G:
#                 all_names.update(self.G.nodes[node_name].get('names', {node_name}))

#         self.G.nodes[canonical_name]['names'] = all_names

#         for node_name in nodes_to_merge:
#             if node_name == canonical_name or node_name not in self.G:
#                 continue

#             # é‡å®šå‘å…¥è¾¹
#             for u, _, key, data in self.G.in_edges(node_name, keys=True, data=True):
#                 self.G.add_edge(u, canonical_name, key=key, **data)
            
#             # é‡å®šå‘å‡ºè¾¹
#             for _, v, key, data in self.G.out_edges(node_name, keys=True, data=True):
#                 self.G.add_edge(canonical_name, v, key=key, **data)
            
#             self.G.remove_node(node_name)


#     def save_graph(self, filepath=config.GRAPH_STORE_PATH):
#         """å°†NetworkXå›¾ä¿å­˜åˆ°æ–‡ä»¶"""
#         import pickle
#         try:
#             # ä½¿ç”¨æ›´æ–°çš„æ–¹æ³•ä¿å­˜å›¾
#             with open(filepath, 'wb') as f:
#                 pickle.dump(self.G, f)
#             print(f"Graph saved to {filepath}")
#         except Exception as e:
#             print(f"Error saving graph: {e}")

#     def load_graph(self, filepath=config.GRAPH_STORE_PATH):
#         """ä»æ–‡ä»¶åŠ è½½NetworkXå›¾"""
#         import pickle
#         try:
#             # ä½¿ç”¨æ›´æ–°çš„æ–¹æ³•åŠ è½½å›¾
#             with open(filepath, 'rb') as f:
#                 self.G = pickle.load(f)
#             print(f"Graph loaded from {filepath}")
#         except FileNotFoundError:
#             print(f"Graph file {filepath} not found, starting with empty graph")
#             self.G = nx.MultiDiGraph()
#         except Exception as e:
#             print(f"Error loading graph: {e}, starting with empty graph")
#             self.G = nx.MultiDiGraph()

#     def save_to_neo4j(self):
#         """å°†å›¾è°±æ•°æ®æŒä¹…åŒ–åˆ°Neo4jæ•°æ®åº“"""
#         try:
#             db = Py2neoGraph(config.NEO4J_URI, auth=(config.NEO4J_USER, config.NEO4J_PASSWORD))
#             # æ¸…ç©ºæ•°æ®åº“ä»¥è¿›è¡Œå…¨æ–°å¯¼å…¥
#             db.delete_all()
            
#             tx = db.begin()
#             # åˆ›å»ºèŠ‚ç‚¹
#             for node, data in self.G.nodes(data=True):
#                 node_type = data.get('type', 'Entity')
#                 properties = {'name': node, 'names': list(data.get('names', {node}))}
#                 n = Node(node_type, **properties)
#                 tx.create(n)
#             tx.commit()

#             # åˆ›å»ºå…³ç³»
#             tx = db.begin()
#             for u, v, data in self.G.edges(data=True):
#                 relation_type = data['relation'].upper()
#                 start_node = db.nodes.match(name=u).first()
#                 end_node = db.nodes.match(name=v).first()
#                 if start_node and end_node:
#                     rel = Relationship(start_node, relation_type, end_node, source_url=data.get('source_url'))
#                     tx.create(rel)
#             tx.commit()
#             print("Graph successfully saved to Neo4j.")
#         except Exception as e:
#             print(f"Failed to save to Neo4j: {e}")
#!/usr/bin/env python3
"""
å¢å¼ºçš„çŸ¥è¯†å›¾è°±ç®¡ç†å™¨ - æ”¯æŒå®æ—¶ä¿å­˜
"""

import os
import json
import pickle
import pandas as pd
import networkx as nx
import recordlinkage
from datetime import datetime
from jarowinkler import jarowinkler_similarity
from py2neo import Graph as Py2neoGraph, Node, Relationship

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import config

class GraphManager:
    """å¢å¼ºçš„çŸ¥è¯†å›¾è°±ç®¡ç†å™¨ï¼Œæ”¯æŒå®æ—¶ä¿å­˜å’Œå¢é‡æ›´æ–°"""
    
    def __init__(self, graph_path=None, auto_save=True, save_frequency=1):
        """
        åˆå§‹åŒ–å›¾è°±ç®¡ç†å™¨
        
        Args:
            graph_path: å›¾è°±æ–‡ä»¶è·¯å¾„
            auto_save: æ˜¯å¦å¯ç”¨è‡ªåŠ¨ä¿å­˜
            save_frequency: ä¿å­˜é¢‘ç‡ï¼ˆå¤„ç†å¤šå°‘ä¸ªURLåä¿å­˜ä¸€æ¬¡ï¼‰
        """
        self.auto_save = auto_save
        self.save_frequency = save_frequency
        self.processed_urls = 0
        
        # è®¾ç½®æ–‡ä»¶è·¯å¾„
        self.graph_path = graph_path or config.GRAPH_STORE_PATH
        self.triples_log_path = os.path.join(
            os.path.dirname(self.graph_path), 
            "triples_log.jsonl"
        )
        self.backup_dir = os.path.join(
            os.path.dirname(self.graph_path),
            "backups"
        )
        
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        os.makedirs(self.backup_dir, exist_ok=True)
        
        # åŠ è½½æˆ–åˆ›å»ºå›¾è°±
        if graph_path and os.path.exists(graph_path):
            self.load_graph(graph_path)
        else:
            self.G = nx.MultiDiGraph()
        
        print(f"âœ… å›¾è°±ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   å›¾è°±æ–‡ä»¶: {self.graph_path}")
        print(f"   ä¸‰å…ƒç»„æ—¥å¿—: {self.triples_log_path}")
        print(f"   è‡ªåŠ¨ä¿å­˜: {'å¯ç”¨' if auto_save else 'ç¦ç”¨'}")
        if auto_save:
            print(f"   ä¿å­˜é¢‘ç‡: æ¯å¤„ç†{save_frequency}ä¸ªURLä¿å­˜ä¸€æ¬¡")

    def add_triples_with_logging(self, triples, source_url):
        """
        æ·»åŠ ä¸‰å…ƒç»„å¹¶è®°å½•æ—¥å¿—
        
        Args:
            triples: ä¸‰å…ƒç»„åˆ—è¡¨
            source_url: æºURL
        """
        if not triples:
            return []
        
        # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
        self._log_triples(triples, source_url)
        
        # æ·»åŠ åˆ°å›¾è°±
        new_nodes = self.add_triples(triples)
        
        # æ‰§è¡Œå®ä½“å¯¹é½
        if new_nodes:
            self.resolve_entities_incremental(new_nodes)
        
        # æ›´æ–°å¤„ç†è®¡æ•°
        self.processed_urls += 1
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨ä¿å­˜
        if self.auto_save and self.processed_urls % self.save_frequency == 0:
            self._auto_save()
        
        return new_nodes
    
    def _log_triples(self, triples, source_url):
        """å°†ä¸‰å…ƒç»„è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "source_url": source_url,
            "triples_count": len(triples),
            "triples": triples
        }
        
        try:
            with open(self.triples_log_path, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
            
            print(f"ğŸ“ å·²è®°å½• {len(triples)} ä¸ªä¸‰å…ƒç»„åˆ°æ—¥å¿—æ–‡ä»¶")
            
        except Exception as e:
            print(f"âš ï¸  è®°å½•ä¸‰å…ƒç»„æ—¥å¿—å¤±è´¥: {e}")
    
    def _auto_save(self):
        """è‡ªåŠ¨ä¿å­˜å›¾è°±"""
        print(f"ğŸ’¾ è‡ªåŠ¨ä¿å­˜å›¾è°± (å·²å¤„ç†{self.processed_urls}ä¸ªURL)...")
        
        # åˆ›å»ºå¤‡ä»½
        self._create_backup()
        
        # ä¿å­˜ä¸»å›¾è°±
        self.save_graph()
        
        print(f"âœ… è‡ªåŠ¨ä¿å­˜å®Œæˆ")
    
    def _create_backup(self):
        """åˆ›å»ºå›¾è°±å¤‡ä»½"""
        if not os.path.exists(self.graph_path):
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_filename = f"graph_backup_{timestamp}_{self.processed_urls}urls.gpickle"
        backup_path = os.path.join(self.backup_dir, backup_filename)
        
        try:
            # å¤åˆ¶å½“å‰å›¾è°±æ–‡ä»¶
            import shutil
            shutil.copy2(self.graph_path, backup_path)
            print(f"ğŸ“¦ å·²åˆ›å»ºå¤‡ä»½: {backup_filename}")
            
            # æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™æœ€è¿‘10ä¸ªï¼‰
            self._cleanup_old_backups()
            
        except Exception as e:
            print(f"âš ï¸  åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}")
    
    def _cleanup_old_backups(self, keep_count=10):
        """æ¸…ç†æ—§çš„å¤‡ä»½æ–‡ä»¶"""
        try:
            import glob
            backup_files = glob.glob(os.path.join(self.backup_dir, "graph_backup_*.gpickle"))
            backup_files.sort(key=os.path.getmtime, reverse=True)
            
            # åˆ é™¤å¤šä½™çš„å¤‡ä»½
            for old_backup in backup_files[keep_count:]:
                os.remove(old_backup)
                print(f"ğŸ—‘ï¸  åˆ é™¤æ—§å¤‡ä»½: {os.path.basename(old_backup)}")
                
        except Exception as e:
            print(f"âš ï¸  æ¸…ç†å¤‡ä»½å¤±è´¥: {e}")
    
    def force_save(self):
        """å¼ºåˆ¶ä¿å­˜å›¾è°±å’Œæ—¥å¿—"""
        print(f"ğŸ’¾ å¼ºåˆ¶ä¿å­˜å›¾è°±...")
        self._create_backup()
        self.save_graph()
        self._save_progress_report()
        print(f"âœ… å¼ºåˆ¶ä¿å­˜å®Œæˆ")
    
    def _save_progress_report(self):
        """ä¿å­˜å¤„ç†è¿›åº¦æŠ¥å‘Š"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "processed_urls": self.processed_urls,
            "total_nodes": len(self.G.nodes),
            "total_edges": len(self.G.edges),
            "graph_file": self.graph_path,
            "triples_log": self.triples_log_path
        }
        
        report_path = os.path.join(
            os.path.dirname(self.graph_path),
            "progress_report.json"
        )
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“Š å·²ä¿å­˜è¿›åº¦æŠ¥å‘Š: {os.path.basename(report_path)}")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜è¿›åº¦æŠ¥å‘Šå¤±è´¥: {e}")
    
    def get_statistics(self):
        """è·å–å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
        backup_count = 0
        if os.path.exists(self.backup_dir):
            backup_files = [f for f in os.listdir(self.backup_dir) if f.startswith("graph_backup_")]
            backup_count = len(backup_files)
        
        last_save_time = "æœªä¿å­˜"
        if os.path.exists(self.graph_path):
            mod_time = datetime.fromtimestamp(os.path.getmtime(self.graph_path))
            last_save_time = mod_time.strftime('%Y-%m-%d %H:%M:%S')
        
        stats = {
            "urls_processed": self.processed_urls,
            "total_nodes": len(self.G.nodes),
            "total_edges": len(self.G.edges),
            "graph_file_exists": os.path.exists(self.graph_path),
            "triples_log_exists": os.path.exists(self.triples_log_path),
            "backup_count": backup_count,
            "last_save_time": last_save_time,
            "auto_save_enabled": self.auto_save,
            "save_frequency": self.save_frequency
        }
        return stats
    
    def recover_from_log(self, start_timestamp=None):
        """ä»æ—¥å¿—æ–‡ä»¶æ¢å¤ä¸‰å…ƒç»„"""
        if not os.path.exists(self.triples_log_path):
            print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {self.triples_log_path}")
            return
        
        print(f"ğŸ”„ ä»æ—¥å¿—æ–‡ä»¶æ¢å¤ä¸‰å…ƒç»„...")
        recovered_count = 0
        
        try:
            with open(self.triples_log_path, 'r', encoding='utf-8') as f:
                for line in f:
                    entry = json.loads(line.strip())
                    
                    # å¦‚æœæŒ‡å®šäº†å¼€å§‹æ—¶é—´ï¼Œè·³è¿‡ä¹‹å‰çš„è®°å½•
                    if start_timestamp and entry['timestamp'] < start_timestamp:
                        continue
                    
                    # æ·»åŠ ä¸‰å…ƒç»„åˆ°å›¾è°±
                    triples = entry['triples']
                    for triple in triples:
                        triple['source_url'] = entry['source_url']
                    
                    new_nodes = self.add_triples(triples)
                    if new_nodes:
                        self.resolve_entities_incremental(new_nodes)
                    
                    recovered_count += len(triples)
            
            print(f"âœ… å·²ä»æ—¥å¿—æ¢å¤ {recovered_count} ä¸ªä¸‰å…ƒç»„")
            
        except Exception as e:
            print(f"âŒ ä»æ—¥å¿—æ¢å¤å¤±è´¥: {e}")

    # ä»¥ä¸‹æ–¹æ³•ç»§æ‰¿è‡ªåŸå§‹GraphManager
    def add_triples(self, triples):
        """æ·»åŠ ä¸‰å…ƒç»„åˆ—è¡¨åˆ°å›¾ä¸­ï¼Œå¹¶è¿”å›æ–°å¢çš„å®ä½“èŠ‚ç‚¹"""
        new_nodes = set()
        for triple in triples:
            subj, rel, obj = triple['subject'], triple['relation'], triple['object']
            
            # ä¸ºèŠ‚ç‚¹æ·»åŠ ç±»å‹å±æ€§ï¼Œå¦‚æœLLMèƒ½æä¾›çš„è¯
            subj_type = triple.get('subject_type', 'Unknown')
            obj_type = triple.get('object_type', 'Unknown')

            if subj not in self.G:
                self.G.add_node(subj, type=subj_type, names={subj})
                new_nodes.add(subj)
            if obj not in self.G:
                self.G.add_node(obj, type=obj_type, names={obj})
                new_nodes.add(obj)
            
            # è¾¹ä¸Šå­˜å‚¨å…³ç³»ç±»å‹å’ŒæºURL
            self.G.add_edge(subj, obj, key=rel, relation=rel, source_url=triple.get('source_url'))
        return list(new_nodes)

    def resolve_entities_incremental(self, new_nodes):
        """å¯¹æ–°å¢èŠ‚ç‚¹è¿›è¡Œå¢é‡å¼å®ä½“å¯¹é½"""
        if not new_nodes:
            return

        existing_nodes = list(self.G.nodes)
        all_nodes = list(set(existing_nodes + new_nodes))
        
        # åˆ›å»ºä¸€ä¸ªPandas DataFrameç”¨äºrecordlinkage
        df = pd.DataFrame({'name': all_nodes, 'type': [self.G.nodes[n].get('type', 'Unknown') for n in all_nodes]})
        df.index.name = 'id'

        # 1. é˜»å¡ï¼šåªæ¯”è¾ƒåŒç±»å‹çš„å®ä½“
        indexer = recordlinkage.Index()
        indexer.block('type')
        candidate_links = indexer.index(df)

        print(f"Generated {len(candidate_links)} candidate pairs for entity resolution.")

        # 2. æ¯”è¾ƒï¼šä½¿ç”¨Jaro-Winklerç›¸ä¼¼åº¦
        compare = recordlinkage.Compare()
        compare.string('name', 'name', method='jarowinkler', threshold=config.SIMILARITY_THRESHOLD, label='name_sim')
        features = compare.compute(candidate_links, df)
        
        matches = features[features.sum(axis=1) > 0].index
        print(f"Found {len(matches)} matching pairs above threshold.")

        # 3. èšç±»ä¸åˆå¹¶ - å¢åŠ é¢å¤–çš„è¿‡æ»¤é€»è¾‘
        if not matches.empty:
            match_graph = nx.Graph(list(matches))
            for component in nx.connected_components(match_graph):
                nodes_to_merge_indices = list(component)
                nodes_to_merge = [df.iloc[i]['name'] for i in nodes_to_merge_indices]
                
                # å¢åŠ ä¸¥æ ¼çš„è¿‡æ»¤æ¡ä»¶
                if self._should_merge_entities(nodes_to_merge):
                    # é€‰æ‹©æœ€é•¿çš„åå­—ä½œä¸ºè§„èŒƒå
                    canonical_name = max(nodes_to_merge, key=len)
                    
                    print(f"Merging {nodes_to_merge} into {canonical_name}")
                    
                    # åœ¨ä¸»å›¾ä¸­æ‰§è¡Œåˆå¹¶
                    self._merge_nodes(nodes_to_merge, canonical_name)
                else:
                    print(f"Skipping merge for {nodes_to_merge} - failed validation checks")

    def _should_merge_entities(self, entities):
        """åˆ¤æ–­å®ä½“æ˜¯å¦åº”è¯¥è¢«åˆå¹¶çš„ä¸¥æ ¼æ£€æŸ¥"""
        if len(entities) < 2:
            return False
        
        # ===== æ–°å¢æ£€æŸ¥1: æ•°å­—/å¹´ä»½ç²¾ç¡®åŒ¹é…æ£€æŸ¥ =====
        import re
        # è¯†åˆ«æ•°å­—æˆ–å¹´ä»½çš„æ­£åˆ™è¡¨è¾¾å¼
        number_pattern = re.compile(r'^[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+(å¹´)?$')
        chinese_year_pattern = re.compile(r'^[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+å¹´?$')
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å®ä½“éƒ½æ˜¯æ•°å­—/å¹´ä»½æ ¼å¼
        all_are_numbers = all(number_pattern.match(entity) or chinese_year_pattern.match(entity) for entity in entities)
        
        if all_are_numbers:
            # å¦‚æœéƒ½æ˜¯æ•°å­—/å¹´ä»½ï¼Œè¦æ±‚å®Œå…¨ç›¸ç­‰æ‰èƒ½åˆå¹¶
            first_entity = entities[0]
            if not all(entity == first_entity for entity in entities):
                print(f"Number/Year mismatch: {entities} - different numbers cannot be merged")
                return False
        
        # ===== æ–°å¢æ£€æŸ¥2: åç§°åŒ…å«å…³ç³»ä¸¥æ ¼æ£€æŸ¥ =====
        for i, entity1 in enumerate(entities):
            for j, entity2 in enumerate(entities):
                if i >= j:  # é¿å…é‡å¤æ£€æŸ¥
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åŒ…å«å…³ç³»
                if entity1 in entity2 or entity2 in entity1:
                    shorter = entity1 if len(entity1) < len(entity2) else entity2
                    longer = entity2 if len(entity1) < len(entity2) else entity1
                    
                    # è®¡ç®—é•¿åº¦å·®å¼‚
                    length_diff = len(longer) - len(shorter)
                    
                    # å¦‚æœé•¿åº¦å·®å¼‚è¾ƒå°ï¼ˆ1-3ä¸ªå­—ç¬¦ï¼‰ï¼Œéœ€è¦æ£€æŸ¥é¢å¤–éƒ¨åˆ†
                    if 1 <= length_diff <= 3:
                        extra_part = longer.replace(shorter, '', 1)  # è·å–é¢å¤–éƒ¨åˆ†
                        
                        # å®šä¹‰å¯èƒ½è¡¨ç¤ºä¸åŒå®ä½“çš„åç¼€/ä¿®é¥°è¯
                        distinct_suffixes = ['æ˜Ÿ', 'å', 'æ˜', 'å¼º', 'ä¼Ÿ', 'å³°', 'éœ', 'å¨Ÿ', 'ä¸½', 'çº¢', 
                                           'é™¢', 'æ‰€', 'ä¼š', 'å¸', 'å‚', 'åº—', 'é¦†', 'æ¥¼', 'å®¤', 'éƒ¨', 'ç§‘', 'ç»„', 'é˜Ÿ']
                        
                        # å¦‚æœé¢å¤–éƒ¨åˆ†æ˜¯æ˜æ˜¾çš„åŒºåˆ†è¯ï¼Œåˆ™ä¸åº”åˆå¹¶
                        if any(suffix in extra_part for suffix in distinct_suffixes):
                            print(f"Name containment with distinct suffix: '{shorter}' vs '{longer}' - extra part '{extra_part}' indicates different entities")
                            return False
                        
                        # ç‰¹åˆ«æ£€æŸ¥ï¼šå¦‚æœé¢å¤–éƒ¨åˆ†æ˜¯å¸¸è§çš„äººåå­—ç¬¦ï¼Œä¹Ÿä¸åº”åˆå¹¶
                        common_name_chars = set('åæ˜å¼ºä¼Ÿå³°éœå¨Ÿä¸½çº¢æ¢…å…°èŠ³é›¯é›…å©·æ´é™æ•é¢–æ…§ä½³ç¾å›é›„é£ç¿”å®‡è¾‰é¹æ¶›ç£Šåˆšå‹‡å†›æ³¢æ¶›')
                        if any(char in common_name_chars for char in extra_part):
                            print(f"Name containment with name character: '{shorter}' vs '{longer}' - extra part '{extra_part}' likely indicates different person")
                            return False
        
        # ===== æ–°å¢æ£€æŸ¥3: å¼ºåŒ–å®ä½“ç±»å‹å†²çªæ£€æµ‹ =====
        # æ£€æŸ¥å…³é”®è¯å†²çª - é˜²æ­¢æ€§åˆ«ã€ç±»å‹ç­‰å…³é”®å·®å¼‚è¢«å¿½ç•¥
        conflict_keywords = [
            ('ç”·', 'å¥³'), ('ç”·æ€§', 'å¥³æ€§'), ('ç”·ä¸»è§’', 'å¥³ä¸»è§’'),
            ('æœ€ä½³', 'æœ€å·®'), ('è·å¥–', 'æå'), 
            ('ç”µå½±', 'ç”µè§†å‰§'), ('å¯¼æ¼”', 'æ¼”å‘˜'), ('åˆ¶ç‰‡äºº', 'ç¼–å‰§')
        ]
        
        # è¯­ä¹‰å†²çªæ£€æµ‹ - é˜²æ­¢æ¦‚å¿µä¸å…·ä½“å®ä¾‹/äººç‰©çš„é”™è¯¯åˆå¹¶
        semantic_conflicts = [
            # æŠ€æœ¯æ¦‚å¿µ vs äººç‰©ç§°è°“
            ('æ·±åº¦å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ä¹‹çˆ¶'),
            ('äººå·¥æ™ºèƒ½', 'äººå·¥æ™ºèƒ½ä¹‹çˆ¶'),
            ('æœºå™¨å­¦ä¹ ', 'æœºå™¨å­¦ä¹ ä¸“å®¶'),
            # æ¦‚å¿µ vs å‘å±•/å†å²
            ('æ·±åº¦å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ å‘å±•'),
            ('äººå·¥æ™ºèƒ½', 'äººå·¥æ™ºèƒ½å‘å±•'),
            ('äººå·¥æ™ºèƒ½', 'äººå·¥æ™ºèƒ½æ¦‚å¿µ'),
            ('æœºå™¨å­¦ä¹ ', 'æœºå™¨å­¦ä¹ å†å²'),
            ('ç¥ç»ç½‘ç»œ', 'ç¥ç»ç½‘ç»œå†å²'),
            # æŠ€æœ¯ vs æ¦‚å¿µ
            ('æŠ€æœ¯', 'æ¦‚å¿µ'),
            ('ç†è®º', 'å®è·µ'),
            ('æ–¹æ³•', 'ç»“æœ'),
            # æœºæ„ vs äººç‰©
            ('å¤§å­¦', 'æ•™æˆ'),
            ('å…¬å¸', 'åšå£«'),
        ]
        
        # æ‰©å±•çš„å®ä½“ç±»å‹æŒ‡ç¤ºå™¨ - å¢åŠ æ›´å¤šäººç‰©å’Œæœºæ„çš„è¯†åˆ«è¯
        entity_type_indicators = {
            'person': ['ä¹‹çˆ¶', 'ä¹‹æ¯', 'ä¸“å®¶', 'å­¦è€…', 'æ•™æˆ', 'åšå£«', 'å…ˆç”Ÿ', 'å¥³å£«', 'é™¢å£«', 
                      'å‘˜', 'å®¶', 'å¸ˆ', 'é•¿', 'ä¸»ä»»', 'ç»ç†', 'æ€»è£', 'è‘£äº‹', 'ä¸»å¸­'],  # æ–°å¢: å‘˜ã€å®¶ã€å¸ˆç­‰
            'concept': ['æ¦‚å¿µ', 'ç†è®º', 'æ€æƒ³', 'åŸç†', 'å®šä¹‰'],
            'process': ['å‘å±•', 'å†å²', 'æ¼”è¿›', 'è¿‡ç¨‹', 'é˜¶æ®µ', 'è¶‹åŠ¿'],
            'technology': ['æŠ€æœ¯', 'æ–¹æ³•', 'ç®—æ³•', 'æ¨¡å‹', 'ç³»ç»Ÿ'],
            'organization': ['å…¬å¸', 'ä¼ä¸š', 'æœºæ„', 'ç»„ç»‡', 'å›¢é˜Ÿ', 'å®éªŒå®¤', 
                           'é™¢', 'æ‰€', 'ä¼š', 'å¸', 'å‚', 'åº—', 'é¦†', 'éƒ¨', 'ç§‘', 'ç»„', 'é˜Ÿ'],  # æ–°å¢: é™¢ã€æ‰€ã€ä¼šã€å¸ç­‰
            'event': ['ä¼šè®®', 'å¤§ä¼š', 'è®ºå›', 'æ¯”èµ›', 'ç«èµ›', 'æ´»åŠ¨']
        }
        
        # æ£€æŸ¥æ—¥æœŸç²¾åº¦å†²çª - é˜²æ­¢ä¸åŒç²¾åº¦çš„æ—¥æœŸè¢«åˆå¹¶
        date_patterns = ['å¹´', 'æœˆ', 'æ—¥', 'å·']
        
        for entity1 in entities:
            for entity2 in entities:
                if entity1 != entity2:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å†²çªå…³é”®è¯
                    for kw1, kw2 in conflict_keywords:
                        if (kw1 in entity1 and kw2 in entity2) or (kw2 in entity1 and kw1 in entity2):
                            print(f"Conflict detected between '{entity1}' and '{entity2}': {kw1}/{kw2}")
                            return False
                    
                    # æ£€æŸ¥è¯­ä¹‰å†²çª
                    for concept, related in semantic_conflicts:
                        if (concept in entity1 and related in entity2) or (related in entity1 and concept in entity2):
                            print(f"Semantic conflict detected between '{entity1}' and '{entity2}': concept/related entity")
                            return False
                    
                    # å¼ºåŒ–çš„å®ä½“ç±»å‹å†²çªæ£€æŸ¥
                    entity1_types = set()
                    entity2_types = set()
                    
                    for entity_type, indicators in entity_type_indicators.items():
                        for indicator in indicators:
                            if indicator in entity1:
                                entity1_types.add(entity_type)
                            if indicator in entity2:
                                entity2_types.add(entity_type)
                    
                    # å¦‚æœä¸¤ä¸ªå®ä½“è¢«åˆ†ç±»ä¸ºä¸åŒç±»å‹ï¼Œåˆ™ä¸åˆå¹¶
                    if entity1_types and entity2_types and entity1_types.isdisjoint(entity2_types):
                        print(f"Entity type conflict between '{entity1}' ({entity1_types}) and '{entity2}' ({entity2_types})")
                        return False
                    
                    # ç‰¹åˆ«é’ˆå¯¹äººç‰©vsæœºæ„çš„æ£€æŸ¥ - è§£å†³"ç ”ç©¶å‘˜"vs"ç ”ç©¶é™¢"é—®é¢˜
                    person_suffixes = ['å‘˜', 'å®¶', 'å¸ˆ', 'é•¿', 'ä¸»ä»»', 'ç»ç†', 'æ€»è£', 'è‘£äº‹', 'ä¸»å¸­', 'äºº']
                    org_suffixes = ['é™¢', 'æ‰€', 'ä¼š', 'å¸', 'å‚', 'åº—', 'é¦†', 'éƒ¨', 'ç§‘', 'ç»„', 'é˜Ÿ']
                    
                    entity1_is_person = any(entity1.endswith(suffix) for suffix in person_suffixes)
                    entity1_is_org = any(entity1.endswith(suffix) for suffix in org_suffixes)
                    entity2_is_person = any(entity2.endswith(suffix) for suffix in person_suffixes)
                    entity2_is_org = any(entity2.endswith(suffix) for suffix in org_suffixes)
                    
                    # å¦‚æœä¸€ä¸ªæ˜æ˜¾æ˜¯äººç‰©ï¼Œå¦ä¸€ä¸ªæ˜æ˜¾æ˜¯æœºæ„ï¼Œåˆ™ç¦æ­¢åˆå¹¶
                    if (entity1_is_person and entity2_is_org) or (entity1_is_org and entity2_is_person):
                        print(f"Person/Organization conflict between '{entity1}' and '{entity2}'")
                        return False
                    
                    # æ£€æŸ¥æ—¥æœŸç²¾åº¦å†²çªï¼ˆå¦‚"1929å¹´" vs "1929å¹´5æœˆ16æ—¥"ï¼‰
                    entity1_has_date = any(dp in entity1 for dp in date_patterns)
                    entity2_has_date = any(dp in entity2 for dp in date_patterns)
                    
                    if entity1_has_date and entity2_has_date:
                        # éƒ½åŒ…å«æ—¥æœŸå…ƒç´ ï¼Œæ£€æŸ¥ç²¾åº¦æ˜¯å¦ä¸åŒ
                        entity1_precision = 0
                        entity2_precision = 0
                        
                        if 'å¹´' in entity1: entity1_precision += 1
                        if 'æœˆ' in entity1: entity1_precision += 1
                        if 'æ—¥' in entity1 or 'å·' in entity1: entity1_precision += 1
                        
                        if 'å¹´' in entity2: entity2_precision += 1
                        if 'æœˆ' in entity2: entity2_precision += 1
                        if 'æ—¥' in entity2 or 'å·' in entity2: entity2_precision += 1
                        
                        if entity1_precision != entity2_precision:
                            print(f"Date precision conflict between '{entity1}' and '{entity2}': {entity1_precision} vs {entity2_precision}")
                            return False
                    
                    # é•¿åº¦å·®å¼‚è¿‡å¤§çš„ä¸åˆå¹¶ï¼ˆå¯èƒ½æ˜¯ç¼©å†™vså…¨ç§°ä¹‹å¤–çš„æƒ…å†µï¼‰
                    len_ratio = min(len(entity1), len(entity2)) / max(len(entity1), len(entity2))
                    if len_ratio < 0.5:  # é•¿åº¦æ¯”ä¾‹å°äº0.5
                        print(f"Length ratio too small between '{entity1}' and '{entity2}': {len_ratio}")
                        return False
        
        return True

    def _merge_nodes(self, nodes_to_merge, canonical_name):
        """æ‰‹åŠ¨åˆå¹¶MultiDiGraphä¸­çš„èŠ‚ç‚¹"""
        if canonical_name not in nodes_to_merge:
            # å¦‚æœè§„èŒƒåæœ¬èº«ä¸åœ¨å¾…åˆå¹¶åˆ—è¡¨ä¸­ï¼Œå…ˆæ·»åŠ å®ƒçš„å±æ€§
            self.G.add_node(canonical_name, **self.G.nodes[nodes_to_merge[0]])
        
        all_names = set()
        for node_name in nodes_to_merge:
            if node_name in self.G:
                all_names.update(self.G.nodes[node_name].get('names', {node_name}))

        self.G.nodes[canonical_name]['names'] = all_names

        for node_name in nodes_to_merge:
            if node_name == canonical_name or node_name not in self.G:
                continue

            # é‡å®šå‘å…¥è¾¹
            for u, _, key, data in self.G.in_edges(node_name, keys=True, data=True):
                self.G.add_edge(u, canonical_name, key=key, **data)
            
            # é‡å®šå‘å‡ºè¾¹
            for _, v, key, data in self.G.out_edges(node_name, keys=True, data=True):
                self.G.add_edge(canonical_name, v, key=key, **data)
            
            self.G.remove_node(node_name)

    def save_graph(self, filepath=None):
        """å°†NetworkXå›¾ä¿å­˜åˆ°æ–‡ä»¶"""
        filepath = filepath or self.graph_path
        try:
            # ä½¿ç”¨æ›´æ–°çš„æ–¹æ³•ä¿å­˜å›¾
            with open(filepath, 'wb') as f:
                pickle.dump(self.G, f)
            print(f"Graph saved to {filepath}")
        except Exception as e:
            print(f"Error saving graph: {e}")

    def load_graph(self, filepath=None):
        """ä»æ–‡ä»¶åŠ è½½NetworkXå›¾"""
        filepath = filepath or self.graph_path
        try:
            with open(filepath, 'rb') as f:
                self.G = pickle.load(f)
            print(f"Graph loaded from {filepath}")
            return self.G  # è¿”å›åŠ è½½çš„å›¾è°±
        except Exception as e:
            print(f"Error loading graph: {e}")
            self.G = nx.MultiDiGraph()
            return self.G  # è¿”å›ç©ºå›¾è°±
