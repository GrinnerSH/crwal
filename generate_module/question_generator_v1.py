import json
import requests
import random
import os
from datetime import datetime
from config import config
from graph_module.fact_extractor import LLMExtractor

class QuestionGenerator:
    def __init__(self):
        self.llm_extractor = LLMExtractor()

    def _batch_llm_call(self, prompt, max_retries=2):
        """
        æ‰¹é‡è°ƒç”¨LLMï¼Œæ”¯æŒä¸åŒçš„APIå“åº”æ ¼å¼ï¼Œå¸¦é‡è¯•æœºåˆ¶
        """
        for retry in range(max_retries + 1):
            try:
                payload = {
                    "model": config.LLM_MODEL_NAME,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.8,
                    "max_tokens": min(config.LLM_MAX_TOKENS, 4000)
                }
                headers = {'Authorization': f'Bearer {config.LLM_API_KEY}', 'Content-Type': 'application/json'}
                
                # æ ¹æ®prompté•¿åº¦è°ƒæ•´è¶…æ—¶æ—¶é—´
                prompt_length = len(prompt)
                if prompt_length > 10000:
                    timeout = 60
                elif prompt_length > 5000:
                    timeout = 45
                else:
                    timeout = 30
                
                response = requests.post(config.LLM_API_BASE_URL, headers=headers, data=json.dumps(payload), timeout=timeout)
                
                if response.status_code != 200:
                    error_msg = f"APIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
                    print(f"  {error_msg}")
                    if retry < max_retries:
                        print(f"  é‡è¯• {retry + 1}/{max_retries}...")
                        continue
                    return f"APIè°ƒç”¨å¤±è´¥ï¼šHTTP {response.status_code}"
                
                response_data = response.json()
                
                # å°è¯•ä¸åŒçš„å“åº”æ ¼å¼
                if 'choices' in response_data and len(response_data['choices']) > 0:
                    if 'message' in response_data['choices'][0]:
                        content = response_data['choices'][0]['message']['content']
                        if content:
                            return content.strip()
                    elif 'text' in response_data['choices'][0]:
                        content = response_data['choices'][0]['text']
                        if content:
                            return content.strip()
                elif 'data' in response_data:
                    return str(response_data['data']).strip()
                elif 'content' in response_data:
                    return str(response_data['content']).strip()
                
                error_msg = f"æœªçŸ¥çš„APIå“åº”æ ¼å¼: {response_data}"
                print(f"  {error_msg}")
                if retry < max_retries:
                    print(f"  é‡è¯• {retry + 1}/{max_retries}...")
                    continue
                return "APIè°ƒç”¨å¤±è´¥ï¼šæœªçŸ¥å“åº”æ ¼å¼"
                    
            except requests.exceptions.Timeout:
                error_msg = "APIè°ƒç”¨è¶…æ—¶"
                print(f"  {error_msg}")
                if retry < max_retries:
                    print(f"  é‡è¯• {retry + 1}/{max_retries}...")
                    continue
                return "APIè°ƒç”¨å¤±è´¥ï¼šè¯·æ±‚è¶…æ—¶"
            except requests.exceptions.RequestException as e:
                error_msg = f"ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {e}"
                print(f"  {error_msg}")
                if retry < max_retries:
                    print(f"  é‡è¯• {retry + 1}/{max_retries}...")
                    continue
                return f"APIè°ƒç”¨å¤±è´¥ï¼šç½‘ç»œé”™è¯¯ - {str(e)}"
            except json.JSONDecodeError as e:
                error_msg = f"JSONè§£æå¼‚å¸¸: {e}"
                print(f"  {error_msg}")
                if retry < max_retries:
                    print(f"  é‡è¯• {retry + 1}/{max_retries}...")
                    continue
                return "APIè°ƒç”¨å¤±è´¥ï¼šå“åº”æ ¼å¼é”™è¯¯"
            except Exception as e:
                error_msg = f"LLM APIè°ƒç”¨å¤±è´¥: {e}"
                print(f"  {error_msg}")
                if retry < max_retries:
                    print(f"  é‡è¯• {retry + 1}/{max_retries}...")
                    continue
                return f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}"
                
        return "APIè°ƒç”¨å¤±è´¥ï¼šé‡è¯•æ¬¡æ•°è€—å°½"

    def _parse_llm_response(self, response_str, expected_count, prefix="å¥å­", separator=None):
        """
        è§£æLLMè¿”å›çš„å¸¦ç¼–å·çš„å“åº”å­—ç¬¦ä¸² - æ”¹è¿›ç‰ˆæœ¬æ”¯æŒå¤šç§æ ¼å¼å’Œè‡ªå®šä¹‰åˆ†éš”ç¬¦ã€‚
        """
        results = []
        try:
            lines = response_str.strip().split('\n')
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                content_part = None
                # æå–å†’å·æˆ–ç‚¹å·åé¢çš„ä¸»è¦å†…å®¹
                if f'{prefix} ' in line and (':' in line or 'ï¼š' in line):
                    content_part = line.split(':', 1)[-1].strip() if ':' in line else line.split('ï¼š', 1)[-1].strip()
                elif line and line[0].isdigit() and '. ' in line:
                    content_part = line.split('. ', 1)[1].strip()
                elif line.startswith(f'{prefix}'):
                    content_part = line.replace(f'{prefix}', '').lstrip('0123456789: ï¼š.').strip()
                
                if content_part:
                    content_part = content_part.strip('[]"\'')
                    if separator:
                        parts = content_part.split(separator)
                        if len(parts) >= 2:
                            results.append([parts[0].strip(), parts[1].strip()])
                        else:
                            results.append([content_part, "Error: æ ¼å¼ä¸åŒ¹é…"])
                    else:
                        results.append(content_part)

            # å¡«å……ç¼ºå¤±çš„ç»“æœ
            while len(results) < expected_count:
                error_result = ["Error: è§£æå¤±è´¥"] * 2 if separator else "Error: è§£æå¤±è´¥"
                results.append(error_result)
                
            return results[:expected_count]

        except Exception as e:
            print(f"è§£æLLMå“åº”æ—¶å‡ºé”™: {e}")
            print(f"åŸå§‹å“åº”å†…å®¹: {repr(response_str)}")
            error_result = ["Error: è§£æå¼‚å¸¸"] * 2 if separator else "Error: è§£æå¼‚å¸¸"
            return [error_result] * expected_count

    def generate_questions_cascade_new_strategy(self, graph, total_questions=50, batch_size=10):
        """
        å…¨æ–°çš„ä¸‰é˜¶æ®µæ·±åº¦é—®é¢˜ç”Ÿæˆç­–ç•¥ï¼š
        1. å¤šæºçº¿ç´¢èšåˆ - å…ˆç¡®å®šç­”æ¡ˆï¼Œå†å¯»æ‰¾å¤šæ¡ç‹¬ç«‹æ¨ç†è·¯å¾„ä½œä¸ºçº¿ç´¢
        2. æ·±åº¦æ¨¡ç³Šä¸æ•…äº‹ç¼–ç»‡ - å¯¹çº¿ç´¢è¿›è¡ŒåŸå­åŒ–æ¨¡ç³Šï¼Œç„¶åç¼–ç»‡æˆè¿è´¯æ•…äº‹
        3. é—®é¢˜ç”Ÿæˆä¸æ ¡éªŒ - ç”Ÿæˆé—®é¢˜å¹¶è¿›è¡Œç­”æ¡ˆé€»è¾‘æ ¡éªŒ
        
        Args:
            graph: çŸ¥è¯†å›¾è°±å¯¹è±¡
            total_questions: æ€»çš„é—®é¢˜ç”Ÿæˆæ•°é‡
            batch_size: æ‰¹æ¬¡å¤§å°
        """
        print(f"ğŸš€ å¯åŠ¨å…¨æ–°çš„ä¸‰é˜¶æ®µæ·±åº¦é—®é¢˜ç”Ÿæˆç­–ç•¥")
        print(f"ç›®æ ‡é—®é¢˜æ•°é‡ï¼š{total_questions}ï¼Œæ‰¹æ¬¡å¤§å°ï¼š{batch_size}")
        print(f"ç­–ç•¥æµç¨‹ï¼šå¤šæºçº¿ç´¢èšåˆ â†’ æ·±åº¦æ¨¡ç³Šä¸æ•…äº‹ç¼–ç»‡ â†’ é—®é¢˜ç”Ÿæˆä¸æ ¡éªŒ")
        
        # ç¬¬ä¸€é˜¶æ®µï¼šå¤šæºçº¿ç´¢èšåˆ
        print(f"\n=== ç¬¬ä¸€é˜¶æ®µï¼šå¤šæºçº¿ç´¢èšåˆ ===")
        aggregated_samples = self._multi_source_clue_aggregation(graph, total_questions)
        
        if not aggregated_samples:
            print("âŒ çº¿ç´¢èšåˆå¤±è´¥ï¼Œæ— æ³•ç»§ç»­ç”Ÿæˆé—®é¢˜")
            return []
        
        print(f"âœ… çº¿ç´¢èšåˆå®Œæˆï¼Œç”Ÿæˆäº† {len(aggregated_samples)} ä¸ªç­”æ¡ˆ-çº¿ç´¢ç»„åˆ")
        
        # ç¬¬äºŒé˜¶æ®µå’Œç¬¬ä¸‰é˜¶æ®µï¼šåˆ†æ‰¹å¤„ç†
        total_batches = (len(aggregated_samples) + batch_size - 1) // batch_size
        print(f"\nå°† {len(aggregated_samples)} ä¸ªæ ·æœ¬åˆ†ä¸º {total_batches} æ‰¹æ¬¡å¤„ç†")
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(aggregated_samples))
            batch_samples = aggregated_samples[start_idx:end_idx]
            
            print(f"\n=== å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} (æ ·æœ¬ {start_idx+1}-{end_idx}) ===")
            
            # ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦æ¨¡ç³Šä¸æ•…äº‹ç¼–ç»‡
            print(f"ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦æ¨¡ç³Šä¸æ•…äº‹ç¼–ç»‡ (æ‰¹æ¬¡ {batch_idx + 1})")
            self._deep_obfuscation_and_narrative_weaving_batch(batch_samples)
            self._save_stage_results(batch_samples, "stage2_deep_obfuscation", batch_idx + 1)
            
            # ç¬¬ä¸‰é˜¶æ®µï¼šé—®é¢˜ç”Ÿæˆä¸æ ¡éªŒ
            print(f"ç¬¬ä¸‰é˜¶æ®µï¼šé—®é¢˜ç”Ÿæˆä¸æ ¡éªŒ (æ‰¹æ¬¡ {batch_idx + 1})")
            self._question_generation_and_validation_batch(batch_samples)
            self._save_stage_results(batch_samples, "stage3_question_validation", batch_idx + 1)
        
        print(f"\nğŸ‰ å…¨æ–°ä¸‰é˜¶æ®µé—®é¢˜ç”Ÿæˆå®Œæˆï¼Œå…±å¤„ç† {len(aggregated_samples)} ä¸ªæ ·æœ¬")
        return aggregated_samples

    def generate_questions_cascade(self, path_samples, batch_size=10, questions_per_path=2):
        """
        [å·²åºŸå¼ƒ] åŸæœ‰çš„å››é˜¶æ®µé—®é¢˜ç”Ÿæˆçº§è”ï¼Œä¿ç•™ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
        è¯·ä½¿ç”¨æ–°çš„ generate_questions_cascade_new_strategy æ–¹æ³•
        """
        print(f"âš ï¸  æ‚¨æ­£åœ¨ä½¿ç”¨å·²åºŸå¼ƒçš„æ—§ç‰ˆé—®é¢˜ç”Ÿæˆæ–¹æ³•")
        print(f"ğŸ’¡ å»ºè®®ä½¿ç”¨æ–°çš„ä¸‰é˜¶æ®µç­–ç•¥ï¼šgenerate_questions_cascade_new_strategy")
        print(f"å¼€å§‹å¤„ç† {len(path_samples)} ä¸ªè·¯å¾„æ ·æœ¬çš„é—®é¢˜ç”Ÿæˆ")
        print(f"ä¼˜åŒ–ç­–ç•¥ï¼šæ•´åˆé“¾è·¯ -> é€‰æ‹©ç›®æ ‡ -> ã€æƒ…å¢ƒæ¨¡ç³Šä¸é‡æ„ã€‘ -> é—®é¢˜æ„å»º")
        print(f"æ‰¹æ¬¡å¤§å°ï¼š{batch_size}ï¼Œæ¯è·¯å¾„ç”Ÿæˆé—®é¢˜æ•°ï¼š{questions_per_path}")
        
        # ç¬¬ä¸€æ­¥ï¼šæ•´åˆæ¨ç†é“¾è·¯å¹¶ç”Ÿæˆå¤šä¸ªé—®é¢˜æ ·æœ¬
        expanded_samples = []
        for original_sample in path_samples:
            path = original_sample['path']
            if len(path) < 2:  # è·¯å¾„å¤ªçŸ­ï¼Œè·³è¿‡
                continue
                
            # ä»ä¸€æ¡è·¯å¾„ç”Ÿæˆå¤šä¸ªé—®é¢˜æ ·æœ¬
            path_expanded_samples = self._expand_path_to_multiple_questions(
                original_sample, questions_per_path
            )
            expanded_samples.extend(path_expanded_samples)
        
        print(f"è·¯å¾„æ‰©å±•å®Œæˆï¼šä» {len(path_samples)} æ¡è·¯å¾„ç”Ÿæˆäº† {len(expanded_samples)} ä¸ªé—®é¢˜æ ·æœ¬")
        
        # ç¬¬äºŒæ­¥ï¼šåˆ†æ‰¹å¤„ç†æ‰©å±•åçš„æ ·æœ¬
        if expanded_samples:
            total_batches = (len(expanded_samples) + batch_size - 1) // batch_size
            print(f"å°† {len(expanded_samples)} ä¸ªæ ·æœ¬åˆ†ä¸º {total_batches} æ‰¹æ¬¡å¤„ç†")
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(expanded_samples))
                batch_samples = expanded_samples[start_idx:end_idx]
                
                print(f"\n=== å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} (æ ·æœ¬ {start_idx+1}-{end_idx}) ===")
                
                # é˜¶æ®µä¸€ï¼šçº¿ç´¢æ•´åˆä¸é™ˆè¿°ç”Ÿæˆ (ä¿æŒä¸å˜)
                print(f"é˜¶æ®µä¸€ï¼šæ•´åˆçº¿ç´¢ä¿¡æ¯ (æ‰¹æ¬¡ {batch_idx + 1})")
                self._process_clue_integration_batch(batch_samples)
                self._save_stage_results(batch_samples, "stage1_clue_integration", batch_idx + 1)
                
                # é˜¶æ®µäºŒï¼šã€æ–°å¢ã€‘æƒ…å¢ƒæ¨¡ç³Šä¸é‡æ„
                print(f"é˜¶æ®µäºŒï¼šæƒ…å¢ƒæ¨¡ç³Šä¸é‡æ„ (æ‰¹æ¬¡ {batch_idx + 1})")
                self._process_obfuscation_and_weaving_batch(batch_samples)
                self._save_stage_results(batch_samples, "stage2_obfuscation_weaving", batch_idx + 1)
                
                # é˜¶æ®µä¸‰ï¼šæœ€ç»ˆé—®é¢˜æ„å»º
                print(f"é˜¶æ®µä¸‰ï¼šåŸºäºæ¨¡ç³Šæƒ…å¢ƒæ„å»ºé—®é¢˜ (æ‰¹æ¬¡ {batch_idx + 1})")
                self._process_final_question_batch(batch_samples)
                self._save_stage_results(batch_samples, "stage3_final_questions", batch_idx + 1)
        
        print(f"é—®é¢˜ç”Ÿæˆå®Œæˆï¼Œå…±å¤„ç† {len(expanded_samples)} ä¸ªæ ·æœ¬")
        return expanded_samples

    def _multi_source_clue_aggregation(self, graph, target_count):
        """
        ç¬¬ä¸€é˜¶æ®µï¼šå¤šæºçº¿ç´¢èšåˆ
        å…ˆç¡®å®šç­”æ¡ˆå®ä½“ï¼Œç„¶åä»çŸ¥è¯†å›¾è°±ä¸­ä¸ºè¿™ä¸ªç­”æ¡ˆå¯»æ‰¾å¤šæ¡ç›¸äº’ç‹¬ç«‹çš„æ¨ç†è·¯å¾„ä½œä¸ºçº¿ç´¢
        
        Args:
            graph: çŸ¥è¯†å›¾è°±å¯¹è±¡
            target_count: ç›®æ ‡ç”Ÿæˆçš„é—®é¢˜æ•°é‡
            
        Returns:
            list: åŒ…å«ç­”æ¡ˆå’Œå¤šæºçº¿ç´¢çš„æ ·æœ¬åˆ—è¡¨
        """
        print(f"å¼€å§‹å¤šæºçº¿ç´¢èšåˆï¼Œç›®æ ‡æ•°é‡ï¼š{target_count}")
        
        # è·å–å›¾ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹
        all_nodes = list(graph.nodes())
        if len(all_nodes) < 10:
            print(f"âŒ å›¾èŠ‚ç‚¹æ•°é‡è¿‡å°‘ ({len(all_nodes)})ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆçš„çº¿ç´¢èšåˆ")
            return []
        
        aggregated_samples = []
        attempts = 0
        max_attempts = target_count * 10  # å…è®¸æ›´å¤šå°è¯•æ¬¡æ•°
        
        print(f"å›¾ä¸­å…±æœ‰ {len(all_nodes)} ä¸ªèŠ‚ç‚¹ï¼Œå¼€å§‹èšåˆçº¿ç´¢...")
        
        while len(aggregated_samples) < target_count and attempts < max_attempts:
            attempts += 1
            
            # 1. éšæœºé€‰æ‹©ç›®æ ‡å®ä½“ä½œä¸ºç­”æ¡ˆ
            answer_entity = random.choice(all_nodes)
            
            # 2. ä¸ºè¿™ä¸ªç­”æ¡ˆå®ä½“å¯»æ‰¾å¤šæ¡ç‹¬ç«‹çš„æ¨ç†è·¯å¾„
            independent_paths = self._discover_independent_clue_paths(graph, answer_entity)
            
            if len(independent_paths) < 2:  # è‡³å°‘éœ€è¦2æ¡ç‹¬ç«‹è·¯å¾„
                continue
            
            # 3. æ„å»ºçº¿ç´¢åŒ…
            clue_package = self._build_clue_package(answer_entity, independent_paths, graph)
            
            if clue_package:
                aggregated_samples.append(clue_package)
                if len(aggregated_samples) % 10 == 0:
                    print(f"  å·²èšåˆ {len(aggregated_samples)}/{target_count} ä¸ªæ ·æœ¬")
        
        print(f"å¤šæºçº¿ç´¢èšåˆå®Œæˆï¼šæˆåŠŸç”Ÿæˆ {len(aggregated_samples)} ä¸ªæ ·æœ¬ (å°è¯• {attempts} æ¬¡)")
        return aggregated_samples
    
    def _discover_independent_clue_paths(self, graph, answer_entity, max_paths=4, max_path_length=3):
        """
        ä¸ºæŒ‡å®šçš„ç­”æ¡ˆå®ä½“å‘ç°å¤šæ¡ç›¸äº’ç‹¬ç«‹çš„æ¨ç†è·¯å¾„
        
        Args:
            graph: çŸ¥è¯†å›¾è°±å¯¹è±¡
            answer_entity: ç›®æ ‡ç­”æ¡ˆå®ä½“
            max_paths: æœ€å¤§è·¯å¾„æ•°é‡
            max_path_length: å•æ¡è·¯å¾„çš„æœ€å¤§é•¿åº¦
            
        Returns:
            list: ç‹¬ç«‹çš„æ¨ç†è·¯å¾„åˆ—è¡¨
        """
        independent_paths = []
        used_entities = set([answer_entity])  # å·²ä½¿ç”¨çš„å®ä½“ï¼Œé¿å…è·¯å¾„é‡å 
        
        # è·å–ç­”æ¡ˆå®ä½“çš„å‰é©±èŠ‚ç‚¹ï¼ˆæŒ‡å‘å®ƒçš„èŠ‚ç‚¹ï¼‰
        predecessors = list(graph.predecessors(answer_entity))
        
        if len(predecessors) < 2:
            # å¦‚æœå‰é©±å¤ªå°‘ï¼Œå°è¯•ä»ç­”æ¡ˆå®ä½“å‡ºå‘çš„è·¯å¾„
            successors = list(graph.successors(answer_entity))
            if len(successors) >= 2:
                # æ„å»ºä»ç­”æ¡ˆå®ä½“å‡ºå‘çš„è·¯å¾„
                for successor in successors[:max_paths]:
                    if successor not in used_entities:
                        path = self._extend_path_from_node(graph, answer_entity, successor, max_path_length)
                        if path and len(path) >= 2:
                            independent_paths.append(path)
                            used_entities.update(path)
                            if len(independent_paths) >= max_paths:
                                break
            return independent_paths
        
        # ä»å‰é©±èŠ‚ç‚¹å¼€å§‹æ„å»ºæŒ‡å‘ç­”æ¡ˆçš„è·¯å¾„
        random.shuffle(predecessors)  # éšæœºåŒ–é¡ºåº
        
        for pred_node in predecessors:
            if pred_node in used_entities:
                continue
                
            # æ„å»ºä»æŸä¸ªèµ·ç‚¹åˆ°ç­”æ¡ˆå®ä½“çš„è·¯å¾„
            clue_path = self._build_clue_path_to_answer(graph, pred_node, answer_entity, max_path_length)
            
            if clue_path and len(clue_path) >= 2:
                # æ£€æŸ¥è·¯å¾„æ˜¯å¦ä¸å·²æœ‰è·¯å¾„ç‹¬ç«‹ï¼ˆæ²¡æœ‰å…±åŒçš„ä¸­é—´èŠ‚ç‚¹ï¼‰
                path_entities = set(clue_path[:-1])  # é™¤äº†ç­”æ¡ˆå®ä½“å¤–çš„æ‰€æœ‰èŠ‚ç‚¹
                if not path_entities.intersection(used_entities):
                    independent_paths.append(clue_path)
                    used_entities.update(path_entities)
                    
                    if len(independent_paths) >= max_paths:
                        break
        
        return independent_paths
    
    def _extend_path_from_node(self, graph, start_node, next_node, max_length):
        """ä»æŒ‡å®šèŠ‚ç‚¹å¼€å§‹æ‰©å±•è·¯å¾„"""
        path = [start_node, next_node]
        current_node = next_node
        
        for _ in range(max_length - 2):
            successors = list(graph.successors(current_node))
            if not successors:
                break
            
            # é¿å…å›åˆ°è·¯å¾„ä¸­å·²æœ‰çš„èŠ‚ç‚¹
            available_successors = [n for n in successors if n not in path]
            if not available_successors:
                break
            
            next_node = random.choice(available_successors)
            path.append(next_node)
            current_node = next_node
        
        return path
    
    def _build_clue_path_to_answer(self, graph, start_node, answer_entity, max_length):
        """æ„å»ºä»èµ·ç‚¹åˆ°ç­”æ¡ˆå®ä½“çš„çº¿ç´¢è·¯å¾„"""
        try:
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä»start_nodeåˆ°answer_entityçš„è·¯å¾„
            if not graph.has_node(start_node) or not graph.has_node(answer_entity):
                return None
            
            # å°è¯•æ‰¾åˆ°æœ€çŸ­è·¯å¾„
            if hasattr(graph, 'has_path') and graph.has_path(start_node, answer_entity):
                import networkx as nx
                shortest_path = nx.shortest_path(graph, start_node, answer_entity)
                if len(shortest_path) <= max_length:
                    return shortest_path
            
            # å¦‚æœæ²¡æœ‰ç›´æ¥è·¯å¾„æˆ–è·¯å¾„å¤ªé•¿ï¼Œå°è¯•æ„å»ºé—´æ¥è·¯å¾„
            path = [start_node]
            current_node = start_node
            
            for _ in range(max_length - 1):
                successors = list(graph.successors(current_node))
                if answer_entity in successors:
                    # æ‰¾åˆ°äº†åˆ°ç­”æ¡ˆçš„ç›´æ¥è¿æ¥
                    path.append(answer_entity)
                    return path
                
                if not successors:
                    break
                
                # é€‰æ‹©ä¸€ä¸ªåç»§èŠ‚ç‚¹ç»§ç»­
                available_successors = [n for n in successors if n not in path]
                if not available_successors:
                    break
                
                current_node = random.choice(available_successors)
                path.append(current_node)
            
            return None
            
        except Exception as e:
            print(f"æ„å»ºçº¿ç´¢è·¯å¾„æ—¶å‡ºé”™: {e}")
            return None
    
    def _build_clue_package(self, answer_entity, independent_paths, graph=None):
        """
        æ„å»ºçº¿ç´¢åŒ…ï¼šå°†å¤šæ¡ç‹¬ç«‹è·¯å¾„æ•´åˆä¸ºä¸€ä¸ªé—®é¢˜ç”Ÿæˆæ ·æœ¬
        
        Args:
            answer_entity: ç­”æ¡ˆå®ä½“
            independent_paths: å¤šæ¡ç‹¬ç«‹çš„æ¨ç†è·¯å¾„
            graph: çŸ¥è¯†å›¾è°±å¯¹è±¡ï¼ˆç”¨äºæå–çœŸå®å…³ç³»ï¼‰
            
        Returns:
            dict: åŒ…å«ç­”æ¡ˆå’Œçº¿ç´¢ä¿¡æ¯çš„æ ·æœ¬
        """
        if not independent_paths:
            return None
        
        # æå–æ‰€æœ‰çº¿ç´¢å®ä½“ï¼ˆè·¯å¾„ç»ˆç‚¹ï¼Œé™¤äº†ç­”æ¡ˆå®ä½“ï¼‰
        clue_entities = []
        clue_paths_info = []
        
        for i, path in enumerate(independent_paths):
            if len(path) >= 2:
                # è·¯å¾„ä¿¡æ¯
                path_triples = self._convert_path_to_triples(path, graph)
                clue_paths_info.append({
                    'path_id': i + 1,
                    'path_nodes': path,
                    'path_triples': path_triples,
                    'clue_entity': path[-1] if path[-1] != answer_entity else path[-2]
                })
                
                # çº¿ç´¢å®ä½“ï¼ˆè·¯å¾„ä¸­é™¤ç­”æ¡ˆå¤–çš„å…³é”®å®ä½“ï¼‰
                if path[-1] != answer_entity:
                    clue_entities.append(path[-1])
                elif len(path) >= 2:
                    clue_entities.append(path[-2])
        
        if not clue_entities:
            return None
        
        # ç”Ÿæˆæ ·æœ¬ID
        sample_id = f"multi_clue_{hash(answer_entity)}_{len(independent_paths)}"
        
        clue_package = {
            'sample_id': sample_id,
            'strategy': 'multi_source_clue_aggregation',
            'answer_entity': answer_entity,
            'independent_paths': independent_paths,
            'clue_entities': clue_entities,
            'clue_paths_info': clue_paths_info,
            'num_clue_paths': len(independent_paths),
            'generation_timestamp': datetime.now().isoformat(),
            'status': 'aggregated'
        }
        
        return clue_package
    
    def _convert_path_to_triples(self, path, graph=None):
        """å°†èŠ‚ç‚¹è·¯å¾„è½¬æ¢ä¸ºä¸‰å…ƒç»„æ ¼å¼"""
        if len(path) < 2:
            return []
        
        triples = []
        for i in range(len(path) - 1):
            subject = path[i]
            obj = path[i + 1]
            
            # å¦‚æœæä¾›äº†å›¾å¯¹è±¡ï¼Œå°è¯•è·å–çœŸå®çš„å…³ç³»
            if graph and hasattr(graph, 'get_edge_data'):
                edge_data = graph.get_edge_data(subject, obj)
                if edge_data:
                    # ä»è¾¹æ•°æ®ä¸­é€‰æ‹©ä¸€ä¸ªå…³ç³»
                    if isinstance(edge_data, dict):
                        relations = list(edge_data.keys())
                        relation = relations[0] if relations else 'ç›¸å…³'
                    else:
                        relation = 'ç›¸å…³'
                else:
                    relation = 'ç›¸å…³'
            else:
                relation = 'ç›¸å…³'  # é»˜è®¤å…³ç³»
            
            triple = {
                'subject': subject,
                'relation': relation,
                'object': obj
            }
            triples.append(triple)
        
        return triples

    def _expand_path_to_multiple_questions(self, original_sample, questions_per_path):
        """
        ä»ä¸€æ¡æ¨ç†è·¯å¾„ç”Ÿæˆå¤šä¸ªé—®é¢˜æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬é€‰æ‹©ä¸åŒçš„ç›®æ ‡å®ä½“
        """
        path = original_sample['path']
        expanded_samples = []
        
        # æ”¶é›†è·¯å¾„ä¸­çš„æ‰€æœ‰å®ä½“ï¼ˆå»é‡ï¼‰
        all_entities = []
        all_relations = []
        
        for triple in path:
            all_entities.extend([triple['subject'], triple['object']])
            all_relations.append({
                'relation': triple['relation'],
                'subject': triple['subject'],
                'object': triple['object']
            })
        
        # å»é‡ä½†ä¿æŒé¡ºåº
        unique_entities = list(dict.fromkeys(all_entities))
        
        # å¦‚æœå®ä½“æ•°é‡ä¸è¶³ï¼Œç›´æ¥è¿”å›åŸºç¡€æ ·æœ¬
        if len(unique_entities) < 2:
            return [original_sample]
        
        # éšæœºé€‰æ‹©ç›®æ ‡å®ä½“ï¼ˆç¡®ä¿ç”Ÿæˆè¶³å¤Ÿçš„æ ·æœ¬ï¼‰
        target_entities = []
        if len(unique_entities) >= questions_per_path:
            target_entities = random.sample(unique_entities, questions_per_path)
        else:
            # å¦‚æœå®ä½“ä¸å¤Ÿï¼Œè¿›è¡Œé‡å¤é‡‡æ ·
            target_entities = random.choices(unique_entities, k=questions_per_path)
        
        # ä¸ºæ¯ä¸ªç›®æ ‡å®ä½“ç”Ÿæˆä¸€ä¸ªæ ·æœ¬
        for i, target_entity in enumerate(target_entities):
            # å¯»æ‰¾ä¸ç›®æ ‡å®ä½“ç›¸å…³çš„å…³ç³»
            target_relations = []
            for rel_info in all_relations:
                if rel_info['subject'] == target_entity or rel_info['object'] == target_entity:
                    if rel_info['object'] == target_entity:
                        target_relations.append(rel_info['relation'])
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³å…³ç³»ï¼Œä½¿ç”¨é»˜è®¤å…³ç³»
            if not target_relations:
                target_relations = ['ç›¸å…³å±æ€§']
            
            # æ„å»ºçº¿ç´¢ä¿¡æ¯ï¼ˆæ’é™¤ç›®æ ‡å®ä½“çš„ä¸‰å…ƒç»„ï¼‰
            clue_triples = []
            target_triple = None
            
            for triple in path:
                if triple['object'] == target_entity:
                    target_triple = triple
                else:
                    clue_triples.append(triple)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›®æ ‡ä¸‰å…ƒç»„ï¼Œä½¿ç”¨è·¯å¾„ä¸­çš„æœ€åä¸€ä¸ª
            if target_triple is None and path:
                target_triple = path[-1]
                target_entity = target_triple['object']
                clue_triples = path[:-1]
            
            # åˆ›å»ºæ‰©å±•æ ·æœ¬
            expanded_sample = {
                'sample_id': f"{original_sample.get('sample_id', 'unknown')}_{i+1}",
                'original_path': path,
                'target_entity': target_entity,
                'target_relation': target_triple['relation'] if target_triple else target_relations[0],
                'clue_triples': clue_triples,
                'clue_entities': [entity for entity in unique_entities if entity != target_entity],
                
                # ä¿ç•™åŸå§‹ä¿¡æ¯
                'original_reasoning_path': [
                    {'step': j + 1, 'subject': triple['subject'], 'relation': triple['relation'], 'object': triple['object']} 
                    for j, triple in enumerate(path)
                ],
                'reasoning_chain_raw': " -> ".join([f"({t['subject']},{t['relation']},{t['object']})" for t in path]),
                
                # æ–°å¢å­—æ®µ
                'question_generation_strategy': 'random_target_selection',
                'target_selection_method': f'random_from_{len(unique_entities)}_entities',
                'clue_count': len(clue_triples)
            }
            
            expanded_samples.append(expanded_sample)
        
        return expanded_samples

    def _process_clue_integration_batch(self, batch_samples):
        """
        ã€æ–°é˜¶æ®µä¸€ã€‘ï¼šçº¿ç´¢æ•´åˆä¸é™ˆè¿°ç”Ÿæˆ - å°†çº¿ç´¢ä¸‰å…ƒç»„æ•´åˆä¸ºè¿è´¯çš„èƒŒæ™¯é™ˆè¿°
        """
        prompt = """ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æ•´åˆä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹æ¯ç»„äº‹å®ä¸‰å…ƒç»„æ•´åˆä¸ºä¸€æ®µè¿è´¯çš„èƒŒæ™¯é™ˆè¿°ï¼Œè¦æ±‚ï¼š

1. å°†æ‰€æœ‰ç»™å®šçš„äº‹å®ä¿¡æ¯è‡ªç„¶åœ°ç»„ç»‡åœ¨ä¸€èµ·
2. è¯­è¨€æµç•…ã€é€»è¾‘æ¸…æ™°ï¼Œé¿å…ç®€å•çš„å †ç Œ
3. ä¿æŒå®¢è§‚ä¸­æ€§ï¼Œä¸åšé¢å¤–æ¨ç†æˆ–æš—ç¤º
4. ç¡®ä¿æ‰€æœ‰å…³é”®å®ä½“å’Œå…³ç³»éƒ½å¾—åˆ°ä½“ç°
5. ä¸è¦æåŠæˆ–æš—ç¤ºä»»ä½•æœªåœ¨çº¿ç´¢ä¸­å‡ºç°çš„ä¿¡æ¯

ç°åœ¨ï¼Œè¯·å¤„ç†ä»¥ä¸‹çº¿ç´¢ç»„åˆï¼š
"""
        
        for i, sample in enumerate(batch_samples):
            clue_triples = sample['clue_triples']
            if clue_triples:
                clue_descriptions = []
                for triple in clue_triples:
                    clue_descriptions.append(f"({triple['subject']}, {triple['relation']}, {triple['object']})")
                clue_str = " + ".join(clue_descriptions)
                prompt += f"\nçº¿ç´¢ç»„åˆ {i+1}: {clue_str}"
            else:
                prompt += f"\nçº¿ç´¢ç»„åˆ {i+1}: (æš‚æ— å…·ä½“çº¿ç´¢)"
        
        prompt += "\n\nè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›ï¼Œæ¯ä¸ªé™ˆè¿°å ä¸€è¡Œ:\né™ˆè¿° 1: [æ•´åˆåçš„èƒŒæ™¯é™ˆè¿°]\né™ˆè¿° 2: [æ•´åˆåçš„èƒŒæ™¯é™ˆè¿°]\n..."

        results_str = self._batch_llm_call(prompt)
        if results_str and not results_str.startswith("APIè°ƒç”¨å¤±è´¥"):
            parsed_statements = self._parse_llm_response(results_str, len(batch_samples), "é™ˆè¿°")
            success_count = 0
            for i, sample in enumerate(batch_samples):
                if i < len(parsed_statements) and not parsed_statements[i].startswith("Error"):
                    sample['integrated_clue_statement'] = parsed_statements[i]
                    success_count += 1
                else:
                    # å›é€€ï¼šåŸºäºçº¿ç´¢å®ä½“çš„ç®€åŒ–æè¿°
                    clue_entities = sample.get('clue_entities', [])
                    if clue_entities:
                        sample['integrated_clue_statement'] = f"å·²çŸ¥å®ä½“{clue_entities[0]}ä¸å…¶ä»–ç›¸å…³å®ä½“å­˜åœ¨å¤šç§å…³è”å…³ç³»"
                    else:
                        sample['integrated_clue_statement'] = "å·²çŸ¥å­˜åœ¨ä¸€ç³»åˆ—ç›¸å…³çš„å®ä½“å…³ç³»"
            
            print(f"  é˜¶æ®µä¸€å®Œæˆï¼Œçº¿ç´¢æ•´åˆæˆåŠŸå¤„ç† {success_count}/{len(batch_samples)} ä¸ªæ ·æœ¬")
        else:
            print(f"  é˜¶æ®µä¸€APIè°ƒç”¨å¤±è´¥: {results_str}")
            # å…¨éƒ¨å›é€€å¤„ç†
            for sample in batch_samples:
                clue_entities = sample.get('clue_entities', [])
                if clue_entities:
                    sample['integrated_clue_statement'] = f"å·²çŸ¥å®ä½“{clue_entities[0]}ä¸å…¶ä»–ç›¸å…³å®ä½“å­˜åœ¨å¤šç§å…³è”å…³ç³»"
                else:
                    sample['integrated_clue_statement'] = "å·²çŸ¥å­˜åœ¨ä¸€ç³»åˆ—ç›¸å…³çš„å®ä½“å…³ç³»"

    def _deep_obfuscation_and_narrative_weaving_batch(self, batch_samples):
        """
        ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦æ¨¡ç³Šä¸æ•…äº‹ç¼–ç»‡ï¼ˆå…¨æ–°å®ç°ï¼‰
        
        è¿™ä¸ªé˜¶æ®µå°†ï¼š
        1. å¯¹æ¯ä¸ªçº¿ç´¢å®ä½“è¿›è¡Œ"åŸå­åŒ–æ¨¡ç³Š"å¤„ç†
        2. å°†æ‰€æœ‰æ¨¡ç³ŠåŒ–çš„æè¿°ç¼–ç»‡æˆä¸€ä¸ªè¿è´¯çš„æ‚¬ç–‘æ•…äº‹
        3. ç¡®ä¿æ•…äº‹æŒ‡å‘å…±åŒç›®æ ‡ä½†ä¸æ³„éœ²ç­”æ¡ˆ
        """
        print(f"  å¼€å§‹æ·±åº¦æ¨¡ç³Šä¸æ•…äº‹ç¼–ç»‡å¤„ç†...")
        
        # ç¬¬ä¸€æ­¥ï¼šå¯¹æ‰€æœ‰çº¿ç´¢å®ä½“è¿›è¡ŒåŸå­åŒ–æ¨¡ç³Š
        self._atomic_obfuscation_batch(batch_samples)
        
        # ç¬¬äºŒæ­¥ï¼šå°†æ¨¡ç³Šæè¿°ç¼–ç»‡æˆæ•…äº‹
        self._narrative_weaving_batch(batch_samples)
        
        print(f"  æ·±åº¦æ¨¡ç³Šä¸æ•…äº‹ç¼–ç»‡å®Œæˆ")
    
    def _atomic_obfuscation_batch(self, batch_samples):
        """
        åŸå­åŒ–æ¨¡ç³Šï¼šå¯¹æ¯ä¸ªçº¿ç´¢å®ä½“è¿›è¡Œç‹¬ç«‹çš„ã€ä¸Šä¸‹æ–‡æ— å…³çš„æ¨¡ç³ŠåŒ–å¤„ç†
        """
        print(f"    æ‰§è¡ŒåŸå­åŒ–æ¨¡ç³Šå¤„ç†...")
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦æ¨¡ç³ŠåŒ–çš„å®ä½“
        all_entities_to_obfuscate = []
        entity_to_samples = {}  # å®ä½“åˆ°æ ·æœ¬çš„æ˜ å°„
        
        for sample in batch_samples:
            clue_entities = sample.get('clue_entities', [])
            for entity in clue_entities:
                if entity not in all_entities_to_obfuscate:
                    all_entities_to_obfuscate.append(entity)
                
                if entity not in entity_to_samples:
                    entity_to_samples[entity] = []
                entity_to_samples[entity].append(sample)
        
        if not all_entities_to_obfuscate:
            print(f"    è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¨¡ç³ŠåŒ–çš„å®ä½“")
            for sample in batch_samples:
                sample['obfuscated_entities'] = {}
            return
        
        # æ‰¹é‡å¯¹å®ä½“è¿›è¡Œæ¨¡ç³ŠåŒ–
        prompt = """ä½ æ˜¯ä¸€ä½ä¿¡æ¯æŠ½è±¡ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹æ¯ä¸ªå®ä½“ç”Ÿæˆä¸€ä¸ªæ¨¡ç³Šä½†å…·æœ‰å”¯ä¸€æŒ‡å‘æ€§çš„æè¿°ã€‚

è¦æ±‚ï¼š
1. æè¿°ä¸­ä¸èƒ½å‡ºç°è¯¥å®ä½“çš„å…·ä½“åç§°
2. æè¿°è¦è¶³å¤Ÿæ¨¡ç³Šï¼Œä½†åˆè¦èƒ½å”¯ä¸€æŒ‡å‘è¯¥å®ä½“
3. ä½¿ç”¨é€šç”¨çš„ã€æè¿°æ€§çš„è¯­è¨€
4. æ¯ä¸ªæè¿°æ§åˆ¶åœ¨15å­—ä»¥å†…

ç¤ºä¾‹ï¼š
- é™ˆæ™“å¿ -> ä¸€ä½è‘—åçš„ç¾é£Ÿçºªå½•ç‰‡å¯¼æ¼”
- ä¸­å›½ä¼ åª’å¤§å­¦ -> ä¸€æ‰€ä»¥ä¿¡æ¯ä¼ æ’­é—»åçš„é«˜ç­‰å­¦åºœ
- æç«‹å® -> ä¸€ä½å£°éŸ³æµ‘åšçš„å›½å®¶çº§é…éŸ³æ¼”å‘˜

ç°åœ¨è¯·å¤„ç†ä»¥ä¸‹å®ä½“ï¼š
"""
        
        for i, entity in enumerate(all_entities_to_obfuscate):
            prompt += f"\nå®ä½“ {i+1}: {entity}"
        
        prompt += "\n\nè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›ï¼Œæ¯ä¸ªæè¿°å ä¸€è¡Œ:\næè¿° 1: [æ¨¡ç³ŠåŒ–æè¿°]\næè¿° 2: [æ¨¡ç³ŠåŒ–æè¿°]\n..."
        
        results_str = self._batch_llm_call(prompt)
        
        # è§£æç»“æœå¹¶åˆ†é…ç»™æ ·æœ¬
        obfuscated_descriptions = {}
        
        if results_str and not results_str.startswith("APIè°ƒç”¨å¤±è´¥"):
            parsed_descriptions = self._parse_llm_response(results_str, len(all_entities_to_obfuscate), "æè¿°")
            
            for i, entity in enumerate(all_entities_to_obfuscate):
                if i < len(parsed_descriptions) and not parsed_descriptions[i].startswith("Error"):
                    obfuscated_descriptions[entity] = parsed_descriptions[i]
                else:
                    # å›é€€ï¼šç®€å•çš„é€šç”¨æè¿°
                    obfuscated_descriptions[entity] = f"ä¸€ä¸ªä¸{entity[:2]}ç›¸å…³çš„å®ä½“"
        else:
            print(f"    åŸå­åŒ–æ¨¡ç³ŠAPIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å›é€€ç­–ç•¥")
            for entity in all_entities_to_obfuscate:
                obfuscated_descriptions[entity] = f"ä¸€ä¸ªç›¸å…³å®ä½“"
        
        # å°†æ¨¡ç³ŠåŒ–ç»“æœåˆ†é…ç»™å„ä¸ªæ ·æœ¬
        for sample in batch_samples:
            sample['obfuscated_entities'] = {}
            clue_entities = sample.get('clue_entities', [])
            for entity in clue_entities:
                if entity in obfuscated_descriptions:
                    sample['obfuscated_entities'][entity] = obfuscated_descriptions[entity]
        
        success_count = len([s for s in batch_samples if s.get('obfuscated_entities')])
        print(f"    åŸå­åŒ–æ¨¡ç³Šå®Œæˆï¼š{success_count}/{len(batch_samples)} ä¸ªæ ·æœ¬")
    
    def _narrative_weaving_batch(self, batch_samples):
        """
        æ•…äº‹ç¼–ç»‡ï¼šå°†æ¨¡ç³ŠåŒ–çš„æè¿°ç¼–ç»‡æˆè¿è´¯çš„æ‚¬ç–‘æ•…äº‹
        """
        print(f"    æ‰§è¡Œæ•…äº‹ç¼–ç»‡...")
        
        valid_samples = []
        for sample in batch_samples:
            obfuscated_entities = sample.get('obfuscated_entities', {})
            if obfuscated_entities:
                valid_samples.append(sample)
        
        if not valid_samples:
            print(f"    è­¦å‘Šï¼šæ²¡æœ‰æœ‰æ•ˆçš„æ¨¡ç³ŠåŒ–å®ä½“å¯ä¾›æ•…äº‹ç¼–ç»‡")
            for sample in batch_samples:
                sample['narrative_story'] = "å­˜åœ¨ä¸€äº›ç›¸å…³çš„å®ä½“å’Œå…³ç³»"
            return
        
        # ä¸ºæ¯ä¸ªæ ·æœ¬å•ç‹¬ç¼–ç»‡æ•…äº‹
        for sample in valid_samples:
            self._weave_single_narrative(sample)
        
        success_count = len([s for s in valid_samples if s.get('narrative_story')])
        print(f"    æ•…äº‹ç¼–ç»‡å®Œæˆï¼š{success_count}/{len(valid_samples)} ä¸ªæ ·æœ¬")
    
    def _weave_single_narrative(self, sample):
        """
        ä¸ºå•ä¸ªæ ·æœ¬ç¼–ç»‡æ•…äº‹
        """
        obfuscated_entities = sample.get('obfuscated_entities', {})
        answer_entity = sample.get('answer_entity', '')
        
        if not obfuscated_entities:
            sample['narrative_story'] = "å­˜åœ¨ä¸€äº›ç›¸å…³çš„å®ä½“å’Œå…³ç³»"
            return
        
        # æ„å»ºæ•…äº‹ç¼–ç»‡çš„prompt
        prompt = f"""ä½ æ˜¯ä¸€ä½é€»è¾‘ä¸¥è°¨çš„è°œé¢˜æ„å»ºä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä¸‹é¢æä¾›çš„ä¸€ç³»åˆ—å…³äº**åŒä¸€ä¸ªæœªçŸ¥ç›®æ ‡**çš„æ¨¡ç³Šæè¿°ï¼Œå°†å®ƒä»¬æ•´åˆå¹¶æ”¹å†™æˆä¸€æ®µä¿¡æ¯é™ˆè¿°ã€‚è¿™æ®µé™ˆè¿°å°†ä½œä¸ºä¸€é“è°œé¢˜çš„é¢˜å¹²ã€‚

è¯·éµå¾ªä»¥ä¸‹æ ¸å¿ƒå‡†åˆ™ï¼š
1.  **å®¢è§‚æ•´åˆ**ï¼šå°†æ‰€æœ‰æ¨¡ç³Šæè¿°ä½œä¸ºç‹¬ç«‹çš„çº¿ç´¢ï¼Œå®¢è§‚åœ°æ•´åˆè¿›ä¸€æ®µè¯ä¸­ã€‚ä¸“æ³¨äºäº‹å®çš„å‘ˆç°ï¼Œè€Œä¸æ˜¯åˆ›é€ ä¸€ä¸ªæ•…äº‹ã€‚æ‰€æœ‰çº¿ç´¢*æœ€ç»ˆ***éƒ½æè¿°çš„æ˜¯**åŒä¸€ä¸ªä¸»ä½“**ï¼ˆå¯èƒ½æ˜¯ä¸€ä¸ªäººã€ä¸€ä¸ªåœ°æ–¹ã€ä¸€ä¸ªæ¦‚å¿µç­‰ï¼‰ã€‚ä½ å¿…é¡»å°†å®ƒä»¬æ•´åˆèµ·æ¥ï¼Œå…±åŒæŒ‡å‘è¿™ä¸ªå”¯ä¸€çš„ã€æœªçŸ¥çš„ç›®æ ‡ã€‚
2.  **é¿å…æ–‡å­¦åŒ–**ï¼šä¸è¦ä½¿ç”¨æ¯”å–»ã€æ‹Ÿäººã€å¤¸å¼ ç­‰æ–‡å­¦ä¿®è¾æ‰‹æ³•ã€‚è¯­è¨€é£æ ¼åº”ä¿æŒä¸­ç«‹ã€ç®€æ´ã€ç›´æ¥ï¼Œç±»ä¼¼äºç™¾ç§‘å…¨ä¹¦æˆ–äº‹å®æŠ¥å‘Šã€‚
3.  **å»ºç«‹é€»è¾‘å…³è”**ï¼šç¡®ä¿å„ä¸ªçº¿ç´¢ä¹‹é—´å­˜åœ¨å†…åœ¨çš„é€»è¾‘è”ç³»ï¼Œå…±åŒæŒ‡å‘ä¸€ä¸ªæœ€ç»ˆç­”æ¡ˆï¼Œä½†ä¸è¦ç›´æ¥æ­ç¤ºç­”æ¡ˆã€‚
4.  **ä¿¡æ¯å®Œæ•´**ï¼šç¡®ä¿æ‰€æœ‰æä¾›çš„æ¨¡ç³Šæè¿°éƒ½ä»¥æŸç§å½¢å¼å‡ºç°åœ¨æœ€ç»ˆçš„é™ˆè¿°ä¸­ã€‚
5.  **èšç„¦é—®é¢˜**ï¼šæœ€ç»ˆç”Ÿæˆçš„æ–‡æœ¬åº”è¯¥æ˜¯ä¸€æ®µæ¸…æ™°çš„ã€ç”¨äºå¼•å‡ºé—®é¢˜çš„èƒŒæ™¯ä¿¡æ¯é™ˆè¿°ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ•…äº‹ã€‚

**ç¤ºä¾‹è¾“å…¥ï¼š**
-   æ¨¡ç³Šæè¿°1: ä¸€ä½è‘—åçš„ç¾é£Ÿçºªå½•ç‰‡å¯¼æ¼”
-   æ¨¡ç³Šæè¿°2: ä¸€æ‰€ä»¥ä¿¡æ¯ä¼ æ’­é—»åçš„é«˜ç­‰å­¦åºœ
-   æ¨¡ç³Šæè¿°3: ä¸€ä½å£°éŸ³æµ‘åšçš„å›½å®¶çº§é…éŸ³æ¼”å‘˜

**ç¬¦åˆè¦æ±‚çš„è¾“å‡ºç¤ºä¾‹ï¼š**
â€œä¸€ä½è‘—åçš„ç¾é£Ÿçºªå½•ç‰‡å¯¼æ¼”ï¼Œæ¯•ä¸šäºä¸€æ‰€ä»¥ä¿¡æ¯ä¼ æ’­é—»åçš„é«˜ç­‰å­¦åºœã€‚ä»–å¯¼æ¼”çš„ä¸€éƒ¨çŸ¥åçºªå½•ç‰‡ï¼Œå…¶æ—ç™½ç”±ä¸€ä½å£°éŸ³æµ‘åšçš„å›½å®¶çº§é…éŸ³æ¼”å‘˜å®Œæˆã€‚â€

ç°åœ¨ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æä¾›çš„æ¨¡ç³Šæè¿°ï¼Œä¸ºæˆ‘æ„å»ºä¿¡æ¯é™ˆè¿°ï¼š

"""
        
        descriptions = list(obfuscated_entities.values())
        for i, desc in enumerate(descriptions):
            prompt += f"{i+1}. {desc}\n"
        
        
        result = self._batch_llm_call(prompt)
        
        if result and not result.startswith("APIè°ƒç”¨å¤±è´¥"):
            sample['narrative_story'] = result.strip()
        else:
            # å›é€€ï¼šç®€å•è¿æ¥æ‰€æœ‰æè¿°
            story = "ï¼Œ".join(descriptions)
            sample['narrative_story'] = f"è¿™é‡Œæ¶‰åŠ{story}ç­‰ç›¸å…³ä¿¡æ¯ã€‚"

    def _question_generation_and_validation_batch(self, batch_samples):
        """
        ç¬¬ä¸‰é˜¶æ®µï¼šé—®é¢˜ç”Ÿæˆä¸æ ¡éªŒï¼ˆå…¨æ–°å®ç°ï¼‰
        
        è¿™ä¸ªé˜¶æ®µå°†ï¼š
        1. åŸºäºæ•…äº‹ç”Ÿæˆé—®é¢˜
        2. å¯¹ç”Ÿæˆçš„é—®é¢˜è¿›è¡Œç­”æ¡ˆé€»è¾‘æ ¡éªŒ
        3. ç¡®ä¿é—®é¢˜å¯ä»¥é€šè¿‡æ¨ç†å¾—åˆ°æ­£ç¡®ç­”æ¡ˆ
        """
        print(f"  å¼€å§‹é—®é¢˜ç”Ÿæˆä¸æ ¡éªŒ...")
        
        # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆé—®é¢˜
        self._generate_questions_from_narrative_batch(batch_samples)
        
        # ç¬¬äºŒæ­¥ï¼šæ ¡éªŒé—®é¢˜
        self._validate_questions_batch(batch_samples)
        
        print(f"  é—®é¢˜ç”Ÿæˆä¸æ ¡éªŒå®Œæˆ")
    
    def _generate_questions_from_narrative_batch(self, batch_samples):
        """
        åŸºäºæ•…äº‹èƒŒæ™¯ç”Ÿæˆé—®é¢˜
        """
        print(f"    æ‰§è¡Œé—®é¢˜ç”Ÿæˆ...")
        
        valid_samples = []
        for sample in batch_samples:
            narrative_story = sample.get('narrative_story', '')
            answer_entity = sample.get('answer_entity', '')
            if narrative_story and not narrative_story.startswith("Error") and answer_entity:
                valid_samples.append(sample)
        
        if not valid_samples:
            print(f"    è­¦å‘Šï¼šæ²¡æœ‰æœ‰æ•ˆçš„æ•…äº‹èƒŒæ™¯å¯ä¾›é—®é¢˜ç”Ÿæˆ")
            for sample in batch_samples:
                sample['generated_question'] = "è¿™ä¸ªå®ä½“æ˜¯ä»€ä¹ˆï¼Ÿ"
            return
        
        # ä¸ºæ¯ä¸ªæ ·æœ¬ç”Ÿæˆé—®é¢˜
        for sample in valid_samples:
            self._generate_single_question(sample)
        
        success_count = len([s for s in valid_samples if s.get('generated_question')])
        print(f"    é—®é¢˜ç”Ÿæˆå®Œæˆï¼š{success_count}/{len(valid_samples)} ä¸ªæ ·æœ¬")
    
    def _generate_single_question(self, sample):
        """
        ä¸ºå•ä¸ªæ ·æœ¬ç”Ÿæˆé—®é¢˜
        """
        narrative_story = sample.get('narrative_story', '')
        answer_entity = sample.get('answer_entity', '')
        
        prompt = f"""ä½ æ˜¯ä¸€ä½ç²¾å‡†çš„æé—®ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä¸‹é¢æä¾›çš„ã€èƒŒæ™¯é™ˆè¿°ã€‘å’Œã€é¢„è®¾ç­”æ¡ˆã€‘ï¼Œæå‡ºä¸€ä¸ªèƒ½å¤Ÿå”¯ä¸€æŒ‡å‘è¯¥ç­”æ¡ˆçš„ç®€æ´é—®é¢˜ã€‚

æ ¸å¿ƒè¦æ±‚ï¼š
1.  **ç­”æ¡ˆå¯¼å‘**ï¼šç”Ÿæˆçš„é—®é¢˜çš„å”¯ä¸€æ­£ç¡®ç­”æ¡ˆå¿…é¡»æ˜¯ç»™å®šçš„ã€é¢„è®¾ç­”æ¡ˆã€‘ã€‚
2.  **è‡ªç„¶è¡”æ¥**ï¼šé—®é¢˜éœ€è¦ä¸ã€èƒŒæ™¯é™ˆè¿°ã€‘çš„ç»“å°¾ç´§å¯†è¡”æ¥ï¼Œè¯»èµ·æ¥é€šé¡ºè‡ªç„¶ã€‚
3.  **ç¦æ­¢æ³„éœ²**ï¼šé—®é¢˜æœ¬èº«ä¸èƒ½åŒ…å«ã€é¢„è®¾ç­”æ¡ˆã€‘çš„ä»»ä½•ç›´æ¥çº¿ç´¢æˆ–æ–‡å­—ã€‚
4.  **ç®€æ´æ¸…æ™°**ï¼šé—®é¢˜è¡¨è¿°åº”ç®€æ´ã€æ¸…æ™°ï¼Œç¬¦åˆäººç±»çš„æé—®ä¹ æƒ¯ï¼Œæ§åˆ¶åœ¨20å­—ä»¥å†…ã€‚

---
ã€èƒŒæ™¯é™ˆè¿°ã€‘ï¼š
{narrative_story}

ã€é¢„è®¾ç­”æ¡ˆã€‘ï¼š
{answer_entity}
---

è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸Šè¦æ±‚ï¼Œç›´æ¥ç”Ÿæˆä¸€ä¸ªé—®é¢˜ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–è¯´æ˜æˆ– "é—®é¢˜ï¼š" å‰ç¼€ã€‚
"""

        
        result = self._batch_llm_call(prompt)
        
        if result and not result.startswith("APIè°ƒç”¨å¤±è´¥"):
            # æ¸…ç†ç»“æœï¼Œæå–çº¯é—®é¢˜
            question = result.strip()
            # ç§»é™¤å¯èƒ½çš„å‰ç¼€
            if question.startswith("é—®é¢˜ï¼š") or question.startswith("é—®ï¼š"):
                question = question.split("ï¼š", 1)[-1].strip()
            
            sample['generated_question'] = question
        else:
            # å›é€€ï¼šç”Ÿæˆé€šç”¨é—®é¢˜
            sample['generated_question'] = "è¿™æ˜¯ä»€ä¹ˆï¼Ÿ"
    
    def _validate_questions_batch(self, batch_samples):
        """
        æ‰¹é‡æ ¡éªŒé—®é¢˜çš„é€»è¾‘æ­£ç¡®æ€§
        """
        print(f"    æ‰§è¡Œé—®é¢˜æ ¡éªŒ...")
        
        valid_samples = []
        for sample in batch_samples:
            if sample.get('generated_question') and sample.get('answer_entity'):
                valid_samples.append(sample)
        
        if not valid_samples:
            print(f"    è­¦å‘Šï¼šæ²¡æœ‰æœ‰æ•ˆçš„é—®é¢˜å¯ä¾›æ ¡éªŒ")
            return
        
        # æ‰¹é‡æ ¡éªŒ
        validation_prompt = """ä½ æ˜¯ä¸€ä½é€»è¾‘æ¨ç†ä¸“å®¶ã€‚è¯·åˆ¤æ–­ä»¥ä¸‹æ¯ç»„ã€èƒŒæ™¯æ•…äº‹+é—®é¢˜ã€‘æ˜¯å¦èƒ½å¤Ÿé€»è¾‘æ¨ç†å‡ºå¯¹åº”çš„ã€ç­”æ¡ˆã€‘ã€‚

è¯„åˆ¤æ ‡å‡†ï¼š
1. èƒŒæ™¯æ•…äº‹ä¸­çš„ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿæ¨å¯¼å‡ºç­”æ¡ˆ
2. é—®é¢˜ä¸ç­”æ¡ˆæ˜¯å¦é€»è¾‘ä¸€è‡´
3. æ¨ç†é“¾æ¡æ˜¯å¦å®Œæ•´

è¯·å¯¹æ¯ä¸ªé—®é¢˜ç»„åˆåªå›ç­”"é€šè¿‡"æˆ–"ä¸é€šè¿‡"ï¼š

"""
        
        for i, sample in enumerate(valid_samples):
            narrative_story = sample.get('narrative_story', '')
            question = sample.get('generated_question', '')
            answer = sample.get('answer_entity', '')
            
            validation_prompt += f"""
é—®é¢˜ç»„åˆ {i+1}:
èƒŒæ™¯æ•…äº‹ï¼š{narrative_story}
é—®é¢˜ï¼š{question}
ç­”æ¡ˆï¼š{answer}
"""
        
        validation_prompt += "\nè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›ï¼Œæ¯ä¸ªç»“æœå ä¸€è¡Œ:\nç»“æœ 1: [é€šè¿‡/ä¸é€šè¿‡]\nç»“æœ 2: [é€šè¿‡/ä¸é€šè¿‡]\n..."
        
        validation_result = self._batch_llm_call(validation_prompt)
        
        # è§£ææ ¡éªŒç»“æœ
        if validation_result and not validation_result.startswith("APIè°ƒç”¨å¤±è´¥"):
            parsed_results = self._parse_llm_response(validation_result, len(valid_samples), "ç»“æœ")
            
            passed_count = 0
            for i, sample in enumerate(valid_samples):
                if i < len(parsed_results):
                    validation_status = parsed_results[i].lower()
                    sample['validation_status'] = "é€šè¿‡" if "é€šè¿‡" in validation_status else "ä¸é€šè¿‡"
                    
                    if "é€šè¿‡" in validation_status:
                        passed_count += 1
                        sample['final_status'] = 'validated'
                    else:
                        sample['final_status'] = 'validation_failed'
                else:
                    sample['validation_status'] = "æœªæ ¡éªŒ"
                    sample['final_status'] = 'validation_failed'
            
            print(f"    æ ¡éªŒå®Œæˆï¼š{passed_count}/{len(valid_samples)} ä¸ªé—®é¢˜é€šè¿‡æ ¡éªŒ")
        else:
            print(f"    æ ¡éªŒAPIè°ƒç”¨å¤±è´¥ï¼Œæ ‡è®°æ‰€æœ‰é—®é¢˜ä¸ºæœªæ ¡éªŒçŠ¶æ€")
            for sample in valid_samples:
                sample['validation_status'] = "APIå¤±è´¥"
                sample['final_status'] = 'validation_failed'
        
        # ä¸ºä¸åœ¨valid_samplesä¸­çš„æ ·æœ¬è®¾ç½®é»˜è®¤çŠ¶æ€
        for sample in batch_samples:
            if 'validation_status' not in sample:
                sample['validation_status'] = "æ— æ•ˆé—®é¢˜"
                sample['final_status'] = 'invalid'

    def _process_obfuscation_and_weaving_batch(self, batch_samples):
        """
        ã€æ–°é˜¶æ®µäºŒã€‘ï¼šæƒ…å¢ƒæ¨¡ç³Šä¸é‡æ„ - å°†ç²¾ç¡®çš„äº‹å®é™ˆè¿°è½¬åŒ–ä¸ºæ¨¡ç³Šä½†æœ‰è¶£çš„å™è¿°
        
        è¿™ä¸ªé˜¶æ®µæ˜¯æ•´ä¸ªä¼˜åŒ–æ–¹æ¡ˆçš„æ ¸å¿ƒï¼Œå®ƒå°†ï¼š
        1. å¯¹å®ä½“è¿›è¡Œæ³›åŒ–å¤„ç†ï¼Œéšè—å…·ä½“çš„åç§°
        2. å°†æ•°å€¼å˜å¾—æ¨¡ç³Š
        3. é€šè¿‡æ•…äº‹æ€§å™è¿°éšè—ç›´æ¥çš„å…³ç³»
        4. åˆ›å»ºç›¸å¯¹æ—¶é—´å’Œé€»è¾‘è¿æ¥
        5. ç¡®ä¿ä¸æ³„éœ²ç›®æ ‡ç­”æ¡ˆ
        """
        prompt = """ä½ æ˜¯ä¸€ä½é¡¶çº§çš„å‡ºé¢˜ä¸“å®¶å’Œæ•…äº‹ç¼–ç»‡è€…ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ä»¥ä¸‹ã€åŸå§‹äº‹å®é™ˆè¿°ã€‘è½¬åŒ–ä¸ºä¸€æ®µå¼•äººå…¥èƒœä¸”å……æ»¡æ‚¬å¿µçš„ã€èƒŒæ™¯æ•…äº‹ã€‘ã€‚

è½¬åŒ–è¦æ±‚ï¼š
1. **ä¿¡æ¯æ¨¡ç³ŠåŒ–**ï¼š
   - **å®ä½“æ³›åŒ–**ï¼šå°†å…·ä½“çš„äººåã€åœ°åã€ç»„ç»‡åç­‰æ›¿æ¢ä¸ºæ›´é€šç”¨çš„æè¿°ï¼ˆä¾‹å¦‚ï¼Œ"è¯—äººC" -> "ä¸€ä½è¯—äºº"ï¼›"å¹³æ±‰é“è·¯" -> "ä¸€æ¡é‡è¦çš„äº¤é€šå¹²çº¿"ï¼‰ã€‚
   - **æ•°å€¼æ¨¡ç³Š**ï¼šå°†ç²¾ç¡®çš„æ•°å­—æˆ–å¹´ä»½æ›¿æ¢ä¸ºç›¸å¯¹æˆ–æ¨¡ç³Šçš„æè¿°ï¼ˆä¾‹å¦‚ï¼Œ"36å¹³æ–¹åƒç±³" -> "è¶…è¿‡35å¹³æ–¹åƒç±³"ï¼›"1933å¹´" -> "åœ¨æŸåœºæ··æˆ˜çˆ†å‘åçš„ç¬¬ä¸‰å¹´"ï¼‰ã€‚
   - **å…³ç³»éšè—**ï¼šä¸è¦ç›´æ¥é™ˆè¿°å®ä½“é—´çš„å…³ç³»ï¼Œè€Œæ˜¯é€šè¿‡æè¿°äº‹ä»¶æ¥æš—ç¤ºå®ƒä»¬ã€‚
   **æ³¨æ„**ï¼Œåœ¨è¿™ä¸€æ­¥éª¤ä¸­ï¼Œ**ä¸å¯ä»¥å¼•å…¥ä»»ä½•é”™è¯¯ä¿¡æ¯æˆ–è™šæ„çš„äº‹å®ã€‚**

2. **æƒ…å¢ƒé‡æ„**ï¼š
   - **å»ºç«‹ç›¸å¯¹æ—¶é—´**ï¼šåˆ›é€ åŸºäºäº‹ä»¶çš„æ—¶é—´çº¿ç´¢ï¼ˆä¾‹å¦‚ï¼Œ"5å¹´å"ã€"ä¸¤å¹´ååˆæ¢å¤äº†"ï¼‰ã€‚
   - **é€»è¾‘ä¸²è”**ï¼šå°†ç¦»æ•£çš„äº‹å®æœ‰æœºåœ°ä¸²è”æˆä¸€ä¸ªè¿è´¯ã€æµç•…çš„æ•…äº‹ï¼Œè€Œä¸æ˜¯ç®€å•çš„ç½—åˆ—ã€‚
   - **ä¿æŒæ ¸å¿ƒé€»è¾‘**ï¼šæ•…äº‹å¿…é¡»ä¿ç•™åŸå§‹äº‹å®ä¸­çš„æ ¸å¿ƒå› æœå’Œæ—¶åºå…³ç³»ï¼Œç¡®ä¿é—®é¢˜æœ€ç»ˆå¯ä»¥è¢«è§£ç­”ã€‚

3. **ç¦æ­¢æ³„éœ²ç­”æ¡ˆ**ï¼š
   - åœ¨ç”Ÿæˆçš„ã€èƒŒæ™¯æ•…äº‹ã€‘ä¸­ï¼Œç»å¯¹ä¸èƒ½å‡ºç°ã€ç›®æ ‡å®ä½“ã€‘ï¼ˆå³é—®é¢˜çš„ç­”æ¡ˆï¼‰ã€‚

ç°åœ¨ï¼Œè¯·å¤„ç†ä»¥ä¸‹å†…å®¹ï¼š
"""
        
        valid_samples = []
        for i, sample in enumerate(batch_samples):
            if 'integrated_clue_statement' in sample and not sample['integrated_clue_statement'].startswith("Error"):
                prompt += f"\nåŸå§‹äº‹å®é™ˆè¿° {len(valid_samples)+1}: {sample['integrated_clue_statement']}"
                prompt += f"\nç›®æ ‡å®ä½“ {len(valid_samples)+1}: {sample.get('target_entity', '')}\n"
                valid_samples.append(sample)
        
        if not valid_samples:
            print("  é˜¶æ®µäºŒï¼šæ²¡æœ‰æœ‰æ•ˆçš„äº‹å®é™ˆè¿°å¯ä¾›æ¨¡ç³ŠåŒ–å¤„ç†")
            for sample in batch_samples:
                sample['obfuscated_story'] = sample.get('integrated_clue_statement', 'ç›¸å…³å®ä½“é—´å­˜åœ¨æŸç§å…³è”')
            return

        prompt += "\n\nè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›ï¼Œæ¯ä¸ªèƒŒæ™¯æ•…äº‹å ä¸€è¡Œ:\nèƒŒæ™¯æ•…äº‹ 1: [è½¬åŒ–åçš„æ¨¡ç³ŠåŒ–æ•…äº‹]\nèƒŒæ™¯æ•…äº‹ 2: [è½¬åŒ–åçš„æ¨¡ç³ŠåŒ–æ•…äº‹]\n..."
        
        results_str = self._batch_llm_call(prompt)
        if results_str and not results_str.startswith("APIè°ƒç”¨å¤±è´¥"):
            parsed_stories = self._parse_llm_response(results_str, len(valid_samples), "èƒŒæ™¯æ•…äº‹")
            success_count = 0
            for i, sample in enumerate(valid_samples):
                if i < len(parsed_stories) and not parsed_stories[i].startswith("Error"):
                    sample['obfuscated_story'] = parsed_stories[i]
                    success_count += 1
                else:
                    # å›é€€åˆ°æ•´åˆçš„çº¿ç´¢é™ˆè¿°ï¼Œä½†è¿›è¡Œç®€å•çš„æ³›åŒ–å¤„ç†
                    orig_statement = sample.get('integrated_clue_statement', 'ç›¸å…³å®ä½“é—´å­˜åœ¨æŸç§å…³è”')
                    target_entity = sample.get('target_entity', '')
                    # ç®€å•æ³›åŒ–å¤„ç†ï¼šæ›¿æ¢ç›®æ ‡å®ä½“ä¸º"æŸä¸ªå®ä½“"
                    if target_entity and target_entity in orig_statement:
                        simplified_story = orig_statement.replace(target_entity, "æŸä¸ªç›¸å…³å®ä½“")
                    else:
                        simplified_story = orig_statement
                    sample['obfuscated_story'] = simplified_story
            
            # å¤„ç†æ— æ•ˆæ ·æœ¬
            for sample in batch_samples:
                if sample not in valid_samples:
                    sample['obfuscated_story'] = sample.get('integrated_clue_statement', 'ç›¸å…³å®ä½“é—´å­˜åœ¨æŸç§å…³è”')
                    
            print(f"  é˜¶æ®µäºŒå®Œæˆï¼ŒæˆåŠŸæ¨¡ç³ŠåŒ–å¤„ç† {success_count}/{len(batch_samples)} ä¸ªæ ·æœ¬")
        else:
            print(f"  é˜¶æ®µäºŒAPIè°ƒç”¨å¤±è´¥: {results_str}")
            # å›é€€å¤„ç†ï¼šç®€å•æ³›åŒ–
            for sample in batch_samples:
                orig_statement = sample.get('integrated_clue_statement', 'ç›¸å…³å®ä½“é—´å­˜åœ¨æŸç§å…³è”')
                target_entity = sample.get('target_entity', '')
                # ç®€å•æ³›åŒ–å¤„ç†ï¼šæ›¿æ¢ç›®æ ‡å®ä½“ä¸º"æŸä¸ªå®ä½“"
                if target_entity and target_entity in orig_statement:
                    simplified_story = orig_statement.replace(target_entity, "æŸä¸ªç›¸å…³å®ä½“")
                else:
                    simplified_story = orig_statement
                sample['obfuscated_story'] = simplified_story

    def _process_targeted_question_batch(self, batch_samples):
        """
        [å·²åºŸå¼ƒ] æ—§çš„é—®é¢˜æ„å»ºå‡½æ•° - ä¿ç•™ä½œä¸ºå‚è€ƒ
        """
        pass
        
    def _process_final_question_batch(self, batch_samples):
        """
        ã€æ–°é˜¶æ®µä¸‰ã€‘ï¼šæœ€ç»ˆé—®é¢˜æ„å»º - åŸºäºæ¨¡ç³ŠåŒ–æ•…äº‹å’Œç›®æ ‡å®ä½“ç”Ÿæˆé—®é¢˜
        
        è¿™ä¸ªé˜¶æ®µæ˜¯æ•´ä¸ªæµç¨‹çš„æ”¶å°¾ï¼Œå®ƒå°†ï¼š
        1. åŸºäºæ¨¡ç³ŠåŒ–çš„èƒŒæ™¯æ•…äº‹æ„å»ºé—®é¢˜
        2. ç¡®ä¿é—®é¢˜è‡ªç„¶åœ°è¡”æ¥æ•…äº‹ç»“å°¾
        3. é—®é¢˜çš„ç­”æ¡ˆå¿…é¡»æ˜¯ç›®æ ‡å®ä½“ï¼Œä½†ä¸èƒ½é€éœ²ä»»ä½•ç›´æ¥çº¿ç´¢
        """
        prompt = """ä½ æ˜¯ä¸€ä½æé—®ä¸“å®¶ã€‚è¯·æ ¹æ®ä¸‹é¢æä¾›çš„ã€èƒŒæ™¯æ•…äº‹ã€‘å’Œã€ç­”æ¡ˆã€‘ï¼Œæå‡ºä¸€ä¸ªç®€æ´ã€è‡ªç„¶çš„é—®å¥ã€‚

è¦æ±‚ï¼š
1. é—®é¢˜å¿…é¡»ä¸ã€èƒŒæ™¯æ•…äº‹ã€‘çš„ç»“å°¾ç´§å¯†è¡”æ¥ï¼Œåƒæ˜¯æ•…äº‹å™è¿°çš„è‡ªç„¶å»¶ç»­ã€‚
2. é—®é¢˜çš„ç­”æ¡ˆå¿…é¡»æ˜¯ç»™å®šçš„ã€ç­”æ¡ˆã€‘ã€‚
3. é—®é¢˜æœ¬èº«ä¸èƒ½åŒ…å«ç­”æ¡ˆçš„ä»»ä½•ç›´æ¥çº¿ç´¢ã€‚
4. é—®é¢˜è¡¨è¿°åº”ç®€æ´ã€æ¸…æ™°ï¼Œç¬¦åˆäººç±»çš„æé—®ä¹ æƒ¯ã€‚

ç°åœ¨ï¼Œè¯·å¤„ç†ä»¥ä¸‹å†…å®¹ï¼š
"""
        
        valid_samples = []
        for i, sample in enumerate(batch_samples):
            if 'obfuscated_story' in sample and not sample['obfuscated_story'].startswith("Error"):
                story = sample['obfuscated_story']
                target_entity = sample.get('target_entity', '')
                
                # é™åˆ¶æ•…äº‹é•¿åº¦ä»¥é¿å…promptè¿‡é•¿
                if len(story) > 300:
                    story = story[:300] + "..."
                
                prompt += f"\nèƒŒæ™¯æ•…äº‹ {len(valid_samples)+1}: {story}"
                prompt += f"\nç­”æ¡ˆ {len(valid_samples)+1}: {target_entity}\n"
                valid_samples.append(sample)
        
        if not valid_samples:
            print("  é˜¶æ®µä¸‰ï¼šæ²¡æœ‰æœ‰æ•ˆçš„æ¨¡ç³Šæ•…äº‹å¯ä¾›é—®é¢˜ç”Ÿæˆ")
            for sample in batch_samples:
                sample['final_question'] = f"åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œè¯·é—®ç­”æ¡ˆæ˜¯ä»€ä¹ˆï¼Ÿ"
                sample['answer'] = sample.get('target_entity', '')
            return

        prompt += "\nè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›ï¼Œæ¯ä¸ªé—®é¢˜å ä¸€è¡Œ:\né—®é¢˜ 1: [æœ€ç»ˆçš„é—®é¢˜]\né—®é¢˜ 2: [æœ€ç»ˆçš„é—®é¢˜]\n..."
        
        results_str = self._batch_llm_call(prompt)
        if results_str and not results_str.startswith("APIè°ƒç”¨å¤±è´¥"):
            parsed_questions = self._parse_llm_response(results_str, len(valid_samples), "é—®é¢˜")
            success_count = 0
            for i, sample in enumerate(valid_samples):
                if i < len(parsed_questions) and not parsed_questions[i].startswith("Error"):
                    sample['final_question'] = parsed_questions[i]
                    success_count += 1
                else:
                    # å›é€€é€»è¾‘
                    target_relation = sample.get('target_relation', 'ç›¸å…³ä¿¡æ¯')
                    sample['final_question'] = f"æ ¹æ®è¿™æ®µæè¿°ï¼Œä½ èƒ½æ¨æ–­å‡º{target_relation}æ˜¯ä»€ä¹ˆå—ï¼Ÿ"
                
                # è®¾ç½®ç­”æ¡ˆ
                sample['answer'] = sample.get('target_entity', '')
            
            # å¤„ç†æ— æ•ˆæ ·æœ¬
            for sample in batch_samples:
                if sample not in valid_samples:
                    target_relation = sample.get('target_relation', 'ç›¸å…³ä¿¡æ¯')
                    sample['final_question'] = f"æ ¹æ®è¿™æ®µæè¿°ï¼Œä½ èƒ½æ¨æ–­å‡º{target_relation}æ˜¯ä»€ä¹ˆå—ï¼Ÿ"
                    sample['answer'] = sample.get('target_entity', '')
                    
            print(f"  é˜¶æ®µä¸‰å®Œæˆï¼ŒæˆåŠŸç”Ÿæˆ {success_count}/{len(batch_samples)} ä¸ªæœ€ç»ˆé—®é¢˜")
        else:
            print(f"  é˜¶æ®µä¸‰APIè°ƒç”¨å¤±è´¥: {results_str}")
            # å›é€€å¤„ç†
            for sample in batch_samples:
                target_relation = sample.get('target_relation', 'ç›¸å…³ä¿¡æ¯')
                sample['final_question'] = f"æ ¹æ®è¿™æ®µæè¿°ï¼Œä½ èƒ½æ¨æ–­å‡º{target_relation}æ˜¯ä»€ä¹ˆå—ï¼Ÿ"
                sample['answer'] = sample.get('target_entity', '')

    def _save_stage_results(self, batch_samples, stage_name, batch_idx):
        """
        ä¿å­˜æ¯ä¸ªé˜¶æ®µçš„å¤„ç†ç»“æœ - æ–°ä¸‰é˜¶æ®µç­–ç•¥ç‰ˆæœ¬
        """
        import os
        from datetime import datetime
        
        # åˆ›å»ºé˜¶æ®µç»“æœä¿å­˜ç›®å½•
        stage_dir = os.path.join(os.path.dirname(config.OUTPUT_DATASET_PATH), "stage_results")
        os.makedirs(stage_dir, exist_ok=True)
        
        # åˆ›å»ºé˜¶æ®µæ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        stage_file = os.path.join(stage_dir, f"{stage_name}_batch_{batch_idx:03d}_{timestamp}.jsonl")
        
        with open(stage_file, 'w', encoding='utf-8') as f:
            for i, sample in enumerate(batch_samples):
                # åˆ¤æ–­æ˜¯æ–°ç­–ç•¥è¿˜æ˜¯æ—§ç­–ç•¥çš„æ•°æ®ç»“æ„
                if sample.get('strategy') == 'multi_source_clue_aggregation':
                    # æ–°ä¸‰é˜¶æ®µç­–ç•¥çš„æ•°æ®ç»“æ„
                    stage_result = {
                        'sample_id': sample.get('sample_id', f"batch_{batch_idx:03d}_sample_{i+1:02d}"),
                        'strategy': sample.get('strategy', 'multi_source_clue_aggregation'),
                        'answer_entity': sample.get('answer_entity', ''),
                        'independent_paths': sample.get('independent_paths', []),
                        'clue_entities': sample.get('clue_entities', []),
                        'num_clue_paths': sample.get('num_clue_paths', 0),
                        'generation_timestamp': sample.get('generation_timestamp', ''),
                        'status': sample.get('status', 'unknown')
                    }
                    
                    # æ ¹æ®é˜¶æ®µæ·»åŠ ç›¸åº”çš„æ–°ç­–ç•¥å­—æ®µ
                    if stage_name == "stage2_deep_obfuscation":
                        stage_result.update({
                            'obfuscated_entities': sample.get('obfuscated_entities', {}),
                            'narrative_story': sample.get('narrative_story', ''),
                            'clue_paths_info': sample.get('clue_paths_info', [])
                        })
                    elif stage_name == "stage3_question_validation":
                        stage_result.update({
                            'obfuscated_entities': sample.get('obfuscated_entities', {}),
                            'narrative_story': sample.get('narrative_story', ''),
                            'generated_question': sample.get('generated_question', ''),
                            'validation_status': sample.get('validation_status', ''),
                            'final_status': sample.get('final_status', ''),
                            'clue_paths_info': sample.get('clue_paths_info', [])
                        })
                
                else:
                    # æ—§ç­–ç•¥çš„æ•°æ®ç»“æ„ï¼ˆå‘åå…¼å®¹ï¼‰
                    stage_result = {
                        'sample_id': f"batch_{batch_idx:03d}_sample_{i+1:02d}",
                        'original_reasoning_path': sample.get('original_reasoning_path', []),
                        'reasoning_chain_raw': sample.get('reasoning_chain_raw', ''),
                        'target_entity': sample.get('target_entity', ''),
                        'target_relation': sample.get('target_relation', ''),
                        'clue_triples': sample.get('clue_triples', []),
                        'clue_entities': sample.get('clue_entities', []),
                        # æ·»åŠ æ—§ç‰ˆå­—æ®µ
                        'obfuscated_story': sample.get('obfuscated_story', ''),
                        'question_generation_strategy': sample.get('question_generation_strategy', 'unknown'),
                        'clue_count': sample.get('clue_count', 0)
                    }
                    
                    # æ ¹æ®é˜¶æ®µæ·»åŠ ç›¸åº”çš„æ—§ç­–ç•¥å­—æ®µ
                    if stage_name == "stage1_clue_integration":
                        stage_result.update({
                            'integrated_clue_statement': sample.get('integrated_clue_statement', '')
                        })
                    elif stage_name == "stage2_context_weaving" or stage_name == "stage2_obfuscation_weaving":
                        stage_result.update({
                            'integrated_clue_statement': sample.get('integrated_clue_statement', ''),
                            'contextual_narrative': sample.get('contextual_narrative', '')
                        })
                    elif stage_name == "stage3_targeted_questions":
                        stage_result.update({
                            'integrated_clue_statement': sample.get('integrated_clue_statement', ''),
                            'contextual_narrative': sample.get('contextual_narrative', ''),
                            'final_question': sample.get('final_question', ''),
                            'answer': sample.get('answer', '')
                        })
                
                f.write(json.dumps(stage_result, ensure_ascii=False) + '\n')
        
        print(f"  é˜¶æ®µç»“æœå·²ä¿å­˜åˆ°: {stage_file}")

    def generate_comprehensive_qa_new_strategy(self, graph, total_questions=50, batch_size=10):
        """
        åŸºäºæ–°ä¸‰é˜¶æ®µç­–ç•¥ç”Ÿæˆå…¨é¢çš„é—®ç­”æ•°æ®é›†
        
        Args:
            graph: çŸ¥è¯†å›¾è°±å¯¹è±¡  
            total_questions: æ€»é—®é¢˜æ•°é‡
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            dict: åŒ…å«æ‰€æœ‰ç”Ÿæˆç»“æœçš„å­—å…¸
        """
        print(f"\n" + "="*80)
        print(f"ğŸš€ å¯åŠ¨åŸºäºä¸‰é˜¶æ®µæ·±åº¦ç­–ç•¥çš„å…¨é¢é—®ç­”æ•°æ®é›†ç”Ÿæˆ")
        print(f"="*80)
        
        # æ‰§è¡Œæ–°çš„ä¸‰é˜¶æ®µé—®é¢˜ç”Ÿæˆ
        all_samples = self.generate_questions_cascade_new_strategy(graph, total_questions, batch_size)
        
        if not all_samples:
            print("âŒ æ–°ç­–ç•¥é—®é¢˜ç”Ÿæˆå¤±è´¥ï¼Œæ— æ³•åˆ›å»ºæ•°æ®é›†")
            return {'success': False, 'samples': []}
        
        # ç”Ÿæˆå„ç§æ ¼å¼çš„é—®ç­”æ•°æ®é›†
        print(f"\nğŸ¯ åŸºäºæ–°ç­–ç•¥ç”Ÿæˆé—®ç­”æ•°æ®é›†...")
        
        # 1. ç”ŸæˆéªŒè¯é€šè¿‡çš„é—®é¢˜æ•°æ®é›†
        validated_questions = self._create_validated_questions_dataset(all_samples)
        
        # 2. ç”Ÿæˆç®€åŒ–æ ¼å¼é—®ç­”æ•°æ®é›†  
        simple_count = self._create_simple_qa_new_strategy(validated_questions)
        
        # 3. ç”Ÿæˆè¯¦ç»†æ ¼å¼é—®ç­”æ•°æ®é›†
        detailed_count = self._create_detailed_qa_new_strategy(all_samples)
        
        # 4. ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡æŠ¥å‘Š
        self._generate_new_strategy_summary(all_samples, validated_questions, simple_count, detailed_count)
        
        result = {
            'success': True,
            'total_samples': len(all_samples),
            'validated_questions': validated_questions,
            'validated_count': len(validated_questions),
            'simple_count': simple_count,
            'detailed_count': detailed_count,
            'validation_rate': len(validated_questions) / len(all_samples) * 100 if all_samples else 0
        }
        
        print(f"\nâœ… åŸºäºæ–°ä¸‰é˜¶æ®µç­–ç•¥çš„é—®ç­”æ•°æ®é›†ç”Ÿæˆå®Œæˆ!")
        return result
    
    def _create_validated_questions_dataset(self, samples):
        """åˆ›å»ºéªŒè¯é€šè¿‡çš„é—®é¢˜æ•°æ®é›†"""
        validated_questions = []
        
        for sample in samples:
            # åªåŒ…å«éªŒè¯é€šè¿‡çš„é—®é¢˜
            if sample.get('final_status') == 'validated':
                qa_item = {
                    'id': sample.get('sample_id', ''),
                    'question': sample.get('generated_question', ''),
                    'answer': sample.get('answer_entity', ''),
                    'background_story': sample.get('narrative_story', ''),
                    'strategy': 'multi_source_clue_aggregation',
                    'clue_entities': sample.get('clue_entities', []),
                    'num_clue_paths': sample.get('num_clue_paths', 0),
                    'validation_status': sample.get('validation_status', ''),
                    'generation_metadata': {
                        'has_background': bool(sample.get('narrative_story')),
                        'question_length': len(sample.get('generated_question', '')),
                        'story_length': len(sample.get('narrative_story', '')),
                        'clue_count': len(sample.get('clue_entities', [])),
                        'timestamp': sample.get('generation_timestamp', '')
                    }
                }
                validated_questions.append(qa_item)
        
        # ä¿å­˜éªŒè¯é€šè¿‡çš„é—®é¢˜æ•°æ®é›†
        output_dir = os.path.join(os.path.dirname(config.OUTPUT_DATASET_PATH), "output")
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, "validated_qa_new_strategy.jsonl")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for item in validated_questions:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        print(f"éªŒè¯é€šè¿‡çš„é—®é¢˜æ•°æ®é›†å·²ä¿å­˜: {output_path}")
        print(f"éªŒè¯é€šè¿‡é—®é¢˜æ•°: {len(validated_questions)}")
        
        return validated_questions
    
    def _create_simple_qa_new_strategy(self, validated_questions):
        """åˆ›å»ºç®€åŒ–æ ¼å¼çš„é—®ç­”æ•°æ®é›†ï¼ˆæ–°ç­–ç•¥ç‰ˆæœ¬ï¼‰"""
        output_dir = os.path.join(os.path.dirname(config.OUTPUT_DATASET_PATH), "output")
        output_path = os.path.join(output_dir, "simple_qa_new_strategy.jsonl")
        
        simple_count = 0
        with open(output_path, 'w', encoding='utf-8') as f:
            for item in validated_questions:
                simple_qa = {
                    'question': item['question'],
                    'answer': item['answer']
                }
                f.write(json.dumps(simple_qa, ensure_ascii=False) + '\n')
                simple_count += 1
        
        print(f"ç®€åŒ–æ ¼å¼é—®ç­”æ•°æ®é›†å·²ä¿å­˜: {output_path}")
        print(f"ç®€åŒ–æ ¼å¼é—®é¢˜æ•°: {simple_count}")
        return simple_count
    
    def _create_detailed_qa_new_strategy(self, all_samples):
        """åˆ›å»ºè¯¦ç»†æ ¼å¼çš„é—®ç­”æ•°æ®é›†ï¼ˆåŒ…å«æ‰€æœ‰ä¿¡æ¯ï¼‰"""
        output_dir = os.path.join(os.path.dirname(config.OUTPUT_DATASET_PATH), "output")
        output_path = os.path.join(output_dir, "detailed_qa_new_strategy.jsonl")
        
        detailed_count = 0
        with open(output_path, 'w', encoding='utf-8') as f:
            for sample in all_samples:
                detailed_item = {
                    'sample_id': sample.get('sample_id', ''),
                    'strategy': sample.get('strategy', ''),
                    'answer_entity': sample.get('answer_entity', ''),
                    'independent_paths': sample.get('independent_paths', []),
                    'clue_entities': sample.get('clue_entities', []),
                    'obfuscated_entities': sample.get('obfuscated_entities', {}),
                    'narrative_story': sample.get('narrative_story', ''),
                    'generated_question': sample.get('generated_question', ''),
                    'validation_status': sample.get('validation_status', ''),
                    'final_status': sample.get('final_status', ''),
                    'generation_timestamp': sample.get('generation_timestamp', ''),
                    'debug_info': {
                        'num_clue_paths': sample.get('num_clue_paths', 0),
                        'clue_paths_info': sample.get('clue_paths_info', [])
                    }
                }
                f.write(json.dumps(detailed_item, ensure_ascii=False) + '\n')
                detailed_count += 1
        
        print(f"è¯¦ç»†æ ¼å¼é—®ç­”æ•°æ®é›†å·²ä¿å­˜: {output_path}")
        print(f"è¯¦ç»†æ ¼å¼æ¡ç›®æ•°: {detailed_count}")
        return detailed_count
    
    def _generate_new_strategy_summary(self, all_samples, validated_questions, simple_count, detailed_count):
        """ç”Ÿæˆæ–°ç­–ç•¥çš„æ•°æ®é›†ç»Ÿè®¡æŠ¥å‘Š"""
        print(f"\n" + "="*80)
        print(f"ğŸ“‹ ä¸‰é˜¶æ®µæ·±åº¦é—®é¢˜ç”Ÿæˆç­–ç•¥æ•°æ®é›†æŠ¥å‘Š")
        print(f"="*80)
        
        # åŸºæœ¬ç»Ÿè®¡
        total_samples = len(all_samples)
        validated_count = len(validated_questions)
        validation_rate = validated_count / total_samples * 100 if total_samples > 0 else 0
        
        # ç­–ç•¥åˆ†æ
        status_distribution = {}
        for sample in all_samples:
            status = sample.get('final_status', 'unknown')
            status_distribution[status] = status_distribution.get(status, 0) + 1
        
        # é—®é¢˜è´¨é‡åˆ†æ
        if validated_questions:
            avg_question_len = sum(len(q['question']) for q in validated_questions) / len(validated_questions)
            avg_story_len = sum(len(q['background_story']) for q in validated_questions) / len(validated_questions)
            avg_clue_count = sum(len(q['clue_entities']) for q in validated_questions) / len(validated_questions)
        else:
            avg_question_len = avg_story_len = avg_clue_count = 0
        
        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   â€¢ æ€»ç”Ÿæˆæ ·æœ¬æ•°: {total_samples}")
        print(f"   â€¢ éªŒè¯é€šè¿‡æ•°: {validated_count}")
        print(f"   â€¢ éªŒè¯é€šè¿‡ç‡: {validation_rate:.1f}%")
        print(f"   â€¢ ç®€åŒ–æ ¼å¼é—®é¢˜æ•°: {simple_count}")
        print(f"   â€¢ è¯¦ç»†æ ¼å¼æ¡ç›®æ•°: {detailed_count}")
        
        print(f"\nğŸ“ˆ è´¨é‡æŒ‡æ ‡:")
        print(f"   â€¢ å¹³å‡é—®é¢˜é•¿åº¦: {avg_question_len:.1f} å­—ç¬¦")
        print(f"   â€¢ å¹³å‡æ•…äº‹é•¿åº¦: {avg_story_len:.1f} å­—ç¬¦") 
        print(f"   â€¢ å¹³å‡çº¿ç´¢æ•°é‡: {avg_clue_count:.1f} ä¸ª")
        
        print(f"\nğŸ¯ çŠ¶æ€åˆ†å¸ƒ:")
        for status, count in status_distribution.items():
            percentage = count / total_samples * 100 if total_samples > 0 else 0
            print(f"   â€¢ {status}: {count} ä¸ª ({percentage:.1f}%)")
        
        print(f"\nğŸ”¬ æ–°ç­–ç•¥ç‰¹ç‚¹:")
        print(f"   â€¢ å¤šæºçº¿ç´¢èšåˆï¼šä»å•ä¸€è·¯å¾„è½¬å‘å¤šæ¡ç‹¬ç«‹çº¿ç´¢")
        print(f"   â€¢ åŸå­åŒ–æ¨¡ç³Šï¼šå¯¹æ¯ä¸ªå®ä½“è¿›è¡Œç‹¬ç«‹çš„æ¨¡ç³ŠåŒ–å¤„ç†")
        print(f"   â€¢ æ•…äº‹ç¼–ç»‡ï¼šå°†æ¨¡ç³Šçº¿ç´¢ç¼–ç»‡æˆè¿è´¯çš„æ‚¬ç–‘æ•…äº‹")
        print(f"   â€¢ é€»è¾‘æ ¡éªŒï¼šç¡®ä¿é—®é¢˜å¯ä»¥é€šè¿‡æ¨ç†å¾—åˆ°ç­”æ¡ˆ")
        
        print(f"\nğŸ“ ç”Ÿæˆæ–‡ä»¶:")
        print(f"   â€¢ validated_qa_new_strategy.jsonl: éªŒè¯é€šè¿‡çš„å®Œæ•´é—®é¢˜")
        print(f"   â€¢ simple_qa_new_strategy.jsonl: çº¯é—®ç­”æ ¼å¼")
        print(f"   â€¢ detailed_qa_new_strategy.jsonl: åŒ…å«è°ƒè¯•ä¿¡æ¯çš„è¯¦ç»†æ ¼å¼")
        
        return True

    def generate_questions_cascade_with_context(self, path_samples, batch_size=10, questions_per_path=2, generate_contextual=True):
        """
        æ‰§è¡Œå®Œæ•´çš„é—®é¢˜ç”Ÿæˆæµç¨‹ï¼Œå¹¶è‡ªåŠ¨ç”ŸæˆåŒ…å«èƒŒæ™¯ä¿¡æ¯çš„é—®é¢˜æ•°æ®é›†ã€‚
        
        Args:
            path_samples: è·¯å¾„æ ·æœ¬åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            questions_per_path: æ¯æ¡è·¯å¾„ç”Ÿæˆçš„é—®é¢˜æ•°é‡
            generate_contextual: æ˜¯å¦è‡ªåŠ¨ç”ŸæˆèƒŒæ™¯ä¿¡æ¯æ ¼å¼çš„é—®é¢˜
        
        Returns:
            dict: åŒ…å«æ‰€æœ‰ç”Ÿæˆç»“æœçš„å­—å…¸
        """
        print(f"ğŸš€ å¼€å§‹å…¨æ–°çš„é—®é¢˜ç”Ÿæˆæµç¨‹ï¼ˆéšæœºç›®æ ‡é€‰æ‹©ç­–ç•¥ï¼‰")
        
        # æ‰§è¡Œæ–°çš„å››é˜¶æ®µçº§è”ç”Ÿæˆ
        all_samples = self.generate_questions_cascade(path_samples, batch_size, questions_per_path)
        
        result = {
            'standard_samples': all_samples,
            'contextual_questions': None,
            'simple_count': 0
        }
        
        if generate_contextual and all_samples:
            print(f"\nğŸ¯ å¼€å§‹ç”ŸæˆèƒŒæ™¯ä¿¡æ¯æ ¼å¼çš„é—®é¢˜...")
            
            # è‡ªåŠ¨ç”ŸæˆèƒŒæ™¯ä¿¡æ¯æ ¼å¼çš„é—®é¢˜
            contextual_results = self.generate_comprehensive_qa_datasets(all_samples)
            
            result.update({
                'contextual_questions': contextual_results['contextual_questions'],
                'simple_count': contextual_results['simple_count'],
                'total_samples': contextual_results['total_samples']
            })
            
            print(f"\nâœ… å®Œæ•´çš„é—®é¢˜ç”Ÿæˆæµç¨‹å·²å®Œæˆ!")
            print(f"   â€¢ æ ‡å‡†æ ¼å¼é—®é¢˜: {len(all_samples)} ä¸ª")
            print(f"   â€¢ èƒŒæ™¯å¼é—®é¢˜: {len(contextual_results['contextual_questions'])} ä¸ª")
            print(f"   â€¢ ç®€åŒ–æ ¼å¼é—®é¢˜: {contextual_results['simple_count']} ä¸ª")
        
        return result

    def generate_contextual_questions(self, samples, output_path=None):
        """
        ç”ŸæˆåŒ…å«èƒŒæ™¯ä¿¡æ¯çš„é—®é¢˜æ•°æ®é›†ï¼Œé—®é¢˜æ ¼å¼ä¸º obfuscated_story + final_question
        ä¼˜å…ˆä½¿ç”¨æ¨¡ç³ŠåŒ–åçš„æ•…äº‹ä½œä¸ºèƒŒæ™¯ä¿¡æ¯ï¼Œä½¿é—®é¢˜æ›´å…·æŒ‘æˆ˜æ€§
        """
        print(f"\n=== å¼€å§‹ç”ŸæˆåŒ…å«æ¨¡ç³ŠåŒ–èƒŒæ™¯çš„é—®é¢˜æ•°æ®é›† ===")
        
        if output_path is None:
            output_dir = os.path.join(os.path.dirname(config.OUTPUT_DATASET_PATH), "output")
            os.makedirs(output_dir, exist_ok=True)
            output_path = os.path.join(output_dir, "contextual_qa_dataset.jsonl")
        
        contextual_questions = []
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for idx, sample in enumerate(samples):
                # ä¼˜å…ˆä½¿ç”¨æ¨¡ç³ŠåŒ–çš„æ•…äº‹ï¼Œå›é€€åˆ°å…¶ä»–å¯ç”¨å­—æ®µ
                background_story = sample.get('obfuscated_story', '').strip()
                if not background_story:
                    background_story = sample.get('contextual_narrative', '').strip()
                if not background_story:
                    background_story = sample.get('integrated_clue_statement', '').strip()
                
                final_question = sample.get('final_question', '').strip()
                answer = sample.get('answer', '').strip()
                
                # æ„å»ºåŒ…å«èƒŒæ™¯çš„å®Œæ•´é—®é¢˜
                if background_story and final_question:
                    # ç¡®ä¿å™è¿°å’Œé—®é¢˜ä¹‹é—´æœ‰é€‚å½“çš„è¿æ¥
                    if background_story.endswith(('ã€‚', '.', 'ï¼', '!', 'ï¼Ÿ', '?')):
                        contextual_question = f"{background_story} {final_question}"
                    else:
                        contextual_question = f"{background_story}ã€‚{final_question}"
                elif final_question:
                    # å¦‚æœæ²¡æœ‰æƒ…å¢ƒå™è¿°ï¼Œåªä½¿ç”¨é—®é¢˜
                    contextual_question = final_question
                else:
                    # å›é€€æ–¹æ¡ˆ
                    contextual_question = f"æ ¹æ®ç›¸å…³ä¿¡æ¯ï¼Œè¯·é—®{sample.get('target_relation', 'ç­”æ¡ˆ')}æ˜¯ä»€ä¹ˆï¼Ÿ"
                
                # åˆ›å»ºåŒ…å«èƒŒæ™¯çš„é—®é¢˜è®°å½•
                contextual_record = {
                    "id": f"contextual_mq_{idx:05d}",
                    "question_with_context": contextual_question,
                    "answer": answer,
                    "target_entity": sample.get('target_entity', ''),
                    "target_relation": sample.get('target_relation', ''),
                    
                    # åŸå§‹ç»„æˆéƒ¨åˆ†ï¼ˆä¾›å‚è€ƒï¼‰
                    "original_components": {
                        "background_story": background_story,
                        "final_question": final_question,
                        "integrated_clue_statement": sample.get('integrated_clue_statement', ''),
                        "obfuscated_story": sample.get('obfuscated_story', '')
                    },
                    
                    # æ¨ç†ä¿¡æ¯ï¼ˆæ”¾åœ¨æœ€åï¼‰
                    "reasoning_info": {
                        "original_reasoning_path": sample.get('original_reasoning_path', []),
                        "reasoning_chain_raw": sample.get('reasoning_chain_raw', ''),
                        "clue_triples": sample.get('clue_triples', []),
                        "clue_entities": sample.get('clue_entities', []),
                        "clue_count": sample.get('clue_count', 0)
                    },
                    
                    # å…ƒæ•°æ®
                    "metadata": {
                        "source_sample_id": sample.get('sample_id', f'generated_{idx}'),
                        "question_length": len(contextual_question),
                        "has_context": bool(background_story),
                        "creation_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "generation_method": "obfuscation_and_weaving",
                        "question_generation_strategy": sample.get('question_generation_strategy', 'unknown'),
                        "target_selection_method": sample.get('target_selection_method', 'unknown'),
                        "is_obfuscated": bool(sample.get('obfuscated_story', ''))
                    }
                }
                
                f.write(json.dumps(contextual_record, ensure_ascii=False) + '\n')
                contextual_questions.append(contextual_record)
        
        print(f"åŸºäºéšæœºç›®æ ‡é€‰æ‹©çš„èƒŒæ™¯å¼é—®é¢˜æ•°æ®é›†å·²ç”Ÿæˆ: {output_path}")
        print(f"æ€»é—®é¢˜æ•°: {len(contextual_questions)}")
        
        # è´¨é‡ç»Ÿè®¡
        print(f"\n=== èƒŒæ™¯å¼é—®é¢˜è´¨é‡ç»Ÿè®¡ ===")
        
        # ç»Ÿè®¡æœ‰èƒŒæ™¯çš„é—®é¢˜æ¯”ä¾‹
        with_context_count = sum(1 for q in contextual_questions if q['metadata']['has_context'])
        context_percentage = with_context_count / len(contextual_questions) * 100 if contextual_questions else 0
        
        # ç»Ÿè®¡é—®é¢˜é•¿åº¦
        lengths = [q['metadata']['question_length'] for q in contextual_questions]
        avg_length = sum(lengths) / len(lengths) if lengths else 0
        
        # ç»Ÿè®¡ç­”æ¡ˆå®Œæ•´æ€§
        complete_answers = sum(1 for q in contextual_questions if q['answer'] and q['answer'] != '')
        answer_percentage = complete_answers / len(contextual_questions) * 100 if contextual_questions else 0
        
        # ç»Ÿè®¡çº¿ç´¢å¤šæ ·æ€§
        clue_counts = [q['reasoning_info']['clue_count'] for q in contextual_questions]
        avg_clue_count = sum(clue_counts) / len(clue_counts) if clue_counts else 0
        
        print(f"  åŒ…å«èƒŒæ™¯ä¿¡æ¯: {with_context_count}/{len(contextual_questions)} ({context_percentage:.1f}%)")
        print(f"  å¹³å‡é—®é¢˜é•¿åº¦: {avg_length:.1f} å­—ç¬¦")
        print(f"  ç­”æ¡ˆå®Œæ•´æ€§: {complete_answers}/{len(contextual_questions)} ({answer_percentage:.1f}%)")
        print(f"  å¹³å‡çº¿ç´¢æ•°é‡: {avg_clue_count:.1f} ä¸ª")
        
        # å±•ç¤ºæ ·æœ¬
        print(f"\n=== åŸºäºéšæœºç›®æ ‡é€‰æ‹©çš„é—®é¢˜æ ·æœ¬å±•ç¤º ===")
        for i in range(min(3, len(contextual_questions))):
            question = contextual_questions[i]
            print(f"\n--- æ ·æœ¬ {i+1} ---")
            print(f"ID: {question['id']}")
            print(f"ç›®æ ‡å®ä½“: {question.get('target_entity', 'N/A')}")
            print(f"ç›®æ ‡å…³ç³»: {question.get('target_relation', 'N/A')}")
            print(f"çº¿ç´¢æ•°é‡: {question['reasoning_info']['clue_count']}")
            print(f"åŒ…å«èƒŒæ™¯: {'æ˜¯' if question['metadata']['has_context'] else 'å¦'}")
            print(f"é—®é¢˜: {question['question_with_context'][:150]}{'...' if len(question['question_with_context']) > 150 else ''}")
            print(f"ç­”æ¡ˆ: {question['answer']}")
        
        return contextual_questions
    
    def create_simple_qa_format(self, contextual_questions=None, output_path=None):
        """
        åˆ›å»ºç®€åŒ–çš„é—®ç­”æ ¼å¼æ–‡ä»¶ï¼ŒåªåŒ…å«é—®é¢˜å’Œç­”æ¡ˆ
        """
        print(f"\n=== åˆ›å»ºç®€åŒ–é—®ç­”æ ¼å¼ ===")
        
        if output_path is None:
            output_dir = os.path.join(os.path.dirname(config.OUTPUT_DATASET_PATH), "output")
            os.makedirs(output_dir, exist_ok=True)
            output_path = os.path.join(output_dir, "simple_qa_format.jsonl")
        
        # å¦‚æœæ²¡æœ‰æä¾›contextual_questionsï¼Œä»æ–‡ä»¶è¯»å–
        if contextual_questions is None:
            contextual_file = os.path.join(os.path.dirname(output_path), "contextual_qa_dataset.jsonl")
            if not os.path.exists(contextual_file):
                print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°èƒŒæ™¯å¼é—®é¢˜æ–‡ä»¶ {contextual_file}")
                return 0
            
            contextual_questions = []
            with open(contextual_file, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        contextual_questions.append(json.loads(line.strip()))
                    except json.JSONDecodeError:
                        continue
        
        simple_count = 0
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for data in contextual_questions:
                # åˆ›å»ºç®€åŒ–æ ¼å¼
                simple_record = {
                    "question": data['question_with_context'],
                    "answer": data['answer']
                }
                
                f.write(json.dumps(simple_record, ensure_ascii=False) + '\n')
                simple_count += 1
        
        print(f"ç®€åŒ–é—®ç­”æ ¼å¼å·²ç”Ÿæˆ: {output_path}")
        print(f"é—®é¢˜æ•°: {simple_count}")
        
        return simple_count
    
    def generate_comprehensive_qa_datasets(self, samples):
        """
        ç”Ÿæˆå…¨é¢çš„é—®ç­”æ•°æ®é›†ï¼ŒåŒ…æ‹¬æ‰€æœ‰æ ¼å¼
        """
        print(f"\n" + "="*70)
        print(f"ğŸš€ å¼€å§‹ç”Ÿæˆå…¨é¢çš„é—®ç­”æ•°æ®é›†ï¼ˆéšæœºç›®æ ‡é€‰æ‹©ç­–ç•¥ï¼‰")
        print(f"="*70)
        
        # 1. ç”ŸæˆèƒŒæ™¯å¼é—®é¢˜æ•°æ®é›†
        contextual_questions = self.generate_contextual_questions(samples)
        
        # 2. ç”Ÿæˆç®€åŒ–æ ¼å¼
        simple_count = self.create_simple_qa_format(contextual_questions)
        
        # 3. ç”Ÿæˆæ•°æ®é›†æ€»ç»“æŠ¥å‘Š
        self._generate_comprehensive_summary(len(samples), len(contextual_questions), simple_count)
        
        return {
            'contextual_questions': contextual_questions,
            'simple_count': simple_count,
            'total_samples': len(samples)
        }
    
    def _generate_comprehensive_summary(self, original_samples, contextual_count, simple_count):
        """ç”Ÿæˆå…¨é¢çš„æ•°æ®é›†æ€»ç»“æŠ¥å‘Š"""
        print(f"\n" + "="*70)
        print(f"ğŸ“‹ åŸºäºéšæœºç›®æ ‡é€‰æ‹©çš„é—®é¢˜ç”Ÿæˆå™¨æ•°æ®é›†æŠ¥å‘Š")
        print(f"="*70)
        
        # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
        output_dir = os.path.join(os.path.dirname(config.OUTPUT_DATASET_PATH), "output")
        files_to_check = [
            ("èƒŒæ™¯å¼é—®é¢˜æ•°æ®é›†", os.path.join(output_dir, "contextual_qa_dataset.jsonl")),
            ("ç®€åŒ–é—®ç­”æ ¼å¼", os.path.join(output_dir, "simple_qa_format.jsonl"))
        ]
        
        for name, filepath in files_to_check:
            if os.path.exists(filepath):
                file_size = os.path.getsize(filepath) / 1024  # KB
                print(f"âœ… {name}: {file_size:.1f} KB")
                print(f"   ğŸ“ æ–‡ä»¶è·¯å¾„: {filepath}")
            else:
                print(f"âŒ {name}: æ–‡ä»¶ä¸å­˜åœ¨")
        
        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   â€¢ åŸå§‹æ ·æœ¬æ•°: {original_samples}")
        print(f"   â€¢ èƒŒæ™¯å¼é—®é¢˜æ•°: {contextual_count}")
        print(f"   â€¢ ç®€åŒ–æ ¼å¼é—®é¢˜æ•°: {simple_count}")
        print(f"   â€¢ è½¬æ¢æˆåŠŸç‡: {(contextual_count/original_samples*100):.1f}%" if original_samples > 0 else "   â€¢ è½¬æ¢æˆåŠŸç‡: N/A")
        
        print(f"\nğŸ¯ åŸºäºéšæœºç›®æ ‡é€‰æ‹©çš„é—®é¢˜ç‰¹ç‚¹:")
        print(f"   â€¢ ä»å®Œæ•´æ¨ç†é“¾è·¯éšæœºé€‰æ‹©ç›®æ ‡å®ä½“")
        print(f"   â€¢ æ¯æ¡è·¯å¾„å¯ç”Ÿæˆå¤šä¸ªä¸åŒè§’åº¦çš„é—®é¢˜")
        print(f"   â€¢ çº¿ç´¢ä¿¡æ¯æ›´åŠ ä¸°å¯Œå’Œå¤šæ ·åŒ–")
        print(f"   â€¢ é—®é¢˜éš¾åº¦å’Œç±»å‹æ›´åŠ å‡è¡¡")
        
        print(f"\nğŸ”„ ç”Ÿæˆæ–¹æ³•æ¼”è¿›:")
        print(f"   â€¢ ç¬¬ä¸€ä»£: å®Œæ•´è·¯å¾„ â†’ å®Œæ•´é™ˆè¿° â†’ å¤æ‚é™ˆè¿° â†’ åŸºäºé™ˆè¿°æé—®")
        print(f"   â€¢ ç¬¬äºŒä»£: çº¿ç´¢è·¯å¾„ â†’ çº¿ç´¢é™ˆè¿° â†’ çº¿ç´¢ç¼–ç»‡ â†’ åŸºäºçº¿ç´¢+ç›®æ ‡å…³ç³»æé—®")
        print(f"   â€¢ ç¬¬ä¸‰ä»£: å®Œæ•´é“¾è·¯æ•´åˆ â†’ éšæœºç›®æ ‡é€‰æ‹© â†’ çº¿ç´¢ç¼–ç»‡ â†’ ç›®æ ‡å¯¼å‘é—®é¢˜æ„å»º")
        
        print(f"\nğŸ“ æ–‡ä»¶ç”¨é€”è¯´æ˜:")
        print(f"   â€¢ contextual_qa_dataset.jsonl: åŸºäºéšæœºç›®æ ‡é€‰æ‹©çš„å®Œæ•´èƒŒæ™¯å¼é—®é¢˜")
        print(f"   â€¢ simple_qa_format.jsonl: çº¯é—®ç­”æ ¼å¼ï¼Œé€‚åˆç›´æ¥æµ‹è¯•")
        
        print(f"\nâœ¨ ä½¿ç”¨å»ºè®®:")
        print(f"   â€¢ å¤šæ ·æ€§æµ‹è¯•: åˆ©ç”¨éšæœºç›®æ ‡é€‰æ‹©çš„å¤šæ ·æ€§")
        print(f"   â€¢ éš¾åº¦è¯„ä¼°: æ¯”è¾ƒä¸åŒç”Ÿæˆç­–ç•¥çš„é—®é¢˜éš¾åº¦")
        print(f"   â€¢ ç³»ç»Ÿä¼˜åŒ–: å¯æ ¹æ®éœ€è¦è°ƒæ•´questions_per_pathå‚æ•°")
        
        return True

    def validate_generated_samples(self, samples):
        """
        éªŒè¯ç”Ÿæˆçš„æ ·æœ¬è´¨é‡ - æ›´æ–°éªŒè¯é€»è¾‘ä»¥é€‚åº”æ–°çš„å­—æ®µç»“æ„
        """
        valid_count = 0
        clue_not_contain_answer_count = 0
        target_diversity_count = 0
        
        for sample in samples:
            required_fields = ['integrated_clue_statement', 'contextual_narrative', 'final_question', 'answer']
            if all(field in sample and sample[field] and "Error" not in str(sample[field]) for field in required_fields):
                
                # æ£€æŸ¥1ï¼šç¡®ä¿çº¿ç´¢ä¸åŒ…å«ç­”æ¡ˆ
                clue_statement = sample['integrated_clue_statement'].lower()
                answer = sample['answer'].lower()
                clue_check_passed = answer not in clue_statement
                if clue_check_passed:
                    clue_not_contain_answer_count += 1
                
                # æ£€æŸ¥2ï¼šç¡®ä¿ç›®æ ‡å®ä½“çš„å¤šæ ·æ€§ï¼ˆä¸æ€»æ˜¯è·¯å¾„æœ«ç«¯ï¼‰
                target_entity = sample.get('target_entity', '')
                original_path = sample.get('original_path', [])
                if original_path:
                    last_entity = original_path[-1].get('object', '') if original_path else ''
                    target_diversity_check = target_entity != last_entity
                    if target_diversity_check:
                        target_diversity_count += 1
                
                # ç»¼åˆæ£€æŸ¥é€šè¿‡
                if clue_check_passed:
                    valid_count += 1
                else:
                    print(f"æ ·æœ¬è´¨é‡æ£€æŸ¥å¤±è´¥ï¼ˆçº¿ç´¢åŒ…å«ç­”æ¡ˆï¼‰: {sample.get('target_entity', 'Unknown')}")
            else:
                print(f"æ ·æœ¬è´¨é‡æ£€æŸ¥å¤±è´¥ï¼ˆå­—æ®µç¼ºå¤±ï¼‰: {sample.get('target_entity', 'Unknown')}")
        
        print(f"\n=== æ ·æœ¬è´¨é‡ç»Ÿè®¡ ===")
        print(f"æ€»ä½“è´¨é‡: {valid_count}/{len(samples)} ({(valid_count/len(samples)*100):.1f}%) é€šè¿‡æ£€æŸ¥")
        print(f"çº¿ç´¢çº¯å‡€åº¦: {clue_not_contain_answer_count}/{len(samples)} ({(clue_not_contain_answer_count/len(samples)*100):.1f}%) çº¿ç´¢ä¸åŒ…å«ç­”æ¡ˆ")
        print(f"ç›®æ ‡å¤šæ ·æ€§: {target_diversity_count}/{len(samples)} ({(target_diversity_count/len(samples)*100):.1f}%) ç›®æ ‡ä¸æ˜¯è·¯å¾„æœ«ç«¯")
        
        return valid_count / len(samples) if samples else 0

    def run_new_strategy(self, graph, total_questions=50, batch_size=10):
        """
        ğŸš€ ä¾¿æ·å…¥å£ï¼šè¿è¡Œå…¨æ–°çš„ä¸‰é˜¶æ®µæ·±åº¦é—®é¢˜ç”Ÿæˆç­–ç•¥
        
        è¿™æ˜¯ä¸€ä¸ªä¸€é”®å¼çš„æ–¹æ³•ï¼Œä¼šæ‰§è¡Œå®Œæ•´çš„ä¸‰é˜¶æ®µæµç¨‹å¹¶ç”Ÿæˆæ‰€æœ‰æ ¼å¼çš„æ•°æ®é›†ã€‚
        
        Args:
            graph: çŸ¥è¯†å›¾è°±å¯¹è±¡
            total_questions: ç›®æ ‡ç”Ÿæˆçš„é—®é¢˜æ•°é‡ (é»˜è®¤50)
            batch_size: æ‰¹æ¬¡å¤§å° (é»˜è®¤10)
            
        Returns:
            dict: åŒ…å«ç”Ÿæˆç»“æœçš„å®Œæ•´æŠ¥å‘Š
            
        Usage:
            generator = QuestionGenerator()
            result = generator.run_new_strategy(knowledge_graph, 100, 15)
        """
        print(f"\n" + "ğŸ”¥"*25 + " å…¨æ–°ä¸‰é˜¶æ®µæ·±åº¦é—®é¢˜ç”Ÿæˆç­–ç•¥ " + "ğŸ”¥"*25)
        print(f"ğŸ“‹ å‚æ•°é…ç½®:")
        print(f"   â€¢ ç›®æ ‡é—®é¢˜æ•°é‡: {total_questions}")
        print(f"   â€¢ æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   â€¢ ç­–ç•¥: å¤šæºçº¿ç´¢èšåˆ â†’ æ·±åº¦æ¨¡ç³Šä¸æ•…äº‹ç¼–ç»‡ â†’ é—®é¢˜ç”Ÿæˆä¸æ ¡éªŒ")
        print(f"" + "="*80)
        
        try:
            # æ‰§è¡Œå®Œæ•´çš„é—®ç­”æ•°æ®é›†ç”Ÿæˆæµç¨‹
            result = self.generate_comprehensive_qa_new_strategy(graph, total_questions, batch_size)
            
            if result['success']:
                print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆ! ç”ŸæˆæŠ¥å‘Š:")
                print(f"   âœ… æ€»æ ·æœ¬æ•°: {result['total_samples']}")
                print(f"   âœ… éªŒè¯é€šè¿‡æ•°: {result['validated_count']}")
                print(f"   âœ… éªŒè¯é€šè¿‡ç‡: {result['validation_rate']:.1f}%")
                print(f"   âœ… ç®€åŒ–æ ¼å¼é—®é¢˜: {result['simple_count']} ä¸ª")
                print(f"   âœ… è¯¦ç»†æ ¼å¼æ¡ç›®: {result['detailed_count']} ä¸ª")
                
                print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
                if result['validation_rate'] < 60:
                    print(f"   âš ï¸  éªŒè¯é€šè¿‡ç‡è¾ƒä½ï¼Œå»ºè®®è°ƒæ•´å›¾ç»“æ„æˆ–å¢åŠ æ‰¹æ¬¡å¤§å°")
                elif result['validation_rate'] > 80:
                    print(f"   ğŸŒŸ éªŒè¯é€šè¿‡ç‡å¾ˆé«˜ï¼Œé—®é¢˜è´¨é‡ä¼˜ç§€!")
                else:
                    print(f"   ğŸ‘ éªŒè¯é€šè¿‡ç‡è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ä¼˜åŒ–")
                
                print(f"\nğŸ“‚ è¾“å‡ºæ–‡ä»¶ä½ç½®:")
                output_dir = os.path.join(os.path.dirname(config.OUTPUT_DATASET_PATH), "output")
                print(f"   ğŸ“„ verified_qa_new_strategy.jsonl - éªŒè¯é€šè¿‡çš„å®Œæ•´é—®é¢˜")
                print(f"   ğŸ“„ simple_qa_new_strategy.jsonl - çº¯é—®ç­”æ ¼å¼")
                print(f"   ğŸ“„ detailed_qa_new_strategy.jsonl - è¯¦ç»†è°ƒè¯•ä¿¡æ¯")
                print(f"   ğŸ“ ç›®å½•: {output_dir}")
            else:
                print(f"\nâŒ ä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥å›¾ç»“æ„æˆ–å‚æ•°é…ç½®")
            
            return result
            
        except Exception as e:
            error_result = {
                'success': False,
                'error': str(e),
                'total_samples': 0,
                'validated_count': 0,
                'validation_rate': 0
            }
            print(f"\nâŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return error_result
