# import os
# import argparse
# import json
# import shutil
# from datetime import datetime
# from config import config
# from scraper_module.scraper import Scraper
# from graph_module.poster import parse_html_to_text_blocks
# from graph_module.fact_extractor import LLMExtractor
# from graph_module.graph_manager import GraphManager
# from generate_module.path_sampler import PathSampler
# from generate_module.question_generator import QuestionGenerator
# from generate_module.validator import assemble_final_json

# class RealTimeGraphManager(GraphManager):
    
#     def __init__(self, graph_path=None, auto_save=True, save_frequency=1):
#         """
#         åˆå§‹åŒ–å›¾è°±ç®¡ç†å™¨
        
#         Args:
#             graph_path: å›¾è°±æ–‡ä»¶è·¯å¾„
#             auto_save: æ˜¯å¦å¯ç”¨è‡ªåŠ¨ä¿å­˜
#             save_frequency: ä¿å­˜é¢‘ç‡ï¼ˆå¤„ç†å¤šå°‘ä¸ªæ–‡ä»¶åä¿å­˜ä¸€æ¬¡ï¼‰
#         """
#         super().__init__(graph_path)
        
#         self.auto_save = auto_save
#         self.save_frequency = save_frequency
#         self.processed_files = 0
        
#         # è®¾ç½®æ–‡ä»¶è·¯å¾„
#         self.graph_path = graph_path or config.GRAPH_STORE_PATH
#         self.backup_path = self.graph_path.replace('.gpickle', '_backup.gpickle')
#         self.triples_log_path = os.path.join(
#             os.path.dirname(self.graph_path), 
#             "triples_log.jsonl"
#         )
#         self.progress_report_path = os.path.join(
#             os.path.dirname(self.graph_path),
#             "progress_report.json"
#         )
        
#         print(f"âœ… å®æ—¶ä¿å­˜å›¾è°±ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
#         print(f"   ä¸»å›¾è°±æ–‡ä»¶: {os.path.basename(self.graph_path)}")
#         print(f"   å¤‡ä»½æ–‡ä»¶: {os.path.basename(self.backup_path)}")
#         print(f"   ä¸‰å…ƒç»„æ—¥å¿—: {os.path.basename(self.triples_log_path)}")
#         print(f"   è‡ªåŠ¨ä¿å­˜: {'å¯ç”¨' if auto_save else 'ç¦ç”¨'}")
#         if auto_save:
#             print(f"   ä¿å­˜é¢‘ç‡: æ¯å¤„ç†{save_frequency}ä¸ªæ–‡ä»¶ä¿å­˜ä¸€æ¬¡")

#     def add_triples_with_logging(self, triples, source_url):
#         """
#         æ·»åŠ ä¸‰å…ƒç»„å¹¶è®°å½•æ—¥å¿—ï¼Œæ”¯æŒå®æ—¶ä¿å­˜
        
#         Args:
#             triples: ä¸‰å…ƒç»„åˆ—è¡¨
#             source_url: æºURL
#         """
#         if not triples:
#             return []
        
#         # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
#         self._log_triples(triples, source_url)
        
#         # æ·»åŠ åˆ°å›¾è°±
#         new_nodes = self.add_triples(triples)
        
#         # æ‰§è¡Œå®ä½“å¯¹é½
#         if new_nodes:
#             self.resolve_entities_incremental(new_nodes)
        
#         # æ›´æ–°å¤„ç†è®¡æ•°
#         self.processed_files += 1
        
#         # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨ä¿å­˜
#         if self.auto_save and self.processed_files % self.save_frequency == 0:
#             self._auto_save()
        
#         return new_nodes
    
#     def _log_triples(self, triples, source_url):
#         """å°†ä¸‰å…ƒç»„è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶"""
#         log_entry = {
#             "timestamp": datetime.now().isoformat(),
#             "source_url": source_url,
#             "triples_count": len(triples),
#             "triples": triples
#         }
        
#         try:
#             with open(self.triples_log_path, 'a', encoding='utf-8') as f:
#                 f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
            
#             print(f"ğŸ“ å·²è®°å½• {len(triples)} ä¸ªä¸‰å…ƒç»„åˆ°æ—¥å¿—æ–‡ä»¶")
            
#         except Exception as e:
#             print(f"âš ï¸  è®°å½•ä¸‰å…ƒç»„æ—¥å¿—å¤±è´¥: {e}")
    
#     def _auto_save(self):
#         """è‡ªåŠ¨ä¿å­˜å›¾è°±"""
#         print(f"ğŸ’¾ è‡ªåŠ¨ä¿å­˜å›¾è°± (å·²å¤„ç†{self.processed_files}ä¸ªæ–‡ä»¶)...")
        
#         # åˆ›å»ºå•ä¸€å¤‡ä»½
#         self._create_single_backup()
        
#         # ä¿å­˜ä¸»å›¾è°±
#         self.save_graph()
        
#         # ä¿å­˜è¿›åº¦æŠ¥å‘Š
#         self._save_progress_report()
        
#         print(f"âœ… è‡ªåŠ¨ä¿å­˜å®Œæˆ")
    
#     def _create_single_backup(self):
#         """åˆ›å»ºå•ä¸€å›¾è°±å¤‡ä»½ï¼Œè¦†ç›–ä¹‹å‰çš„å¤‡ä»½"""
#         if not os.path.exists(self.graph_path):
#             return
        
#         try:
#             # å¦‚æœå¤‡ä»½æ–‡ä»¶å­˜åœ¨ï¼Œå…ˆåˆ é™¤
#             if os.path.exists(self.backup_path):
#                 os.remove(self.backup_path)
#                 print(f"ğŸ—‘ï¸  åˆ é™¤æ—§å¤‡ä»½æ–‡ä»¶")
            
#             # å¤åˆ¶å½“å‰å›¾è°±æ–‡ä»¶ä½œä¸ºå¤‡ä»½
#             shutil.copy2(self.graph_path, self.backup_path)
#             print(f"ğŸ“¦ å·²åˆ›å»ºå¤‡ä»½: {os.path.basename(self.backup_path)}")
            
#         except Exception as e:
#             print(f"âš ï¸  åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}")
    
#     def _save_progress_report(self):
#         """ä¿å­˜å¤„ç†è¿›åº¦æŠ¥å‘Š"""
#         report = {
#             "timestamp": datetime.now().isoformat(),
#             "processed_files": self.processed_files,
#             "total_nodes": len(self.G.nodes),
#             "total_edges": len(self.G.edges),
#             "graph_file": self.graph_path,
#             "backup_file": self.backup_path,
#             "triples_log": self.triples_log_path,
#             "auto_save_enabled": self.auto_save,
#             "save_frequency": self.save_frequency
#         }
        
#         try:
#             with open(self.progress_report_path, 'w', encoding='utf-8') as f:
#                 json.dump(report, f, ensure_ascii=False, indent=2)
#             print(f"ğŸ“Š å·²ä¿å­˜è¿›åº¦æŠ¥å‘Š")
#         except Exception as e:
#             print(f"âš ï¸  ä¿å­˜è¿›åº¦æŠ¥å‘Šå¤±è´¥: {e}")
    
#     def force_save(self):
#         """å¼ºåˆ¶ä¿å­˜å›¾è°±å’Œæ—¥å¿—"""
#         print(f"ğŸ’¾ å¼ºåˆ¶ä¿å­˜å›¾è°±...")
#         self._create_single_backup()
#         self.save_graph()
#         self._save_progress_report()
#         print(f"âœ… å¼ºåˆ¶ä¿å­˜å®Œæˆ")
    
#     def get_statistics(self):
#         """è·å–å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
#         last_save_time = "æœªä¿å­˜"
#         if os.path.exists(self.graph_path):
#             mod_time = datetime.fromtimestamp(os.path.getmtime(self.graph_path))
#             last_save_time = mod_time.strftime('%Y-%m-%d %H:%M:%S')
        
#         stats = {
#             "processed_files": self.processed_files,
#             "total_nodes": len(self.G.nodes),
#             "total_edges": len(self.G.edges),
#             "graph_file_exists": os.path.exists(self.graph_path),
#             "backup_file_exists": os.path.exists(self.backup_path),
#             "triples_log_exists": os.path.exists(self.triples_log_path),
#             "last_save_time": last_save_time,
#             "auto_save_enabled": self.auto_save,
#             "save_frequency": self.save_frequency
#         }
#         return stats


# def main(args):
#     # --- æ­¥éª¤ä¸€: çŸ¥è¯†è·å– ---
#     if args.run_scraper:
#         with open(config.SEED_URLS_FILE, 'r') as f:
#             seed_urls = [line.strip() for line in f if line.strip()]
#         scraper = Scraper(seed_urls, config.MAX_DEPTH, config.MAX_PAGES_TO_CRAWL)
#         scraper.run()

#     # --- æ­¥éª¤äºŒ & ä¸‰: å›¾è°±æ„å»ºï¼ˆæ”¯æŒå®æ—¶ä¿å­˜ï¼‰---
#     # ä½¿ç”¨å®æ—¶ä¿å­˜çš„å›¾è°±ç®¡ç†å™¨
#     graph_manager = RealTimeGraphManager(
#         config.GRAPH_STORE_PATH, 
#         auto_save=True, 
#         save_frequency=getattr(args, 'save_frequency', 1)
#     )
#     llm_extractor = LLMExtractor()

#     if args.build_graph:
#         html_files = [f for f in os.listdir(config.RAW_HTML_DIR) if f.endswith('.html')]
#         print(f"Found {len(html_files)} HTML files to process")
        
#         processed_count = 0
#         skipped_count = 0
#         successful_extractions = 0
#         total_triples = 0
        
#         for filename in html_files:
#             filepath = os.path.join(config.RAW_HTML_DIR, filename)
#             try:
#                 with open(filepath, 'r', encoding='utf-8') as f:
#                     content = f.read()
#             except (PermissionError, OSError) as e:
#                 print(f"Skipping {filename}: {e}")
#                 skipped_count += 1
#                 continue
            
#             # ä»HTMLæ³¨é‡Šä¸­æ¢å¤URLï¼Œä½¿ç”¨æ›´å¥å£®çš„è§£ææ–¹æ³•
#             try:
#                 lines = content.split('\n')
#                 source_url = None
                
#                 # æ–¹æ³•1: åœ¨å‰å‡ è¡Œä¸­æŸ¥æ‰¾URLæ³¨é‡Šï¼ˆWikipediaæ ¼å¼ï¼‰
#                 for line in lines[:10]:  # åªæ£€æŸ¥å‰10è¡Œ
#                     if 'URL:' in line:
#                         url_part = line.split('URL:')[1].strip()
#                         # ç§»é™¤HTMLæ³¨é‡Šç¬¦å·
#                         source_url = url_part.rstrip('-->').strip()
#                         break
                
#                 # æ–¹æ³•2: å¦‚æœæ²¡æ‰¾åˆ°URLæ³¨é‡Šï¼Œå°è¯•ä»ç™¾åº¦ç™¾ç§‘é¡µé¢æå–URL
#                 if not source_url:
#                     # æ£€æŸ¥æ˜¯å¦æ˜¯ç™¾åº¦ç™¾ç§‘é¡µé¢
#                     if 'baike.baidu.com' in content or 'data-lemmatitle=' in content:
#                         # å°è¯•ä»lemmatitleå±æ€§æå–è¯æ¡å
#                         import re
#                         lemma_match = re.search(r'data-lemmatitle="([^"]*)"', content)
#                         if lemma_match:
#                             lemma_title = lemma_match.group(1)
#                             # æ„é€ ç™¾åº¦ç™¾ç§‘URL
#                             import urllib.parse
#                             encoded_title = urllib.parse.quote(lemma_title)
#                             source_url = f"https://baike.baidu.com/item/{encoded_title}"
#                             print(f"Extracted Baidu Baike URL: {source_url}")
#                         else:
#                             # å°è¯•ä»é¡µé¢æ ‡é¢˜æå–
#                             title_match = re.search(r'<title>([^_]+)_ç™¾åº¦ç™¾ç§‘</title>', content)
#                             if title_match:
#                                 title = title_match.group(1).strip()
#                                 encoded_title = urllib.parse.quote(title)
#                                 source_url = f"https://baike.baidu.com/item/{encoded_title}"
#                                 print(f"Extracted Baidu Baike URL from title: {source_url}")
                
#                 if not source_url:
#                     print(f"Warning: Could not extract URL from {filename}, skipping...")
#                     skipped_count += 1
#                     continue
                    
#             except Exception as e:
#                 print(f"Error parsing URL from {filename}: {e}, skipping...")
#                 skipped_count += 1
#                 skipped_count += 1
#                 continue

#             text_blocks = parse_html_to_text_blocks(content, source_url)
#             print(f"Extracted {len(text_blocks)} text blocks from {filename}")
            
#             triples = llm_extractor.extract_triples_batch(text_blocks)
            
#             if triples:
#                 # ä½¿ç”¨å®æ—¶ä¿å­˜åŠŸèƒ½
#                 new_nodes = graph_manager.add_triples_with_logging(triples, source_url)
#                 successful_extractions += 1
#                 total_triples += len(triples)
#                 print(f"Added {len(triples)} triples to graph from {filename}")
#             else:
#                 print(f"No triples extracted from {filename}")
            
#             processed_count += 1
#             print(f"Processed {filename} ({processed_count}/{len(html_files)}) - Total triples so far: {total_triples}")
        
#         print(f"Processing complete: {processed_count} processed, {skipped_count} skipped")
#         print(f"Successful extractions: {successful_extractions}/{processed_count}")
#         print(f"Total triples extracted: {total_triples}")
        
#         # æœ€ç»ˆä¿å­˜
#         graph_manager.force_save()
        
#         if args.save_to_neo4j:
#             graph_manager.save_to_neo4j()
        
#         # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
#         stats = graph_manager.get_statistics()
#         print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯:")
#         for key, value in stats.items():
#             print(f"   {key}: {value}")

#     # --- æ­¥éª¤å››: é—®é¢˜ç”Ÿæˆ ---
#     if args.generate_questions:
#         path_sampler = PathSampler(graph_manager.G)
#         question_generator = QuestionGenerator()
        
#         # é‡‡æ ·é“¾å¼æ¨ç†è·¯å¾„
#         chain_paths = path_sampler.sample_chain_reasoning(num_samples=config.NUM_QUESTIONS_TO_GENERATE)
        
#         # æ‰§è¡Œç”Ÿæˆçº§è”
#         generated_samples = question_generator.generate_questions_cascade(chain_paths)
        
#         # --- æ­¥éª¤äº”: éªŒè¯ä¸è¾“å‡º ---
#         os.makedirs(os.path.dirname(config.OUTPUT_DATASET_PATH), exist_ok=True)
#         with open(config.OUTPUT_DATASET_PATH, 'w', encoding='utf-8') as f:
#             for i, sample in enumerate(generated_samples):
#                 final_data_point = assemble_final_json(i, sample, graph_manager.G)
#                 f.write(json.dumps(final_data_point, ensure_ascii=False) + '\n')
        
#         print(f"Successfully generated {len(generated_samples)} questions to {config.OUTPUT_DATASET_PATH}")


# if __name__ == "__main__":
#     parser = argparse.ArgumentParser(description="Multi-Hop QA Dataset Generation Pipeline with Real-time Saving")
#     parser.add_argument("--run-scraper", action="store_true", help="Run the web scraping stage.")
#     parser.add_argument("--build-graph", action="store_true", help="Build the knowledge graph from scraped HTML.")
#     parser.add_argument("--generate-questions", action="store_true", help="Generate questions from the knowledge graph.")
#     parser.add_argument("--save-to-neo4j", action="store_true", help="Save the final graph to Neo4j database.")
#     parser.add_argument("--save-frequency", type=int, default=1, help="Auto-save frequency (save after processing N files, default: 1)")
    
#     args = parser.parse_args()
#     main(args)
import os
import argparse
import json
import shutil
from datetime import datetime
from config import config
from scraper_module.scraper import Scraper
from graph_module.poster import parse_html_to_text_blocks
from graph_module.fact_extractor import LLMExtractor
from graph_module.graph_manager import GraphManager
from generate_module.path_sampler import PathSampler
from generate_module.question_generator import QuestionGenerator
from generate_module.validator import assemble_final_json

class RealTimeGraphManager(GraphManager):
    
    def __init__(self, graph_path=None, auto_save=True, save_frequency=1):
        """
        åˆå§‹åŒ–å›¾è°±ç®¡ç†å™¨
        
        Args:
            graph_path: å›¾è°±æ–‡ä»¶è·¯å¾„
            auto_save: æ˜¯å¦å¯ç”¨è‡ªåŠ¨ä¿å­˜
            save_frequency: ä¿å­˜é¢‘ç‡ï¼ˆå¤„ç†å¤šå°‘ä¸ªæ–‡ä»¶åä¿å­˜ä¸€æ¬¡ï¼‰
        """
        super().__init__(graph_path)
        
        self.auto_save = auto_save
        self.save_frequency = save_frequency
        self.processed_files = 0
        
        # è®¾ç½®æ–‡ä»¶è·¯å¾„
        self.graph_path = graph_path or config.GRAPH_STORE_PATH
        self.backup_path = self.graph_path.replace('.gpickle', '_backup.gpickle')
        self.triples_log_path = os.path.join(
            os.path.dirname(self.graph_path), 
            "triples_log.jsonl"
        )
        self.progress_report_path = os.path.join(
            os.path.dirname(self.graph_path),
            "progress_report.json"
        )
        
        print(f"âœ… å®æ—¶ä¿å­˜å›¾è°±ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ä¸»å›¾è°±æ–‡ä»¶: {os.path.basename(self.graph_path)}")
        print(f"   å¤‡ä»½æ–‡ä»¶: {os.path.basename(self.backup_path)}")
        print(f"   ä¸‰å…ƒç»„æ—¥å¿—: {os.path.basename(self.triples_log_path)}")
        print(f"   è‡ªåŠ¨ä¿å­˜: {'å¯ç”¨' if auto_save else 'ç¦ç”¨'}")
        if auto_save:
            print(f"   ä¿å­˜é¢‘ç‡: æ¯å¤„ç†{save_frequency}ä¸ªæ–‡ä»¶ä¿å­˜ä¸€æ¬¡")

    def add_triples_with_logging(self, triples, source_url):
        """
        æ·»åŠ ä¸‰å…ƒç»„å¹¶è®°å½•æ—¥å¿—ï¼Œæ”¯æŒå®æ—¶ä¿å­˜
        
        Args:
            triples: ä¸‰å…ƒç»„åˆ—è¡¨
            source_url: æºURL
        """
        if not triples:
            return []
        
        # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
        self._log_triples(triples, source_url)
        
        # æ·»åŠ åˆ°å›¾è°±
        new_nodes = self.add_triples(triples)
        
        # æ‰§è¡Œå®ä½“å¯¹é½
        if new_nodes:
            self.resolve_entities_incremental(new_nodes)
        
        # æ›´æ–°å¤„ç†è®¡æ•°
        self.processed_files += 1
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨ä¿å­˜
        if self.auto_save and self.processed_files % self.save_frequency == 0:
            self._auto_save()
        
        return new_nodes
    
    def _log_triples(self, triples, source_url):
        """å°†ä¸‰å…ƒç»„è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "source_url": source_url,
            "triples_count": len(triples),
            "triples": triples
        }
        
        try:
            with open(self.triples_log_path, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
            
            print(f"ğŸ“ å·²è®°å½• {len(triples)} ä¸ªä¸‰å…ƒç»„åˆ°æ—¥å¿—æ–‡ä»¶")
            
        except Exception as e:
            print(f"âš ï¸  è®°å½•ä¸‰å…ƒç»„æ—¥å¿—å¤±è´¥: {e}")
    
    def _auto_save(self):
        """è‡ªåŠ¨ä¿å­˜å›¾è°±"""
        print(f"ğŸ’¾ è‡ªåŠ¨ä¿å­˜å›¾è°± (å·²å¤„ç†{self.processed_files}ä¸ªæ–‡ä»¶)...")
        
        # åˆ›å»ºå•ä¸€å¤‡ä»½
        self._create_single_backup()
        
        # ä¿å­˜ä¸»å›¾è°±
        self.save_graph()
        
        # ä¿å­˜è¿›åº¦æŠ¥å‘Š
        self._save_progress_report()
        
        print(f"âœ… è‡ªåŠ¨ä¿å­˜å®Œæˆ")
    
    def _create_single_backup(self):
        """åˆ›å»ºå•ä¸€å›¾è°±å¤‡ä»½ï¼Œè¦†ç›–ä¹‹å‰çš„å¤‡ä»½"""
        if not os.path.exists(self.graph_path):
            return
        
        try:
            # å¦‚æœå¤‡ä»½æ–‡ä»¶å­˜åœ¨ï¼Œå…ˆåˆ é™¤
            if os.path.exists(self.backup_path):
                os.remove(self.backup_path)
                print(f"ğŸ—‘ï¸  åˆ é™¤æ—§å¤‡ä»½æ–‡ä»¶")
            
            # å¤åˆ¶å½“å‰å›¾è°±æ–‡ä»¶ä½œä¸ºå¤‡ä»½
            shutil.copy2(self.graph_path, self.backup_path)
            print(f"ğŸ“¦ å·²åˆ›å»ºå¤‡ä»½: {os.path.basename(self.backup_path)}")
            
        except Exception as e:
            print(f"âš ï¸  åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}")
    
    def _save_progress_report(self):
        """ä¿å­˜å¤„ç†è¿›åº¦æŠ¥å‘Š"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "processed_files": self.processed_files,
            "total_nodes": len(self.G.nodes),
            "total_edges": len(self.G.edges),
            "graph_file": self.graph_path,
            "backup_file": self.backup_path,
            "triples_log": self.triples_log_path,
            "auto_save_enabled": self.auto_save,
            "save_frequency": self.save_frequency
        }
        
        try:
            with open(self.progress_report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“Š å·²ä¿å­˜è¿›åº¦æŠ¥å‘Š")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜è¿›åº¦æŠ¥å‘Šå¤±è´¥: {e}")
    
    def force_save(self):
        """å¼ºåˆ¶ä¿å­˜å›¾è°±å’Œæ—¥å¿—"""
        print(f"ğŸ’¾ å¼ºåˆ¶ä¿å­˜å›¾è°±...")
        self._create_single_backup()
        self.save_graph()
        self._save_progress_report()
        print(f"âœ… å¼ºåˆ¶ä¿å­˜å®Œæˆ")
    
    def get_statistics(self):
        """è·å–å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
        last_save_time = "æœªä¿å­˜"
        if os.path.exists(self.graph_path):
            mod_time = datetime.fromtimestamp(os.path.getmtime(self.graph_path))
            last_save_time = mod_time.strftime('%Y-%m-%d %H:%M:%S')
        
        stats = {
            "processed_files": self.processed_files,
            "total_nodes": len(self.G.nodes),
            "total_edges": len(self.G.edges),
            "graph_file_exists": os.path.exists(self.graph_path),
            "backup_file_exists": os.path.exists(self.backup_path),
            "triples_log_exists": os.path.exists(self.triples_log_path),
            "last_save_time": last_save_time,
            "auto_save_enabled": self.auto_save,
            "save_frequency": self.save_frequency
        }
        return stats


def main(args):
    # --- æ­¥éª¤ä¸€: çŸ¥è¯†è·å– ---
    if args.run_scraper:
        with open(config.SEED_URLS_FILE, 'r') as f:
            seed_urls = [line.strip() for line in f if line.strip()]
        scraper = Scraper(seed_urls, config.MAX_DEPTH, config.MAX_PAGES_TO_CRAWL)
        scraper.run()

    # --- æ­¥éª¤äºŒ & ä¸‰: å›¾è°±æ„å»ºï¼ˆæ”¯æŒå®æ—¶ä¿å­˜ï¼‰---
    # ä½¿ç”¨å®æ—¶ä¿å­˜çš„å›¾è°±ç®¡ç†å™¨
    graph_manager = RealTimeGraphManager(
        config.GRAPH_STORE_PATH, 
        auto_save=True, 
        save_frequency=getattr(args, 'save_frequency', 1)
    )
    llm_extractor = LLMExtractor()

    if args.build_graph:
        all_html_files = [f for f in os.listdir(config.RAW_HTML_DIR) if f.endswith('.html')]
        total_files = len(all_html_files)
        print(f"Found {total_files} HTML files total")
        
        # æ–­ç‚¹ç»­ä¼ åŠŸèƒ½ï¼šä»æŒ‡å®šç´¢å¼•å¼€å§‹å¤„ç†
        start_index = getattr(args, 'start_from', 0)
        if start_index > 0:
            print(f"Resuming from file index {start_index} (skipping first {start_index} files)")
            html_files = all_html_files[start_index:]
        else:
            html_files = all_html_files
        
        print(f"Will process {len(html_files)} files (from index {start_index} to {total_files-1})")
        
        processed_count = start_index  # ä»æŒ‡å®šç´¢å¼•å¼€å§‹è®¡æ•°
        skipped_count = 0
        successful_extractions = 0
        total_triples = 0
        
        for filename in html_files:
            filepath = os.path.join(config.RAW_HTML_DIR, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
            except (PermissionError, OSError) as e:
                print(f"Skipping {filename}: {e}")
                skipped_count += 1
                continue
            
            # ä»HTMLæ³¨é‡Šä¸­æ¢å¤URLï¼Œä½¿ç”¨æ›´å¥å£®çš„è§£ææ–¹æ³•
            try:
                lines = content.split('\n')
                source_url = None
                
                # æ–¹æ³•1: åœ¨å‰å‡ è¡Œä¸­æŸ¥æ‰¾URLæ³¨é‡Šï¼ˆWikipediaæ ¼å¼ï¼‰
                for line in lines[:10]:  # åªæ£€æŸ¥å‰10è¡Œ
                    if 'URL:' in line:
                        url_part = line.split('URL:')[1].strip()
                        # ç§»é™¤HTMLæ³¨é‡Šç¬¦å·
                        source_url = url_part.rstrip('-->').strip()
                        break
                
                # æ–¹æ³•2: å¦‚æœæ²¡æ‰¾åˆ°URLæ³¨é‡Šï¼Œå°è¯•ä»ç™¾åº¦ç™¾ç§‘é¡µé¢æå–URL
                if not source_url:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç™¾åº¦ç™¾ç§‘é¡µé¢
                    if 'baike.baidu.com' in content or 'data-lemmatitle=' in content:
                        # å°è¯•ä»lemmatitleå±æ€§æå–è¯æ¡å
                        import re
                        lemma_match = re.search(r'data-lemmatitle="([^"]*)"', content)
                        if lemma_match:
                            lemma_title = lemma_match.group(1)
                            # æ„é€ ç™¾åº¦ç™¾ç§‘URL
                            import urllib.parse
                            encoded_title = urllib.parse.quote(lemma_title)
                            source_url = f"https://baike.baidu.com/item/{encoded_title}"
                            print(f"Extracted Baidu Baike URL: {source_url}")
                        else:
                            # å°è¯•ä»é¡µé¢æ ‡é¢˜æå–
                            title_match = re.search(r'<title>([^_]+)_ç™¾åº¦ç™¾ç§‘</title>', content)
                            if title_match:
                                title = title_match.group(1).strip()
                                encoded_title = urllib.parse.quote(title)
                                source_url = f"https://baike.baidu.com/item/{encoded_title}"
                                print(f"Extracted Baidu Baike URL from title: {source_url}")
                
                if not source_url:
                    print(f"Warning: Could not extract URL from {filename}, skipping...")
                    skipped_count += 1
                    continue
                    
            except Exception as e:
                print(f"Error parsing URL from {filename}: {e}, skipping...")
                skipped_count += 1
                skipped_count += 1
                continue

            text_blocks = parse_html_to_text_blocks(content, source_url)
            print(f"Extracted {len(text_blocks)} text blocks from {filename}")
            
            triples = llm_extractor.extract_triples_batch(text_blocks)
            
            if triples:
                # ä½¿ç”¨å®æ—¶ä¿å­˜åŠŸèƒ½
                new_nodes = graph_manager.add_triples_with_logging(triples, source_url)
                successful_extractions += 1
                total_triples += len(triples)
                print(f"Added {len(triples)} triples to graph from {filename}")
            else:
                print(f"No triples extracted from {filename}")
            
            processed_count += 1
            print(f"Processed {filename} ({processed_count}/{total_files}) - Total triples so far: {total_triples}")
        
        print(f"Processing complete: {processed_count} processed, {skipped_count} skipped")
        print(f"Successful extractions: {successful_extractions}/{processed_count}")
        print(f"Total triples extracted: {total_triples}")
        
        # æœ€ç»ˆä¿å­˜
        graph_manager.force_save()
        
        if args.save_to_neo4j:
            graph_manager.save_to_neo4j()
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = graph_manager.get_statistics()
        print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯:")
        for key, value in stats.items():
            print(f"   {key}: {value}")

    # --- æ­¥éª¤å››: é—®é¢˜ç”Ÿæˆ ---
    if args.generate_questions:
        print(f"\nğŸš€ å¼€å§‹ä½¿ç”¨æ–°çš„ä¸‰é˜¶æ®µæ·±åº¦é—®é¢˜ç”Ÿæˆç­–ç•¥...")
        
        # åˆå§‹åŒ–æ–°çš„é—®é¢˜ç”Ÿæˆå™¨
        question_generator = QuestionGenerator()
        
        # æ£€æŸ¥å›¾çš„åŸºæœ¬ä¿¡æ¯
        total_nodes = len(graph_manager.G.nodes)
        total_edges = len(graph_manager.G.edges)
        print(f"ğŸ“Š çŸ¥è¯†å›¾è°±ä¿¡æ¯:")
        print(f"   èŠ‚ç‚¹æ•°: {total_nodes}")
        print(f"   è¾¹æ•°: {total_edges}")
        
        if total_nodes < 10:
            print(f"âš ï¸  è­¦å‘Š: å›¾èŠ‚ç‚¹æ•°é‡è¿‡å°‘ ({total_nodes})ï¼Œå¯èƒ½å½±å“é—®é¢˜ç”Ÿæˆæ•ˆæœ")
            print(f"ğŸ’¡ å»ºè®®: å…ˆè¿è¡Œ --build-graph æ„å»ºæ›´å®Œæ•´çš„çŸ¥è¯†å›¾è°±")
            return
        
        # æ ¹æ®å‚æ•°é€‰æ‹©ç”Ÿæˆæ–¹å¼
        if hasattr(args, 'use_old_strategy') and args.use_old_strategy:
            print(f"ğŸ“š ä½¿ç”¨æ—§ç‰ˆç­–ç•¥ (å·²åºŸå¼ƒï¼Œä»…ç”¨äºå¯¹æ¯”)")
            path_sampler = PathSampler(graph_manager.G)
            question_generator_old = QuestionGeneratorOld()
            
            # é‡‡æ ·é“¾å¼æ¨ç†è·¯å¾„
            chain_paths = path_sampler.sample_chain_reasoning(num_samples=config.NUM_QUESTIONS_TO_GENERATE)
            
            # æ‰§è¡Œæ—§ç‰ˆç”Ÿæˆçº§è”
            generated_samples = question_generator_old.generate_questions_cascade(chain_paths, batch_size=10)
            
            # æ—§ç‰ˆè¾“å‡ºæ ¼å¼
            os.makedirs(os.path.dirname(config.OUTPUT_DATASET_PATH), exist_ok=True)
            with open(config.OUTPUT_DATASET_PATH, 'w', encoding='utf-8') as f:
                for i, sample in enumerate(generated_samples):
                    final_data_point = assemble_final_json(i, sample, graph_manager.G)
                    f.write(json.dumps(final_data_point, ensure_ascii=False) + '\n')
            
            print(f"ğŸ“„ æ—§ç‰ˆç­–ç•¥ç”Ÿæˆå®Œæˆ: {len(generated_samples)} ä¸ªé—®é¢˜ â†’ {config.OUTPUT_DATASET_PATH}")
        
        else:
            print(f"ğŸŒŸ ä½¿ç”¨æ–°çš„ä¸‰é˜¶æ®µæ·±åº¦ç­–ç•¥")
            
            # ç¡®å®šç”Ÿæˆå‚æ•°
            total_questions = getattr(args, 'num_questions', config.NUM_QUESTIONS_TO_GENERATE)
            batch_size = getattr(args, 'batch_size', 10)
            
            print(f"ğŸ“‹ ç”Ÿæˆå‚æ•°:")
            print(f"   ç›®æ ‡é—®é¢˜æ•°: {total_questions}")
            print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
            
            # ä½¿ç”¨æ–°çš„ä¸€é”®è¿è¡Œæ–¹æ³•
            result = question_generator.run_new_strategy(
                graph=graph_manager.G,
                total_questions=total_questions,
                batch_size=batch_size
            )
            
            # æŠ¥å‘Šç»“æœ
            if result['success']:
                print(f"\nğŸ‰ æ–°ç­–ç•¥ç”ŸæˆæˆåŠŸ!")
                print(f"ğŸ“Š ç”Ÿæˆç»“æœ:")
                print(f"   æ€»æ ·æœ¬æ•°: {result['total_samples']}")
                print(f"   éªŒè¯é€šè¿‡æ•°: {result['validated_count']}")
                print(f"   éªŒè¯é€šè¿‡ç‡: {result['validation_rate']:.1f}%")
                print(f"   ç®€åŒ–æ ¼å¼é—®é¢˜æ•°: {result['simple_count']}")
                
                # è¾“å‡ºæ–‡ä»¶ä½ç½®
                output_dir = os.path.join(os.path.dirname(config.OUTPUT_DATASET_PATH), "output")
                print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
                print(f"   éªŒè¯é€šè¿‡çš„é—®é¢˜: {output_dir}/validated_qa_new_strategy.jsonl")
                print(f"   ç®€åŒ–é—®ç­”æ ¼å¼: {output_dir}/simple_qa_new_strategy.jsonl")
                print(f"   è¯¦ç»†è°ƒè¯•ä¿¡æ¯: {output_dir}/detailed_qa_new_strategy.jsonl")
                
                # ä¸ºäº†å‘åå…¼å®¹ï¼Œä¹Ÿç”Ÿæˆä¼ ç»Ÿæ ¼å¼
                if result['validated_count'] > 0:
                    print(f"\nğŸ“„ åŒæ—¶ç”Ÿæˆä¼ ç»Ÿæ ¼å¼æ–‡ä»¶ä»¥ä¿æŒå…¼å®¹æ€§...")
                    os.makedirs(os.path.dirname(config.OUTPUT_DATASET_PATH), exist_ok=True)
                    
                    # è¯»å–éªŒè¯é€šè¿‡çš„é—®é¢˜å¹¶è½¬æ¢ä¸ºä¼ ç»Ÿæ ¼å¼
                    validated_file = os.path.join(output_dir, "validated_qa_new_strategy.jsonl")
                    if os.path.exists(validated_file):
                        with open(validated_file, 'r', encoding='utf-8') as f_in, \
                             open(config.OUTPUT_DATASET_PATH, 'w', encoding='utf-8') as f_out:
                            
                            for i, line in enumerate(f_in):
                                try:
                                    qa_item = json.loads(line)
                                    # è½¬æ¢ä¸ºä¼ ç»Ÿæ ¼å¼
                                    traditional_format = {
                                        "id": i,
                                        "question": qa_item.get('question', ''),
                                        "answer": qa_item.get('answer', ''),
                                        "context": qa_item.get('background_story', ''),
                                        "reasoning_path": [],  # æ–°ç­–ç•¥ä¸­è·¯å¾„ç»“æ„ä¸åŒ
                                        "metadata": {
                                            "strategy": "new_three_stage_strategy",
                                            "validation_status": qa_item.get('validation_status', ''),
                                            "clue_count": len(qa_item.get('clue_entities', [])),
                                            "generation_method": "multi_source_clue_aggregation"
                                        }
                                    }
                                    f_out.write(json.dumps(traditional_format, ensure_ascii=False) + '\n')
                                except Exception as e:
                                    print(f"âš ï¸  è½¬æ¢ç¬¬{i}è¡Œæ—¶å‡ºé”™: {e}")
                        
                        print(f"âœ… ä¼ ç»Ÿæ ¼å¼å…¼å®¹æ–‡ä»¶å·²ç”Ÿæˆ: {config.OUTPUT_DATASET_PATH}")
                
            else:
                print(f"\nâŒ æ–°ç­–ç•¥ç”Ÿæˆå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                print(f"ğŸ’¡ å»ºè®®:")
                print(f"   1. æ£€æŸ¥çŸ¥è¯†å›¾è°±æ˜¯å¦æ­£ç¡®æ„å»º")
                print(f"   2. ç¡®è®¤APIé…ç½®æ˜¯å¦æ­£ç¡®") 
                print(f"   3. å°è¯•å‡å°‘ç›®æ ‡é—®é¢˜æ•°æˆ–æ‰¹æ¬¡å¤§å°")
                print(f"   4. æŸ¥çœ‹è¯¦ç»†é”™è¯¯æ—¥å¿—")
                return


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Multi-Hop QA Dataset Generation Pipeline with New Three-Stage Strategy")
    
    # åŸæœ‰å‚æ•°
    parser.add_argument("--run-scraper", action="store_true", help="Run the web scraping stage.")
    parser.add_argument("--build-graph", action="store_true", help="Build the knowledge graph from scraped HTML.")
    parser.add_argument("--generate-questions", action="store_true", help="Generate questions from the knowledge graph using NEW three-stage strategy.")
    parser.add_argument("--save-to-neo4j", action="store_true", help="Save the final graph to Neo4j database.")
    parser.add_argument("--save-frequency", type=int, default=1, help="Auto-save frequency (save after processing N files, default: 1)")
    parser.add_argument("--start-from", type=int, default=0, help="Resume processing from file index N (0-based indexing, default: 0)")
    
    # æ–°å¢çš„é—®é¢˜ç”Ÿæˆå‚æ•°
    parser.add_argument("--num-questions", type=int, default=None, help="Number of questions to generate (default: from config)")
    parser.add_argument("--batch-size", type=int, default=10, help="Batch size for question generation (default: 10)")
    parser.add_argument("--use-old-strategy", action="store_true", help="Use old question generation strategy (deprecated, for comparison only)")
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šé—®é¢˜æ•°é‡ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
    if args.num_questions is None:
        args.num_questions = config.NUM_QUESTIONS_TO_GENERATE
    
    print(f"ğŸš€ å¤šè·³é—®ç­”æ•°æ®é›†ç”Ÿæˆæµæ°´çº¿")
    print(f"ç­–ç•¥: {'æ–°ä¸‰é˜¶æ®µæ·±åº¦ç­–ç•¥' if not args.use_old_strategy else 'æ—§ç‰ˆç­–ç•¥(å·²åºŸå¼ƒ)'}")
    print(f"="*60)
    
    main(args)